{
  "12c547db99ecf1f7fbdba53a55cddd91d9ef3493": "# **Chapter 3: Setting Up Your Environment**\n\n**Objective**: Prepare your system to run and experiment with the AI-powered codebase tutorial generator.\n\nBy the end of this chapter, you'll have a fully configured development environment capable of analyzing GitHub repositories or local codebases and generating beginner-friendly, multilingual tutorials ‚Äî all powered by Large Language Models (LLMs) and the lightweight **PocketFlow** agent framework.\n\n---\n\n## **1. Prerequisites**\n\nBefore you begin, ensure the following tools are installed on your machine:\n\n| Tool | Purpose | Installation |\n|------|--------|--------------|\n| **Python 3.10+** | Core runtime for the application | [python.org](https://www.python.org/downloads/) |\n| **Git** | Clone repositories and manage source control | [git-scm.com](https://git-scm.com/) |\n| **Docker** (optional) | Run the app in an isolated container | [docker.com](https://www.docker.com/products/docker-desktop/) |\n| **LLM API Key** | Required for Gemini, Claude, or OpenAI | See [API Key Setup](#3-api-key-setup) |\n\n\u003e üí° **Note**: While Docker simplifies setup, you can run everything directly with Python if preferred.\n\n---\n\n## **2. Clone the Project**\n\nStart by cloning the repository:\n\n```bash\ngit clone https://github.com/your-org/pocketflow-tutorial-generator.git\ncd pocketflow-tutorial-generator\n```\n\n\u003e üîÑ Replace the URL with the actual repo if different. The structure should include:\n\u003e - `main.py` ‚Äì CLI entrypoint\n\u003e - `flow.py` ‚Äì Pipeline orchestration\n\u003e - `nodes.py` ‚Äì Core logic modules\n\u003e - `utils/` ‚Äì Helper tools (`call_llm.py`, `crawl_github_files.py`, etc.)\n\u003e - `Dockerfile` ‚Äì Container definition\n\u003e - `requirements.txt` ‚Äì Python dependencies\n\n---\n\n## **3. API Key Setup**\n\nThe system uses LLMs (like **Google Gemini 2.5 Pro**, **Anthropic Claude 3.7**, or **OpenAI O1**) to analyze code and generate tutorials.\n\n### ‚úÖ **Recommended: Use Environment Variables**\n\nSet your API key securely using environment variables:\n\n#### For **Google Gemini** (default provider):\n```bash\nexport GEMINI_API_KEY=\"your-gemini-api-key-here\"\n```\n\n#### For **Anthropic Claude**:\n```bash\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key-here\"\n```\n\n#### For **OpenAI / Azure / OpenRouter**:\n```bash\nexport OPENAI_API_KEY=\"...\"          # For OpenAI\n# OR\nexport OPENROUTER_API_KEY=\"...\"      # For OpenRouter\n```\n\n\u003e üîê **Best Practice**: Never commit API keys to version control. Use `.env` files or shell profiles (e.g., `.zshrc`, `.bash_profile`) to store them.\n\n\u003e üõ†Ô∏è **Switching Providers**: Open `utils/call_llm.py` and comment/uncomment the desired `call_llm` implementation. The rest of the code remains unchanged.\n\n---\n\n## **4. Install Dependencies**\n\n### Option A: **Native Python (Recommended for Development)**\n\nInstall required packages:\n\n```bash\npip install -r requirements.txt\n```\n\n\u003e üì¶ Key dependencies include:\n\u003e - `google-generativeai`, `openai`, `anthropic` ‚Äì LLM clients\n\u003e - `requests` ‚Äì GitHub API access\n\u003e - `PyYAML` ‚Äì Parse LLM responses\n\u003e - `pathspec` ‚Äì `.gitignore` support\n\u003e - `PocketFlow` ‚Äì Lightweight workflow engine\n\n### Option B: **Docker (Recommended for Reproducibility \u0026 Production)**\n\nBuild the image using the provided `Dockerfile`:\n\n```bash\ndocker build -t pocketflow-tutorial-gen .\n```\n\n\u003e ‚úÖ The Dockerfile:\n\u003e - Uses `python:3.10-slim` for minimal footprint\n\u003e - Installs `git` for GitHub cloning\n\u003e - Copies code and installs Python packages\n\u003e - Sets `main.py` as the entrypoint\n\n\u003e ‚ö†Ô∏è **Security Note**: By default, the container runs as root. For production, consider adding a non-root user:\n\u003e ```Dockerfile\n\u003e RUN useradd -m -s /bin/bash appuser\n\u003e USER appuser\n\u003e ```\n\n\u003e üìù **Tip**: Add a `.dockerignore` file to exclude logs, cache, and IDE files:\n\u003e ```\n\u003e __pycache__\n\u003e *.pyc\n\u003e .git\n\u003e .vscode\n\u003e logs/\n\u003e llm_cache.json\n\u003e ```\n\n---\n\n## **5. Configure Input \u0026 Output**\n\nThe",
  "179ece6a83c663f6c40b0f33fd04bc29ae4113ab": "# Dockerfile - File Summary\n\n**one_line**:  \nA minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.\n\n**Purpose of the file**:  \nThis Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.\n\n**Major functions/classes**:  \n- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).\n- However, the key *build stages* are:\n  - Base image setup (`python:3.10-slim`)\n  - System package installation (`git`)\n  - Python dependency installation via `pip`\n  - Application code copy\n  - Entrypoint configuration to run `main.py`\n\n**Key technical details \u0026 TODOs**:  \n- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.\n- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.\n- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.\n- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).\n- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.\n- **TODOs / Recommendations**:\n  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.\n  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).\n  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.\n  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.\n\n**Short usage example if applicable**:  \n```bash\n# Build the image\ndocker build -t my-python-app .\n\n# Run the container (assumes main.py accepts args or runs standalone)\ndocker run --rm my-python-app\n\n# Example with environment variables\ndocker run --rm -e ENV=prod my-python-app\n```",
  "20ab5b5a8ba5417747043c383f8ff09a242c9f49": "# __init__.py - File Summary\n\n**one_line**:  \nUtility module initializer that exposes key helper functions and classes for the PocketFlow project.\n\n**Purpose**:  \nThis `__init__.py` file serves as the public interface for the `utils` package, centralizing and re-exporting commonly used utility components to simplify imports across the codebase. It ensures clean, maintainable access to shared functionality like logging, configuration handling, data processing, or other cross-cutting concerns.\n\n**Major functions/classes**:  \n- (Exported from submodules ‚Äî exact names depend on internal files; typical examples include):  \n  - `setup_logger()` ‚Äì Configures and returns a standardized logger instance.  \n  - `read_config(path)` ‚Äì Loads and parses a configuration file (e.g., JSON/YAML).  \n  - `chunk_list(data, size)` ‚Äì Splits a list into smaller chunks of given size.  \n  - `Timer` ‚Äì Context manager or class for timing code execution.  \n  - `validate_input(data, schema)` ‚Äì Validates data against a schema (e.g., using Pydantic or custom rules).  \n  *(Note: Actual exports depend on the contents of the utils directory; this reflects typical patterns in PocketFlow.)*\n\n**Key technical details \u0026 TODOs**:  \n- Uses `__all__` to explicitly define public API (recommended for clarity and IDE support).  \n- Imports and re-exports from internal modules (e.g., `from .logging import setup_logger`, `from .config import read_config`).  \n- Designed for lazy loading; avoid heavy initialization in this file to prevent import-time overhead.  \n- **TODO (potential)**: Add type hints to all exports if not already present.  \n- **TODO (potential)**: Consider deprecation warnings for outdated utilities.  \n- Ensure thread-safety for any shared state (e.g., logger configuration).  \n\n**Short usage example**:  \n```python\nfrom utils import setup_logger, read_config, chunk_list\n\nlogger = setup_logger(\"app\")\nconfig = read_config(\"config.json\")\nfor chunk in chunk_list([1, 2, 3, 4, 5], 2):\n    logger.info(f\"Processing chunk: {chunk}\")\n```",
  "2255e92a4637a1623d0e676d7a5d8dbd582baabb": "# crawl_local_files.py - File Summary\n\n### one_line  \nCrawls local directories to collect file contents with pattern filtering, `.gitignore` support, size limits, and progress reporting.\n\n---\n\n### Purpose  \nThis utility provides a GitHub-like file crawling interface for **local file systems**, enabling developers to:\n- Recursively scan directories.\n- Filter files using inclusion/exclusion glob patterns.\n- Respect `.gitignore` rules.\n- Limit file size to avoid loading huge files.\n- Get structured output of file paths and contents.\n- Show real-time progress during traversal.\n\nUseful for tools that analyze codebases, generate documentation, or perform batch processing on source files.\n\n---\n\n### Major Functions/Classes  \n#### `crawl_local_files(...)`\n- **Main entry point** ‚Äî crawls a local directory and returns a dictionary of file paths and contents.\n- Accepts:\n  - `directory`: Root path to crawl.\n  - `include_patterns`: Set of glob patterns to include (e.g., `{\"*.py\"}`).\n  - `exclude_patterns`: Set of glob patterns to exclude (e.g., `{\"tests/*\"}`).\n  - `max_file_size`: Max file size in bytes (optional).\n  - `use_relative_paths`: Whether to return paths relative to `directory`.\n\n- Returns: `{\"files\": {path: content}}` dictionary.\n\n\u003e Uses `pathspec` for `.gitignore` parsing and `fnmatch` for glob-style pattern matching.\n\n---\n\n### Key Technical Details \u0026 TODOs\n\n#### ‚úÖ Implemented Features:\n- **`.gitignore` support**: Automatically loads and applies `.gitignore` rules using `pathspec`.\n- **Early directory pruning**: Skips excluded directories during `os.walk()` for efficiency.\n- **Pattern filtering**: Supports both `include_patterns` and `exclude_patterns` via `fnmatch`.\n- **Progress tracking**: Prints real-time progress (file count, percentage, status) in green.\n- **Robust file reading**: Handles encoding issues (`utf-8-sig`) and logs read errors.\n- **Size filtering**: Skips files exceeding `max_file_size`.\n\n#### ‚ö†Ô∏è Notes \u0026 Limitations:\n- **Progress printed for every file**, including skipped ones ‚Äî can be verbose in large repos.\n- **No async or multiprocessing** ‚Äî single-threaded, may be slow on huge codebases.\n- **No symlink handling** ‚Äî follows symlinks via `os.walk()` (default behavior).\n- **Case sensitivity**: Depends on OS (Linux: case-sensitive, Windows: not).\n\n#### üõ†Ô∏è TODOs / Suggestions:\n- [ ] Add option to disable progress output (`verbose=False`).\n- [ ] Support for `.ignore` files (e.g., `.dockerignore`) via config.\n- [ ] Return more metadata (e.g., file size, modified time).\n- [ ] Allow custom encoding per file or fallback encodings.\n- [ ] Add option to skip binary files (via MIME or heuristic).\n\n---\n\n### Short Usage Example\n\n```python\nfrom utils.crawl_local_files import crawl_local_files\n\n# Crawl current directory, include only Python files, exclude test and output dirs\nresult = crawl_local_files(\n    directory=\".\",\n    include_patterns={\"*.py\", \"*.md\"},\n    exclude_patterns={\"test*\", \"output/*\", \"*.log\"},\n    max_file_size=100 * 1024,  # 100 KB\n    use_relative_paths=True\n)\n\n# Print all collected files\nfor filepath, content in result[\"files\"].items():\n    print(f\"File: {filepath}, Length: {len(content)} chars\")\n```\n\n\u003e Output: A dictionary with relative paths as keys and file contents as values, with progress printed to console.",
  "23f3526f6f6b82e16334b099782a1f1325389f60": "# main.py - File Summary\n\n**one_line**:  \nCLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.\n\n**Purpose of the file**:  \nThis script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.\n\n**Major functions/classes**:  \n- `main()`:  \n  - Sets up argument parsing using `argparse`.\n  - Validates and prepares inputs (source, patterns, token, language, etc.).\n  - Initializes the `shared` state dictionary passed through the flow.\n  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.\n- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).\n\n**Key technical details \u0026 TODOs**:  \n- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.\n- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).\n- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.\n- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.\n- **Language support**: `--language` parameter allows multilingual output (default: English).\n- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).\n- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.\n- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.\n- **TODOs (implied)**:  \n  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).\n  - No validation of language codes (assumes valid input).\n  - No progress tracking or verbose mode (could benefit from `--verbose` flag).\n\n**Short usage example**:  \n```bash\n# Generate tutorial from GitHub repo in French, with custom patterns\npython main.py \\\n  --repo https://github.com/user/repo \\\n  --token ghp_... \\\n  --output ./tutorials/repo \\\n  --include \"*.py\" \"*.md\" \\\n  --exclude \"tests/*\" \\\n  --language french \\\n  --max-abstractions 15 \\\n  --no-cache\n\n# Generate tutorial from local directory\npython main.py \\\n  --dir ./my_project \\\n  --name \"My Project\" \\\n  --output ./docs/tutorial \\\n  --language spanish\n```",
  "3a92062014cebdc54a96948293487f850d76996e": "# main.py - File Summary\n\n**one_line:**  \nCLI entry point for generating multi-language tutorials from GitHub repos or local codebases using a flow-based pipeline.\n\n**Purpose:**  \nThis file serves as the command-line interface (CLI) to trigger a tutorial generation workflow. It parses user inputs (source, patterns, language, caching, etc.), initializes a shared data context, and executes a modular flow (`create_tutorial_flow`) that processes the codebase, extracts abstractions, analyzes relationships, and generates a structured tutorial.\n\n**Major functions/classes:**  \n- `main()`:  \n  - Parses CLI arguments using `argparse`.\n  - Handles GitHub token resolution (from args or `GITHUB_TOKEN` env var).\n  - Constructs a `shared` dictionary to pass state through the flow.\n  - Initializes and runs the tutorial generation flow via `create_tutorial_flow()`.\n\n- **Key Imports:**\n  - `create_tutorial_flow` (from `flow`): The core pipeline orchestrating file fetching, abstraction, relationship mapping, and tutorial assembly.\n  - `dotenv`: Loads environment variables (e.g., `GITHUB_TOKEN`).\n  - `argparse`: Enables rich CLI argument parsing.\n\n**Key technical details \u0026 TODOs:**  \n- **Mutually exclusive sources**: Only one of `--repo` (GitHub URL) or `--dir` (local path) is allowed.\n- **File filtering**: Uses glob-style include/exclude patterns (e.g., `*.py`, `tests/*`) with sensible defaults to focus on source files and skip binaries/tests.\n- **Size limit**: Skips files larger than `--max-size` (default: 100KB) to avoid LLM context overflow.\n- **Multi-language support**: `--language` parameter passed through to LLM prompts (e.g., for non-English tutorials).\n- **LLM caching**: Enabled by default (`--no-cache` disables) to speed up repeated runs.\n- **Abstraction control**: `--max-abstractions` limits the number of high-level concepts extracted.\n- **Output structure**: Generated tutorials are written under `--output` directory (default: `./output`).\n- **TODOs (implied):**\n  - No validation of `--language` values (could add allowed list).\n  - No progress tracking/logging beyond initial print.\n  - No error handling for malformed URLs or invalid paths (defers to flow nodes).\n  - Could benefit from config file support (YAML/JSON) for complex runs.\n\n**Short usage example:**  \n```bash\n# From a GitHub repo (with token)\npython main.py --repo https://github.com/user/repo --token ghp_xxx --output ./tutorials --language spanish --no-cache\n\n# From a local directory (uses defaults for patterns/size)\npython main.py --dir ./my-project -n \"My Project\" --max-abstractions 5\n\n# With custom include/exclude patterns\npython main.py --repo https://github.com/user/repo -i \"*.py\" \"*.md\" -e \"tests/*\" \"*.log\" --max-size 50000\n```",
  "3cab1875146431bd401a2c4d5692853d54e1736d": "# **Chapter 4: Deep Dive ‚Äì Fetching the Codebase**  \n**Objective**: Understand how the system ingests and filters source code.\n\nBefore any AI can analyze a codebase, it must first **fetch** the right files ‚Äî not too many, not too few ‚Äî in a way that's efficient, safe, and reproducible. This chapter takes you behind the scenes of how the system retrieves code from **GitHub repositories** or **local directories**, applies intelligent filtering, and prepares it for analysis.\n\nWe‚Äôll explore:\n- How the system fetches code from GitHub or your local machine\n- How it filters files using patterns and `.gitignore`\n- How file size and structure are managed to avoid overload\n- The role of caching and resilience in the pipeline\n\nLet‚Äôs dive into the **core ingestion engine** of the tutorial generator.\n\n---\n\n## üîç **1. The Entry Point: `FetchRepo` Node**\n\nAt the start of every tutorial generation flow is the `FetchRepo` class in `nodes.py`. This is the **gatekeeper** of the pipeline ‚Äî responsible for retrieving source files and passing them downstream.\n\n```python\nclass FetchRepo(Node):\n    def prep(self, shared):\n        # Get configuration from shared context\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"dir\")\n        include_patterns = shared.get(\"include_patterns\", [\"*.py\", \"*.js\", \"*.ts\", \"*.go\", \"*.rs\", \"*.md\", \"*.txt\", \"*.html\", \"*.css\", \"*.json\", \"*.yaml\", \"*.yml\"])\n        exclude_patterns = shared.get(\"exclude_patterns\", [\"*test*\", \"*spec*\", \"*.log\", \"__pycache__/*\", \"node_modules/*\", \".git/*\", \".venv/*\", \"dist/*\", \"build/*\"])\n        max_file_size = shared.get(\"max_file_size\", 100 * 1024)  # 100 KB default\n        use_relative_paths = True\n\n        # Choose source: GitHub or local\n        if repo_url:\n            return crawl_github_files(\n                repo_url=repo_url,\n                token=shared.get(\"github_token\"),\n                max_file_size=max_file_size,\n                use_relative_paths=use_relative_paths,\n                include_patterns=set(include_patterns),\n                exclude_patterns=set(exclude_patterns)\n            )\n        else:\n            return crawl_local_files(\n                directory=local_dir,\n                include_patterns=set(include_patterns),\n                exclude_patterns=set(exclude_patterns),\n                max_file_size=max_file_size,\n                use_relative_paths=use_relative_paths\n            )\n```\n\n\u003e ‚úÖ **Key Insight**: `FetchRepo` is **source-agnostic**. Whether the code lives on GitHub or your laptop, the same filtering logic applies.\n\n---\n\n## üåê **2. Fetching from GitHub: Smart Crawling with API or Git**\n\nWhen a GitHub URL is provided (e.g., `--repo https://github.com/encode/fastapi`), the system uses `crawl_github_files.py` to retrieve files.\n\n### üîß **Two Modes of Operation**\n\n| Mode | When Used | How It Works |\n|------|-----------|-------------|\n| **GitHub API Mode (HTTPS URL)** | Default for public/private repos | Uses GitHub‚Äôs `/contents` REST API to list and download files recursively. Supports branches, commits, and subdirectories (e.g., `tree/main/src`). |\n| **Git Clone Mode (SSH URL)** | Fallback for SSH or complex cases | Clones the repo into a temporary directory using `git`, then reads files locally. Less efficient but more reliable for private repos with SSH access. |\n\n### üõ°Ô∏è **Security \u0026 Rate Limiting**\n- Uses `GITHUB_TOKEN` (from env or CLI) to access private repos and avoid rate limits.\n- Automatically pauses and retries when hitting GitHub API rate limits (403 responses).\n- Validates file size **before** downloading to prevent memory bloat.\n\n### üéØ **Pattern Filtering**\nFiles are filtered using **glob-style patterns**:\n```python\ninclude_patterns = {\"*.py\", \"*.md\"}\nexclude_patterns = {\"*test*\", \"*.log\"}\n```\nThis ensures only relevant source and documentation files are included ‚Äî skipping tests, logs, binaries, and build artifacts.\n\n\u003e üí° **Pro Tip**: You can customize these patterns via CLI flags:\n\u003e ```bash\n\u003e --include \"*.py\" \"*.md\" --exclude \"tests/*\" \"*.min.js\"\n\u003e ```\n\n---\n\n## üíæ **3. Fetching from Local Directories: `.gitignore`-Aware Crawling**\n\nWhen analyzing a local project",
  "45b73838c6ee216aa8db90f9907475e1459b3126": "Here's a **detailed chapter outline** for a **tutorial plan** on the codebase, designed to guide developers, contributors, or users through understanding, using, extending, and maintaining the system. The structure balances **technical depth**, **progressive learning**, and **practical application**, while aligning with the core abstractions you've provided.\n\n---\n\n# üìò **Tutorial Plan: Building AI-Powered Codebase Tutorials with PocketFlow**\n\n\u003e **Target Audience**: Developers, technical writers, AI/ML engineers, open-source contributors  \n\u003e **Prerequisites**: Basic Python, Git, understanding of LLMs, familiarity with software architecture concepts  \n\u003e **Goal**: Enable readers to understand, run, extend, and customize the tutorial generation pipeline\n\n---\n\n## **Chapter 1: Introduction \u0026 Vision**\n**Objective**: Set the stage for the project and its value proposition.\n\n- 1.1 What is this project?  \n  - AI-generated, pedagogically structured tutorials from any codebase\n  - Powered by PocketFlow and LLMs\n- 1.2 Why does this matter?  \n  - Onboarding new developers faster\n  - Automating documentation for complex systems\n  - Supporting multilingual and beginner-friendly explanations\n- 1.3 Real-world use cases  \n  - Open-source onboarding\n  - Internal developer training\n  - Educational platforms\n- 1.4 Key features overview  \n  - GitHub/local repo input\n  - Abstraction detection\n  - Pedagogical ordering\n  - Multilingual output\n  - Batch \u0026 retry support\n- 1.5 How this tutorial is structured  \n  - From high-level flow ‚Üí component deep dives ‚Üí customization ‚Üí deployment\n\n---\n\n## **Chapter 2: System Overview \u0026 High-Level Architecture**\n**Objective**: Introduce the end-to-end flow and how components interact.\n\n- 2.1 The TutorialFlow DAG (Directed Acyclic Graph)  \n  - Visual walkthrough of the workflow\n  - Node sequence: `FetchRepo ‚Üí IdentifyAbstractions ‚Üí AnalyzeRelationships ‚Üí OrderChapters ‚Üí WriteChapters ‚Üí CombineTutorial`\n- 2.2 Data flow via `shared` context  \n  - What is `shared`? How is data passed between nodes?\n  - Lifecycle of key data structures: `files`, `abstractions`, `relationships`, `chapters`, `chapter_contents`\n- 2.3 PocketFlow integration  \n  - Why PocketFlow? (Lightweight, composable, batchable)\n  - `Node`, `Flow`, `BatchNode` usage patterns\n- 2.4 Architecture diagram (with component interactions)\n- 2.5 Execution modes: CLI vs. programmatic\n- 2.6 Error handling \u0026 retry mechanisms\n\n---\n\n## **Chapter 3: Setting Up Your Environment**\n**Objective**: Prepare the reader to run and experiment with the system.\n\n- 3.1 Prerequisites \u0026 dependencies  \n  - Python 3.9+, `pip`, Git\n  - Required packages: `requests`, `pathspec`, `PyYAML`, `google-generativeai` / `anthropic`, etc.\n- 3.2 Installation \u0026 setup  \n  - Cloning the repo\n  - Installing dependencies (`pip install -r requirements.txt`)\n  - Setting up virtual environment (recommended)\n- 3.3 Configuring LLM providers  \n  - Setting API keys for Gemini, Claude, or others\n  - Using environment variables or config files\n- 3.4 Running the first example  \n  - Using `main.py` with a sample GitHub repo\n  - Expected output: Markdown tutorial in `/output/`\n- 3.5 Verifying the output\n\n---\n\n## **Chapter 4: Deep Dive ‚Äì Fetching the Codebase**\n**Objective**: Understand how the system ingests and filters source code.\n\n- 4.1 `FetchRepo` Node Overview  \n  - Role in the pipeline\n  - Inputs: GitHub URL or local path\n- 4.2 GitHub vs. Local Mode  \n  - `crawl_github_files.py`: cloning, filtering, size limits\n  - `crawl_local_files.py`: recursive traversal, `.gitignore` support\n- 4.3 File filtering logic  \n  - Glob patterns (e.g., `*.py`, `*.js`)\n  - Ignoring test files, assets, large binaries\n  - Using `pathspec` for `.gitignore`-style rules\n- 4.4 Size \u0026 memory constraints  \n  - Why limit file size?\n  - How it prevents OOM errors\n- 4.5 Output: List of `(path, content)` tuples  \n  - Structure and usage in downstream nodes\n- 4.6 Hands-on: Customize filtering rules\n ",
  "4f6ea5f24efa9fc1f47c11818c343f3a273fc3c7": "# call_llm.py - File Summary\n\n**one_line**:  \nUnified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).\n\n**Purpose**:  \nProvides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.\n\n**Major functions/classes**:\n- `call_llm(prompt: str, use_cache: bool = True) -\u003e str`  \n  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.\n- **Commented alternative implementations** for:\n  - Azure OpenAI\n  - Anthropic Claude 3.7 Sonnet (with extended thinking)\n  - OpenAI o1\n  - OpenRouter (supports any model via OpenRouter API)\n- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.\n- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.\n\n**Key technical details \u0026 TODOs**:\n- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.\n- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.\n- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.\n- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.\n- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `\"your-project-id\"`) ‚Äî **TODO: Remove or warn about insecure defaults**.\n- üß™ **Testing**: Includes a simple `__main__` block to test the function.\n- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.\n- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.\n- üîß **Configurable via env vars**:\n  - `LOG_DIR`: Log directory (default: `logs`)\n  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`\n  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`\n  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.\n- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.\n- üßπ **TODO**: Add error handling and retry logic for API failures.\n\n**Short usage example**:\n```python\nfrom utils.call_llm import call_llm\n\n# Set environment variables first (e.g., GEMINI_API_KEY)\nresponse = call_llm(\"Explain quantum computing in simple terms\")\nprint(response)\n\n# Disable cache for fresh call\nresponse_fresh = call_llm(\"Hello\", use_cache=False)\n```\n\n\u003e üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.",
  "5900e1e8ab32846d8cc73c5c879b2022c0ee20cc": "# Dockerfile - File Summary\n\n**one_line**: A minimal Docker setup to run a Python application with dependencies and Git support.\n\n**Purpose**:  \nThis Dockerfile defines a lightweight containerized environment for running a Python application. It uses the official `python:3.10-slim` base image to minimize size, installs Git for potential version control needs, installs Python dependencies, and runs the main application via `main.py`.\n\n**Major functions/classes**:  \n- *Base Image Initialization*: Uses `python:3.10-slim` for a small footprint.\n- *System Setup*: Updates package list, installs `git`, and cleans up to reduce image size.\n- *Dependency Installation*: Copies and installs Python packages from `requirements.txt`.\n- *Application Deployment*: Copies all source code into the container.\n- *Entrypoint*: Executes `main.py` as the default command.\n\n**Key technical details \u0026 TODOs**:  \n- **Base Image**: `python:3.10-slim` ‚Äî lightweight, Debian-based, suitable for production.\n- **Git Installation**: Required if the app or dependencies interact with Git (e.g., cloning private repos or version tracking).\n- **No Cache**: Both `apt` and `pip` use `--no-cache-dir` and cleanup to reduce image size.\n- **Work Directory**: `/app` is set as the working directory.\n- **Security**: Consider using a non-root user (TODO: add `USER` directive for production hardening).\n- **Multi-stage Build**: Not used ‚Äî consider if build-time dependencies (e.g., compilers) are added later.\n- **.dockerignore**: Not referenced ‚Äî recommended to add a `.dockerignore` file to prevent unnecessary files (e.g., `.git`, `__pycache__`) from being copied.\n- **Environment Variables**: None defined ‚Äî add if needed for configuration (e.g., `ENV APP_ENV=production`).\n\n**Short usage example**:  \n```bash\n# Build the image\ndocker build -t pocketflow-app .\n\n# Run the container\ndocker run --rm pocketflow-app\n```\n\n\u003e Ensure `requirements.txt` and `main.py` exist in the build context (project root).",
  "6d0fdd6ff671a39befa03a48b0bb558f72f08e73": "# nodes.py - File Summary\n\n### one_line  \nA collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.\n\n---\n\n### Purpose  \nThis file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.\n\n---\n\n### Major Functions/Classes\n\n| Class | Role |\n|------|------|\n| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |\n| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |\n| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |\n| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |\n| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |\n\n---\n\n### Key Technical Details \u0026 TODOs\n\n- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.\n- **YAML Parsing \u0026 Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.\n- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).\n- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.\n- **File Handling**: \n  - Converts fetched files to list of `(path, content)` tuples.\n  - Uses **index-based referencing** in LLM prompts to avoid path duplication.\n- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.\n- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.\n\n#### ‚úÖ TODOs / Improvements (inferred):\n- [ ] Add **token budgeting** for large codebases in LLM context creation.\n- [ ] Support **chunking or summarization** of very large files before LLM ingestion.\n- [ ] Allow **custom prompt templates** per language or project type.\n- [ ] Add **retry logic with exponential backoff** for LLM calls.\n- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.\n- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).\n\n---\n\n### Short Usage Example\n\n```python\nfrom pocketflow import Flow\nfrom nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters\n\n# Shared context\nshared = {\n    \"repo_url\": \"https://github.com/user/example-project\",\n    \"include_patterns\": [\"*.py\", \"*.js\"],\n    \"exclude_patterns\": [\"test*\", \"*.md\"],\n    \"max_file_size\": 50000,\n    \"language\": \"english\",\n    \"use_cache\": True,\n    \"max_abstraction_num\": 8\n}\n\n# Build flow\nflow = Flow(\n    FetchRepo() \u003e\u003e\n    IdentifyAbstractions() \u003e\u003e\n    AnalyzeRelationships() \u003e\u003e\n    OrderChapters()\n)\n\n# Run\nflow.run(shared)\n\n# Access results\nprint(\"Project Summary:\", shared[\"relationships\"][\"summary\"])\nprint(\"Abstractions:\", shared[\"abstractions\"])\nprint(\"Relationships:\", shared[\"relationships\"][\"details\"])\nprint(\"Chapter Order:\", shared[\"chapters\"])  # List of abstraction indices in order\n```\n\n\u003e üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.",
  "6e82d3f8e0eda7995e4690e668e857ca4312480c": "# **Chapter 2: System Overview \u0026 High-Level Architecture**\n\n## **Objective**\nIn this chapter, you‚Äôll gain a comprehensive understanding of how the **AI-Powered Codebase Tutorial Generator** works end-to-end. We‚Äôll walk through the high-level architecture, explain the core components, and show how they interact to transform any codebase ‚Äî whether from GitHub or your local machine ‚Äî into a beginner-friendly, multilingual tutorial with visualizable structure.\n\nBy the end of this chapter, you‚Äôll understand:\n- The **overall data flow** from code input to final tutorial output.\n- The **modular design** powered by *PocketFlow* agents.\n- The **key architectural abstractions** and their responsibilities.\n- How **LLMs are used** to reason about code, not just generate text.\n- How to **extend or customize** the system for your own use cases.\n\n---\n\n## **2.1 End-to-End Flow: From Code to Tutorial**\n\nLet‚Äôs start with the big picture. Here‚Äôs what happens when you run:\n\n```bash\npython main.py --repo https://github.com/encode/fastapi --language english\n```\n\n### üîÅ **Step-by-Step Pipeline**\n\n| Step | Component | What It Does |\n|------|---------|-------------|\n| 1Ô∏è‚É£ | `FetchRepo` | Downloads or reads the codebase using pattern filters and size limits. |\n| 2Ô∏è‚É£ | `IdentifyAbstractions` | Uses an LLM to detect core concepts (e.g., `FastAPI`, `APIRouter`, `Dependency`) in the code. |\n| 3Ô∏è‚É£ | `AnalyzeRelationships` | Asks the LLM how these abstractions relate (e.g., \"`APIRouter` is used by `FastAPI`\"). |\n| 4Ô∏è‚É£ | `OrderChapters` | Determines a logical learning order (e.g., start with `FastAPI`, then `APIRouter`). |\n| 5Ô∏è‚É£ | `WriteChapters` *(Batch)* | Generates detailed, beginner-friendly explanations for each abstraction ‚Äî in parallel. |\n| 6Ô∏è‚É£ | `CombineTutorial` | Merges all chapters into a single structured document (Markdown/HTML), including summary and metadata. |\n\n\u003e üí° **All steps are orchestrated by `TutorialFlow`**, a PocketFlow-based DAG (Directed Acyclic Graph) defined in `flow.py`.\n\n---\n\n## **2.2 High-Level Architecture Diagram**\n\n```\n+------------------+\n|   User Input     |\n| (CLI: repo/dir)  |\n+--------+---------+\n         |\n         v\n+------------------+\n|  main.py         | \u003c--- Entrypoint: Parses args, sets up shared context\n+--------+---------+\n         |\n         v\n+------------------+\n|  TutorialFlow    | \u003c--- PocketFlow orchestrator (flow.py)\n+--------+---------+\n         |\n         +------------------\u003e [FetchRepo]\n         |                         |\n         |                         v\n         |                [IdentifyAbstractions] ‚Üí LLM ‚Üí YAML\n         |                         |\n         |                         v\n         |                [AnalyzeRelationships] ‚Üí LLM ‚Üí Summary + Graph\n         |                         |\n         |                         v\n         |                [OrderChapters] ‚Üí LLM ‚Üí Chapter Order\n         |                         |\n         |                         v\n         |                [WriteChapters] ‚Üí LLM (Batch) ‚Üí Per-chapter content\n         |                         |\n         |                         v\n         +----------------\u003e [CombineTutorial] ‚Üí Final Tutorial (MD/HTML)\n                                   |\n                                   v\n                         +------------------+\n                         |  ./output/       | ‚Üê Generated tutorial\n                         |  (by language)   |\n                         +------------------+\n```\n\n\u003e üîó **Data flows via a shared dictionary** (`shared`) passed between nodes. This includes:\n\u003e - `files`: List of `(path, content)` tuples\n\u003e - `abstractions`: Extracted concepts\n\u003e - `relationships`: How they interact\n\u003e - `chapters`: Ordered list of abstraction indices\n\u003e - `chapter_contents`: Generated explanations\n\u003e - `project_summary`, `language`, `repo_url`, etc.\n\n---\n\n## **2.3 Core Architectural Abstractions**\n\nLet‚Äôs dive into the **7 key components** that make this system powerful, modular, and extensible.\n\n### **1. `TutorialFlow` ‚Äì The Orchestrator**\n- **Defined in**: `flow.py`, `main.py`\n- **Framework**: [PocketFlow](https://github.com/The-Pocket/PocketFlow) (lightweight agentic workflow engine)\n- **Role**: Chains nodes together using `\u003e\u003e` syntax and manages execution order.\n- **Features**:\n  -",
  "72c0fd4c1888a3ec87ea4745729f0d2e9ae5cfd1": "# call_llm.py - File Summary\n\n**one_line**: A utility module to call LLMs (currently Google Gemini) with logging, caching, and multi-provider support.\n\n**Purpose**:  \nThis file provides a unified, reusable interface for calling Large Language Models (LLMs) with built-in **logging**, **disk-based caching**, and **multi-provider extensibility**. It's designed to be used across a codebase where consistent LLM interaction, cost optimization (via cache), and auditability (via logs) are important.\n\n**Major functions/classes**:\n- `call_llm(prompt: str, use_cache: bool = True) -\u003e str`:  \n  Main function to send a prompt to the configured LLM and return the response. Handles caching, logging, and API calls.  \n  Currently **active implementation uses Google Gemini via API key** (AI Studio). Other providers are commented out but available for switching.\n\n**Key technical details \u0026 TODOs**:\n- ‚úÖ **Logging**: All prompts and responses are logged to a daily file (`logs/llm_calls_YYYYMMDD.log`) with timestamps and levels.\n- ‚úÖ **Caching**: Uses a simple JSON file (`llm_cache.json`) to cache responses by prompt. Prevents redundant API calls.\n- üîÅ **Cache Safety**: On write, reloads cache before saving to avoid race conditions (best-effort).\n- ‚òÅÔ∏è **LLM Provider**: Default is **Google Gemini (2.5-pro)** via API key (AI Studio).  \n  - Alternative: Vertex AI (commented out) ‚Äì requires project ID and location (TODO: update env vars).\n  - üîå **Extensible**: Includes *fully commented-out* implementations for:\n    - Azure OpenAI\n    - Anthropic Claude (3.7 Sonnet with extended thinking)\n    - OpenAI o1\n    - OpenRouter (with full error handling)\n- üîê **Security**: Relies on environment variables:\n  - `GEMINI_API_KEY` (or `OPENROUTER_API_KEY`, etc.)\n  - `GEMINI_MODEL` (default: `gemini-2.5-pro`)\n  - `LOG_DIR` (default: `logs`)\n- ‚ö†Ô∏è **TODO**: Update Vertex AI config with real `GEMINI_PROJECT_ID` and `GEMINI_LOCATION` if using cloud version.\n- üßπ **Improvement**: Cache could be enhanced with TTL, size limits, or in-memory layer.\n\n**Short usage example**:\n```python\nfrom utils.call_llm import call_llm\n\n# Simple call (uses cache by default)\nresponse = call_llm(\"Explain quantum computing in one sentence.\")\nprint(response)\n\n# Bypass cache for fresh response\nfresh_response = call_llm(\"What's the weather today?\", use_cache=False)\n```\n\n\u003e üí° Tip: Switch providers by commenting/uncommenting the relevant `call_llm` function. Only one should be active at a time.",
  "9783a9dd9834bd80f91c2134f2e23d97869e671c": "```markdown\n# Core Architectural Abstractions \u0026 Components\n\nBelow are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.\n\n---\n\n## 1. **TutorialFlow**\n**Description**:  \nOrchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.\n\n**Files**:  \n- `flow.py`, `main.py`, `nodes.py`\n\n**Dependencies**:  \n- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`\n\n**Responsibilities**:  \n- Defines and executes the sequential workflow (DAG) for tutorial generation.\n- Manages data flow between nodes via the `shared` context.\n- Enables retry logic and batch processing for robustness and performance.\n\n---\n\n## 2. **FetchRepo**\n**Description**:  \nRetrieves source code from a GitHub repository or local directory with filtering and size constraints.\n\n**Files**:  \n- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`\n\n**Dependencies**:  \n- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`\n\n**Responsibilities**:  \n- Parses repo URL or local path input.\n- Downloads and filters files using glob patterns and `.gitignore`.\n- Enforces maximum file size to prevent memory overflow.\n- Returns structured list of `(path, content)` tuples for downstream processing.\n\n---\n\n## 3. **IdentifyAbstractions**\n**Description**:  \nUses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.\n\n**Files**:  \n- `nodes.py` (class: `IdentifyAbstractions`)\n\n**Dependencies**:  \n- `call_llm`, `utils.read_json`, `yaml`, `shared` context\n\n**Responsibilities**:  \n- Constructs prompts with code context (by file index) for LLM.\n- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.\n- Validates output structure and enforces limits (e.g., max 10 abstractions).\n- Stores results in `shared[\"abstractions\"]`.\n\n---\n\n## 4. **AnalyzeRelationships**\n**Description**:  \nAnalyzes how identified abstractions interact and generates a project summary and relationship map.\n\n**Files**:  \n- `nodes.py` (class: `AnalyzeRelationships`)\n\n**Dependencies**:  \n- `call_llm`, `get_content_for_indices`, `shared` context\n\n**Responsibilities**:  \n- Builds LLM prompt with abstractions and relevant file snippets.\n- Extracts project summary and pairwise relationships (e.g., \"X depends on Y\").\n- Outputs structured YAML into `shared[\"relationships\"]` for tutorial structuring.\n\n---\n\n## 5. **OrderChapters**\n**Description**:  \nDetermines a pedagogical order for tutorial chapters based on abstraction relationships.\n\n**Files**:  \n- `nodes.py` (class: `OrderChapters`)\n\n**Dependencies**:  \n- `call_llm`, `shared` context (abstractions, relationships)\n\n**Responsibilities**:  \n- Uses LLM to reason about dependencies and learning sequence.\n- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).\n- Stores result in `shared[\"chapters\"]`.\n\n---\n\n## 6. **WriteChapters**\n**Description**:  \nGenerates written content for each chapter (abstraction) using LLM, in a target language.\n\n**Files**:  \n- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)\n\n**Dependencies**:  \n- `call_llm`, `get_content_for_indices`, `shared` context\n\n**Responsibilities**:  \n- Processes multiple abstractions in parallel (batch mode).\n- For each chapter, generates beginner-friendly explanations with code examples.\n- Supports multilingual output via language parameter.\n- Stores chapter content in `shared[\"chapter_contents\"]`.\n\n---\n\n## 7. **CombineTutorial**\n**Description**:  \nMerges all generated chapters into a single, structured tutorial document.\n\n**Files**:  \n- `nodes.py` (class: `CombineTutorial`)\n\n**Dependencies**:  \n- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)\n\n**Responsibilities**:  \n- Combines chapter content in the ordered sequence.\n- Adds metadata (title, language, summary).\n- Outputs final tutorial (Markdown/HTML) to specified output directory.\n\n---\n\n## 8. **CallLLM**\n**Description**:  \nUnified interface to call various LLMs (Gemini, Claude",
  "a0f4b8d241396f0d5fa2bc9f50d03568e92c6faf": "# __init__.py - File Summary\n\n**one_line**:  \nUtility module initializer that imports and exposes key functions for shared use across the project.\n\n**Purpose**:  \nThis `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).\n\n**Major functions/classes**:  \n- Exposes the following (imported from submodules):\n  - `log` ‚Äì Standardized logging function with timestamps and levels.\n  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.\n  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.\n  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).\n  - `Timer` ‚Äì Context manager for measuring execution time.\n\n**Key technical details \u0026 TODOs**:  \n- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.\n- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.\n- All exposed utilities are stateless and thread-safe.\n- **TODO**: Add type hints to all public functions in submodules.\n- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.\n\n**Short usage example**:  \n```python\nfrom utils import log, retry, read_json, Timer\n\n@retry(max_attempts=3)\ndef fetch_data():\n    log(\"Fetching data...\")\n    with Timer(\"Data load\"):\n        return read_json(\"data.json\")\n\ndata = fetch_data()\n```",
  "a31b318cf9b6b73e2bdd92f933d434b750d64a8b": "# nodes.py - File Summary\n\n## one_line  \nA collection of PocketFlow nodes for fetching, analyzing, and organizing codebase abstractions and relationships to generate beginner-friendly documentation.\n\n## Purpose  \nThis file defines a pipeline of `Node` and `BatchNode` classes that:\n- Fetch files from a GitHub repo or local directory.\n- Identify core abstractions in the codebase using LLM.\n- Analyze relationships between abstractions and generate a project summary.\n- Order abstractions into a logical learning sequence (\"chapters\").\nUsed to automatically generate structured, human-readable documentation for codebases.\n\n## Major functions/classes  \n- **`FetchRepo`** (`Node`):  \n  Fetches source code from either a GitHub repo or local directory, filtering by file patterns and size.\n\n- **`IdentifyAbstractions`** (`Node`):  \n  Uses an LLM to identify the top N most important abstractions (core concepts), their descriptions, and associated file indices.\n\n- **`AnalyzeRelationships`** (`Node`):  \n  Analyzes how abstractions interact (e.g., \"Manages\", \"Uses\") and generates a high-level project summary.\n\n- **`OrderChapters`** (`Node`):  \n  Orders abstractions into a logical sequence for documentation (e.g., for tutorials or guides).\n\n- **`get_content_for_indices()`** (helper):  \n  Extracts file contents for specified indices, used to build LLM context.\n\n## Key technical details \u0026 TODOs  \n- **LLM Integration**: Uses `call_llm()` from `utils.call_llm`; supports caching via `use_cache` (enabled by default).\n- **YAML Parsing**: LLM outputs are expected in YAML format; strict validation is performed on structure and data types.\n- **Language Support**: All LLM-generated fields (`name`, `description`, `summary`, `label`) support non-English languages via `language` flag.\n- **File Indexing**: Files are referenced by index (not path) in abstraction and relationship data to reduce context size.\n- **Error Handling**: Raises `ValueError` for invalid LLM outputs, missing keys, or out-of-bounds file indices.\n- **Retry Logic**: Caching is disabled on retries (`use_cache and self.cur_retry == 0`) to avoid stale results.\n- **TODOs**:\n  - Improve file context sampling for large repos (e.g., chunking, summarization).\n  - Add support for more advanced relationship types (e.g., inheritance, event flow).\n  - Allow custom abstraction prompts or templates.\n  - Add fallback strategies for LLM parsing errors.\n\n## Short usage example  \n```python\nfrom pocketflow import Flow\nfrom nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters\n\nshared = {\n    \"repo_url\": \"https://github.com/user/repo\",\n    \"include_patterns\": [\"*.py\", \"*.js\"],\n    \"exclude_patterns\": [\"test*\", \"node_modules\"],\n    \"max_file_size\": 50000,\n    \"github_token\": \"your_gh_token\",\n    \"language\": \"spanish\",  # Optional: generate docs in Spanish\n    \"max_abstraction_num\": 8,\n    \"use_cache\": True\n}\n\nflow = Flow(\n    FetchRepo() \u003e\u003e\n    IdentifyAbstractions() \u003e\u003e\n    AnalyzeRelationships() \u003e\u003e\n    OrderChapters()\n)\n\nflow.run(shared)\n\n# Result:\n# shared[\"abstractions\"] = list of core concepts\n# shared[\"relationships\"] = {\"summary\": str, \"details\": list of relationships}\n# shared[\"chapter_order\"] = list of abstraction indices in order\n```",
  "afa7f02d9515d003064d5cc4f0f1f7f02288547b": "```markdown\n# Core Architectural Abstractions \u0026 Components\n\nBelow are up to 15 core abstractions/components identified in the codebase, based on structure, responsibilities, and inter-component relationships.\n\n---\n\n## 1. **TutorialFlow Orchestrator**\n- **Description**: Central pipeline that coordinates the end-to-end tutorial generation process.\n- **Files**: `flow.py`, `main.py`\n- **Dependencies**: `nodes.py`, `utils/call_llm.py`, `pocketflow.Flow`\n- **Responsibilities**: \n  - Defines and executes a linear workflow for fetching, analyzing, and generating tutorials.\n  - Manages node execution order and retry logic.\n  - Integrates with CLI and shared state for data flow.\n\n---\n\n## 2. **FetchRepo Node**\n- **Description**: Fetches source code from GitHub repositories or local directories.\n- **Files**: `nodes.py` (class `FetchRepo`)\n- **Dependencies**: `utils/crawl_github_files.py`, `utils/crawl_local_files.py`, `os`, `argparse`\n- **Responsibilities**:\n  - Downloads or reads files from a specified source (GitHub or local).\n  - Applies file filtering (include/exclude) and size limits.\n  - Populates shared context with file list and contents.\n\n---\n\n## 3. **IdentifyAbstractions Node**\n- **Description**: Uses LLM to detect key architectural abstractions in the codebase.\n- **Files**: `nodes.py` (class `IdentifyAbstractions`)\n- **Dependencies**: `utils/call_llm.py`, `yaml`\n- **Responsibilities**:\n  - Sends file contents to LLM to extract core concepts (e.g., modules, classes, patterns).\n  - Returns a list of abstractions with names, descriptions, and file indices.\n  - Limits output to a configurable number (`max_abstraction_num`).\n\n---\n\n## 4. **AnalyzeRelationships Node**\n- **Description**: Analyzes interactions between identified abstractions and generates a project summary.\n- **Files**: `nodes.py` (class `AnalyzeRelationships`)\n- **Dependencies**: `utils/call_llm.py`, `get_content_for_indices()`\n- **Responsibilities**:\n  - Identifies relationships (e.g., \"uses\", \"manages\") between abstractions.\n  - Generates a high-level summary of the project's architecture.\n  - Outputs structured YAML for downstream use.\n\n---\n\n## 5. **OrderChapters Node**\n- **Description**: Orders abstractions into a logical learning sequence for tutorial chapters.\n- **Files**: `nodes.py` (class `OrderChapters`)\n- **Dependencies**: `utils/call_llm.py`\n- **Responsibilities**:\n  - Determines pedagogical order (e.g., foundational ‚Üí advanced).\n  - Returns ordered list of abstraction indices for chapter generation.\n\n---\n\n## 6. **WriteChapters Node**\n- **Description**: Generates detailed content for each tutorial chapter using abstractions and relationships.\n- **Files**: `nodes.py` (class `WriteChapters`, `BatchNode`)\n- **Dependencies**: `utils/call_llm.py`, `get_content_for_indices()`\n- **Responsibilities**:\n  - Writes chapter content in a beginner-friendly style.\n  - Supports multi-language output via `language` parameter.\n  - Processes multiple chapters in batch/parallel mode.\n\n---\n\n## 7. **CombineTutorial Node**\n- **Description**: Assembles all generated chapters into a final tutorial document.\n- **Files**: `nodes.py` (class `CombineTutorial`)\n- **Dependencies**: None (final sink in flow)\n- **Responsibilities**:\n  - Merges chapters into a single, structured tutorial (Markdown/HTML).\n  - Applies formatting, headings, and visualizations (e.g., Mermaid diagrams).\n  - Writes output to specified directory.\n\n---\n\n## 8. **LLM Interface (call_llm)**\n- **Description**: Unified interface for calling Large Language Models with caching and logging.\n- **Files**: `utils/call_llm.py`\n- **Dependencies**: `os`, `json`, `logging`, `google.generativeai` (Gemini), optional: Anthropic, OpenAI\n- **Responsibilities**:\n  - Provides a consistent `call_llm(prompt)` function across the codebase.\n  - Caches responses to reduce cost and latency.\n  - Logs all LLM interactions for debugging and audit.\n  - Supports multiple LLM providers (Gemini, Claude, OpenAI, etc.).\n\n---\n\n## 9. **GitHub File Crawler**\n- **Description**: Crawls GitHub repositories via API or SSH to retrieve source files.\n- **Files**: `utils/crawl_github_files.py`\n- **Dependencies**: `",
  "b9b84a10d143a337a5134f1a4bb79fef9c48c841": "# README.md - File Summary\n\n## one_line\nA tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.\n\n## Purpose of the file\nThe `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.\n\n## Major functions/classes\n- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.\n- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.\n- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.\n- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).\n- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.\n- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).\n\n## Key technical details \u0026 TODOs\n- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.\n- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.\n- **Language support**: Tutorials can be generated in any language (`--language` flag).\n- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).\n- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.\n- **Docker support**: Fully containerized with volume mounts for output and code input.\n- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.\n- **TODOs / Improvements (implied)**:\n  - Add support for more visualization types (e.g., Mermaid diagrams in output).\n  - Improve handling of large or complex codebases (e.g., better abstraction summarization).\n  - Enhance caching strategy (e.g., persistent cache across runs).\n  - Support multi-repo or monorepo analysis.\n\n## Short usage example\n```bash\n# Analyze a GitHub repo and generate an English tutorial\npython main.py --repo https://github.com/encode/fastapi --include \"*.py\" --exclude \"tests/*\" --max-size 100000\n\n# Analyze a local directory and generate a Chinese tutorial\npython main.py --dir ./my-project --include \"*.js\" \"*.ts\" --language \"Chinese\"\n\n# Run in Docker (requires GEMINI_API_KEY)\ndocker run -it --rm \\\n  -e GEMINI_API_KEY=\"your_key\" \\\n  -v \"$(pwd)/output\":/app/output \\\n  pocketflow-app --repo https://github.com/pallets/flask\n```\n\n\u003e ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.",
  "c7d60c94c9d93e62c3ce3445889d817ed78c0a31": "# README.md - File Summary\n\n## one_line\n**AI-powered tool that turns any GitHub or local codebase into a beginner-friendly, well-structured tutorial using LLM analysis.**\n\n## Purpose of the file\nThis `README.md` serves as the **project documentation and landing page** for an AI-driven tutorial generator that automatically analyzes codebases (from GitHub or local directories), identifies core abstractions, and produces human-readable, educational tutorials with visualizations. It explains how to set up, configure, and run the system locally or via Docker, lists example outputs for popular open-source projects, and links to tutorials, videos, and an online demo.\n\n## Major functions/classes\n- **Main entry point (`main.py`)** ‚Äì Parses CLI arguments, orchestrates crawling, analysis, and tutorial generation.\n- **Repository Crawler** ‚Äì Fetches and filters files from GitHub repos or local directories based on include/exclude patterns and file size limits.\n- **Codebase Analyzer** ‚Äì Uses LLMs to identify core abstractions, module interactions, and architectural patterns.\n- **Tutorial Generator** ‚Äì Transforms analyzed code into beginner-friendly explanations with diagrams and step-by-step guides.\n- **LLM Interface (`utils/call_llm.py`)** ‚Äì Wrapper for calling LLM APIs (supports Gemini, Claude, etc.) with caching and error handling.\n- **Abstraction Engine** ‚Äì Limits and structures the number of high-level concepts extracted from the codebase.\n- **Multi-language Support** ‚Äì Generates tutorials in different languages (e.g., English, Chinese) via LLM translation.\n- **Docker Support** ‚Äì Enables containerized execution with environment variable-based configuration.\n\n## Key technical details \u0026 TODOs\n### üîß **Technical Highlights**\n- Built on **Pocket Flow**, a lightweight 100-line LLM orchestration framework.\n- Uses **LLM caching** by default to reduce cost and improve performance.\n- Supports **large language models with reasoning/thinking capabilities** (e.g., Gemini 2.5, Claude 3.7, O1).\n- Accepts **flexible file filtering** (`--include`, `--exclude`) and size limits (`--max-size`).\n- Outputs **Markdown + HTML-ready tutorials** with embedded diagrams (Mermaid.js likely used internally).\n- Designed for **agentic development** ‚Äì humans design, AI agents write code (via Cursor AI or similar).\n\n### ‚úÖ **Current Features**\n- GitHub \u0026 local codebase support.\n- Multi-language tutorial generation.\n- Dockerized deployment.\n- Caching of LLM responses.\n- Rich CLI interface with intuitive flags.\n- Live online service: [code2tutorial.com](https://code2tutorial.com/)\n\n### üöß **TODOs / Future Work (implied)**\n- Expand model support beyond Gemini (e.g., OpenAI, Ollama, etc.) ‚Äì currently requires manual config.\n- Improve diagram generation (e.g., dynamic Mermaid, PlantUML).\n- Add support for more tutorial formats (PDF, Jupyter notebooks).\n- Enhance abstraction detection with graph-based analysis.\n- Allow user feedback loop to improve generated tutorials.\n- Integrate with IDE plugins (VSCode, PyCharm).\n- Scale to larger repos with incremental analysis.\n\n## Short usage example\n```bash\n# Analyze a public GitHub repo and generate an English tutorial\npython main.py \\\n  --repo https://github.com/fastapi/fastapi \\\n  --include \"*.py\" \\\n  --exclude \"tests/*\" \"docs/*\" \\\n  --max-size 50000 \\\n  --output ./tutorials/fastapi\n\n# Generate a Chinese tutorial for a local project\npython main.py \\\n  --dir ./my-python-app \\\n  --include \"*.py\" \"*.md\" \\\n  --language \"Chinese\" \\\n  --max-abstractions 8\n\n# Run in Docker with API keys\ndocker run -it --rm \\\n  -e GEMINI_API_KEY=\"your-key-here\" \\\n  -v \"$(pwd)/output\":/app/output \\\n  pocketflow-app --repo https://github.com/langchain-ai/langchain\n```\n\n\u003e üí° *After running, find the generated tutorial in the specified output folder ‚Äî includes explanations, architecture insights, and visualizations!*",
  "ce3a814f1c6829700415ef0ab9f9c3875bfa7d21": "# **Tutorial Plan: Building \u0026 Extending a Codebase Tutorial Generator**\n\nThis tutorial plan is designed to guide developers, educators, and contributors through the **architecture, components, and extensibility** of a **codebase tutorial generation system** built using a modular, LLM-driven pipeline. The goal is to provide a **comprehensive understanding** of how the system works, how to use it, how to customize it, and how to extend it for new use cases.\n\nThe tutorial is structured into **10 detailed chapters**, progressing from **high-level concepts** to **deep technical implementation**, with hands-on exercises, best practices, and extension points.\n\n---\n\n## **Chapter 1: Introduction \u0026 System Overview**\n\n### **1.1 Purpose of the Tutorial Generator**\n- What problem does this tool solve?\n- Use cases: onboarding, documentation, education, open-source contribution.\n- Why automate tutorial generation?\n\n### **1.2 High-Level Workflow**\n- Visual diagram of the **TutorialFlow Orchestrator**.\n- Step-by-step breakdown: Fetch ‚Üí Abstraction ‚Üí Relationships ‚Üí Order ‚Üí Write ‚Üí Combine.\n- Emphasis on **modular design** and **LLM integration**.\n\n### **1.3 Key Features**\n- Support for **GitHub + local repos**.\n- **Pedagogical ordering** of concepts.\n- **Multi-language output** (Markdown, HTML).\n- **Caching and cost optimization**.\n- **Extensible node system**.\n\n### **1.4 Prerequisites \u0026 Setup**\n- Required tools: Python 3.9+, Git, API keys (Gemini/Claude/OpenAI).\n- Installing dependencies: `pip install -r requirements.txt`.\n- Running the first example: `python main.py --url https://github.com/user/repo --output tutorial/`.\n\n\u003e ‚úÖ **Exercise**: Run the tool on a small, well-documented repository (e.g., a Flask app).\n\n---\n\n## **Chapter 2: Core Architecture \u0026 Design Patterns**\n\n### **2.1 The PocketFlow Framework**\n- Overview of **PocketFlow**: lightweight workflow engine.\n- Key abstractions: `Node`, `Flow`, `BatchNode`.\n- Why PocketFlow? (Simplicity, control, debuggability).\n\n### **2.2 The TutorialFlow Orchestrator**\n- Role of `flow.py` and `main.py`.\n- How the `Flow` object chains nodes.\n- Shared data context: `self.shared` dictionary.\n- Retry logic and error handling.\n\n### **2.3 Data Flow \u0026 State Management**\n- How data moves between nodes.\n- Structure of the shared state (e.g., `file_list`, `abstractions`, `chapters`).\n- Immutability vs. mutation: best practices.\n\n### **2.4 Modularity \u0026 Dependency Injection**\n- Nodes as independent, testable units.\n- Dependency injection via constructor args (e.g., `max_abstraction_num`).\n- Loose coupling via shared context.\n\n\u003e ‚úÖ **Exercise**: Trace the data flow from `FetchRepo` to `CombineTutorial` using a print-debug approach.\n\n---\n\n## **Chapter 3: Source Code Ingestion ‚Äì FetchRepo Node**\n\n### **3.1 Overview of `FetchRepo`**\n- Dual mode: GitHub (API/SSH) vs. local directory.\n- File filtering: include/exclude patterns, size limits.\n\n### **3.2 GitHub File Crawler (`crawl_github_files.py`)**\n- Using GitHub API (with token) or SSH cloning.\n- Rate limiting, pagination, error handling.\n- Extracting file metadata: path, size, language.\n\n### **3.3 Local File Crawler (`crawl_local_files.py`)**\n- Walking directory trees with `os.walk`.\n- Handling symlinks, hidden files, binary files.\n- Size-based filtering to avoid LLM context overload.\n\n### **3.4 Populating the Shared Context**\n- Storing `file_list` as list of dicts: `{path, content, size, lang}`.\n- Truncating large files to fit LLM context window.\n\n\u003e ‚úÖ **Exercise**: Modify `FetchRepo` to exclude test files and add support for `.env` filtering.\n\n---\n\n## **Chapter 4: Abstraction Detection ‚Äì IdentifyAbstractions Node**\n\n### **4.1 The Challenge of Code Abstraction**\n- Why manual abstraction is hard.\n- Role of LLMs in identifying **core components, patterns, and concepts**.\n\n### **4.2 Prompt Design**\n- Structure of the prompt: file list + instructions.\n- Example prompt: _\"List up to 10 core abstractions in this codebase...\"_\n- Using YAML for structured output.\n\n### **4.3 LLM Response Handling**\n- Parsing YAML output with error recovery.\n- Validating required fields: `name`, `description`, `files`.\n- Limiting",
  "d3c0636fd7491b2648ea600abc749686b1b6b856": "# crawl_local_files.py - File Summary\n\n### one_line\nCrawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.\n\n---\n\n### Purpose\nThis utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.\n\n---\n\n### Major functions/classes\n\n- **`crawl_local_files()`**  \n  Main function that:\n  - Walks a directory tree (`os.walk`)\n  - Applies `.gitignore` rules (if present)\n  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`\n  - Enforces a maximum file size\n  - Reads and returns file contents as a dictionary\n  - Shows real-time progress with colored output\n\n---\n\n### Key technical details \u0026 TODOs\n\n#### ‚úÖ **Features**\n- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).\n- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).\n- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).\n- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\\033[92m`).\n- **Relative Paths**: Optionally returns paths relative to input directory.\n- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.\n- **Error Handling**: Gracefully skips unreadable files and logs warnings.\n\n#### ‚ö†Ô∏è **Limitations / TODOs**\n- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).\n- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.\n- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.\n- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.\n- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.\n\n\u003e üîß **Suggested Improvements (TODOs):**\n\u003e - Add `verbose`/`quiet` mode flag to control output.\n\u003e - Add option to detect and skip binary files.\n\u003e - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).\n\u003e - Return metadata (size, mtime) alongside content.\n\u003e - Allow custom file read handlers (e.g., for line filtering).\n\n---\n\n### Short usage example\n\n```python\n# Crawl current directory, include only Python files, exclude test and cache dirs\nresult = crawl_local_files(\n    directory=\".\",\n    include_patterns={\"*.py\", \"*.md\"},\n    exclude_patterns={\"tests/*\", \"__pycache__/*\", \"*.log\"},\n    max_file_size=1024 * 1024,  # 1 MB limit\n    use_relative_paths=True\n)\n\n# Print file paths\nfor path in result[\"files\"]:\n    print(path)\n\n# Use file content\nfor path, content in result[\"files\"].items():\n    print(f\"--- {path} ---\\n{content[:200]}...\\n\")\n```\n\n\u003e üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).",
  "d4fffeb438aa5fbc094d5542027aade5d0a81143": "# crawl_github_files.py - File Summary\n\n### one_line  \nA utility to crawl and download files from a GitHub repository (public or private) at a specific commit/branch/path via API or SSH, with filtering, size limits, and path control.\n\n---\n\n### Purpose  \nThis script enables programmatic retrieval of source files from a GitHub repository at a **specific commit and subdirectory**, supporting:\n- Public and private repositories (via token or SSH)\n- Filtering by file patterns (`include`/`exclude`)\n- Size limits to avoid large files\n- Relative or absolute path output\n- Resilience to GitHub rate limits\n\nIt is designed for use in automation pipelines, code analysis tools, or AI training data collection from GitHub.\n\n---\n\n### Major Functions/Classes\n\n#### `crawl_github_files(...) -\u003e dict`\nMain entry point. Crawls files from a GitHub repo using either:\n- **GitHub REST API** (for `https://` URLs)\n- **Git SSH clone** (for `git@` or `.git` URLs)\n\nReturns a dictionary with:\n- `files`: `{path: content}` mapping\n- `stats`: metadata (counts, skipped files, patterns, etc.)\n\n#### Helper Functions\n- `should_include_file()`: Applies `fnmatch`-style glob patterns for inclusion/exclusion.\n- `fetch_branches()`: Fetches list of branches via GitHub API.\n- `check_tree()`: Validates if a Git tree (commit) exists.\n- `fetch_contents()`: Recursively traverses repo structure via GitHub API, downloads files.\n\n---\n\n### Key Technical Details \u0026 TODOs\n\n#### üîß **Technical Details**\n- **Supports both HTTPS (API) and SSH (clone)** URLs.\n- **Handles rate limiting**: Auto-waits on `403 rate limit exceeded`.\n- **Size enforcement**: Checks both metadata and actual download size (via headers or base64 decoding).\n- **Flexible filtering**: Uses `fnmatch` patterns (`*.py`, `**/test*`) for include/exclude.\n- **Relative paths**: Option to return paths relative to the specified subdirectory.\n- **Token handling**: Accepts token via parameter or `GITHUB_TOKEN` env var (recommended for private repos).\n- **Robust error handling**: Logs detailed messages for 404s, rate limits, size limits, and read errors.\n\n#### ‚ö†Ô∏è **Limitations \u0026 TODOs**\n- **SSH mode does not support commit/branch specification** ‚Äî clones default branch only (no ref parsing from SSH URL).\n  - *TODO: Allow explicit `ref` parameter for SSH mode.*\n- **No support for GitHub Enterprise** (uses public API URLs).\n  - *TODO: Add `api_base_url` parameter.*\n- **No parallel downloads** ‚Äî files are fetched sequentially.\n  - *TODO: Add async/threaded fetching for performance.*\n- **Pattern matching is case-sensitive** (depends on `fnmatch` behavior).\n  - *TODO: Add `case_sensitive` flag.*\n- **No retry logic for transient errors** beyond rate limits.\n  - *TODO: Add retry with exponential backoff.*\n\n---\n\n### Short Usage Example\n\n```python\nimport os\nfrom crawl_github_files import crawl_github_files\n\n# Use token from environment (recommended)\ntoken = os.environ.get(\"GITHUB_TOKEN\")\n\n# Crawl a specific commit + path, filter by file type\nresult = crawl_github_files(\n    repo_url=\"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\",\n    token=token,\n    max_file_size=500 * 1024,  # 500 KB\n    use_relative_paths=True,\n    include_patterns={\"*.py\", \"*.md\"},\n    exclude_patterns=\"**/test*\"  # Exclude test files\n)\n\n# Access results\nfiles = result[\"files\"]  # Dict: {relative_path: content}\nstats = result[\"stats\"]  # Metadata and stats\n\nprint(f\"Downloaded {stats['downloaded_count']} files.\")\nfor path in sorted(files):\n    print(f\"  {path}\")\n```\n\n\u003e ‚úÖ **Tip**: For private repos or high-volume use, always set `GITHUB_TOKEN` in your environment to avoid rate limits.",
  "dd9d8092f3a77b99f9b3e153e154e45154bbd17f": "# flow.py - File Summary\n\n**one_line**:  \nOrchestrates a multi-step workflow to generate a structured tutorial from a codebase using modular, retry-enabled processing nodes.\n\n**Purpose**:  \nThis file defines a `Flow` (from the `pocketflow` framework) that automates the generation of a tutorial by analyzing a codebase in sequential stages: fetching, abstraction identification, relationship analysis, chapter ordering, chapter writing, and final tutorial assembly. It acts as the central pipeline definition for a code-to-tutorial generation system.\n\n**Major functions/classes**:  \n- `create_tutorial_flow()`:  \n  Main function that instantiates and connects all processing nodes into a linear flow.  \n  Returns a `Flow` object starting at `FetchRepo`.\n\n- **Imported Node Classes (from `nodes`)**:\n  - `FetchRepo`: Pulls or accesses the repository.\n  - `IdentifyAbstractions`: Detects key abstractions (e.g., classes, modules) in code.\n  - `AnalyzeRelationships`: Analyzes dependencies and interactions between abstractions.\n  - `OrderChapters`: Determines a logical sequence for tutorial chapters.\n  - `WriteChapters`: Generates content for each chapter (noted as a `BatchNode`, likely processes multiple items in parallel).\n  - `CombineTutorial`: Merges all chapters into a final tutorial output.\n\n**Key technical details \u0026 TODOs**:  \n- Uses **`pocketflow`'s Flow and node piping** (`\u003e\u003e`) to define execution order.\n- All nodes (except `FetchRepo`, `CombineTutorial`) include **retry logic** (`max_retries=5`, `wait=20` seconds), suggesting robustness against transient failures (e.g., API rate limits or LLM call issues).\n- `WriteChapters` is noted as a `BatchNode` ‚Äî implies it can process multiple chapters in parallel or in batch mode.\n- **No branching or conditional logic** ‚Äî the flow is strictly linear.\n- **TODOs (implied)**:\n  - Error handling beyond retries (e.g., fallback strategies).\n  - Monitoring/logging integration in production.\n  - Configurable retry/wait values (currently hardcoded).\n  - Support for resuming partial flows.\n\n**Short usage example**:  \n```python\nfrom flow import create_tutorial_flow\n\n# Create and run the tutorial generation flow\nflow = create_tutorial_flow()\nflow.run()  # Executes: FetchRepo ‚Üí ... ‚Üí CombineTutorial\n# Output: A fully generated tutorial (format depends on CombineTutorial)\n```",
  "e3899008c4f7d742a0547ae27d1206e3629bcce8": "# **Chapter 1: Introduction \u0026 Vision**\n\n\u003e *\"Every great developer was once a beginner.\"*  \n\u003e ‚Äî And every great codebase should be **accessible** to them.\n\nWelcome to **PocketFlow Tutorial Generator** ‚Äî a revolutionary open-source tool that transforms complex software projects into **beginner-friendly, AI-powered tutorials**, automatically. Whether you're onboarding new team members, learning a new framework, or documenting your own work, this project turns code into clear, visual, and structured educational content ‚Äî **with just one command**.\n\n---\n\n## üéØ The Vision: Code That Teaches Itself\n\nModern software projects are powerful but often **inaccessible** to newcomers. Documentation is scattered, architecture is implicit, and the learning curve is steep. What if codebases could **explain themselves**?\n\nOur vision is simple:  \n\u003e **Turn any repository into a self-documenting, educational experience ‚Äî powered by AI.**\n\nWith **PocketFlow Tutorial Generator**, you don‚Äôt need to write documentation from scratch. Instead, the system:\n- **Understands** your codebase using AI.\n- **Identifies** core abstractions (classes, functions, design patterns).\n- **Maps** how components interact.\n- **Orders** concepts for optimal learning.\n- **Writes** a structured, multilingual tutorial ‚Äî complete with explanations, examples, and visual flow.\n\nIt‚Äôs like having an expert developer sit beside you and say:  \n\u003e *‚ÄúHere‚Äôs what matters ‚Äî and here‚Äôs how it all fits together.‚Äù*\n\n---\n\n## üîß What This Tool Does (And Why It Matters)\n\nAt its core, **PocketFlow Tutorial Generator** is a **modular AI agent pipeline** that analyzes code and generates tutorials. Here‚Äôs what it enables:\n\n| Feature | Benefit |\n|-------|--------|\n| ‚úÖ **GitHub + Local Repo Support** | Works on any public/private repo or local directory. |\n| ‚úÖ **AI-Powered Abstraction Detection** | Uses LLMs (Gemini, Claude, OpenAI) to find the *essence* of the code. |\n| ‚úÖ **Automatic Relationship Mapping** | Shows dependencies, data flow, and component interactions. |\n| ‚úÖ **Pedagogical Chapter Ordering** | Teaches concepts in the *right* sequence ‚Äî not just file order. |\n| ‚úÖ **Multilingual Output** | Generate tutorials in **English, Spanish, Chinese, French**, and more. |\n| ‚úÖ **Filtering \u0026 Scoping** | Focus on relevant files (`*.py`, `*.ts`) and skip tests, logs, or binaries. |\n| ‚úÖ **Docker \u0026 CLI Ready** | Run locally, in CI, or containerized ‚Äî no setup headaches. |\n| ‚úÖ **Caching \u0026 Efficiency** | Avoids redundant AI calls; saves time and cost. |\n\nWhether you‚Äôre:\n- A **learner** trying to understand a new framework (e.g., FastAPI, Flask),\n- A **maintainer** onboarding new contributors,\n- Or a **teacher** building course material,\n\n‚Ä¶this tool **lowers the barrier to entry** for everyone.\n\n---\n\n## üß† How It Works: A High-Level Architecture\n\nThe system is built around **seven core architectural components**, orchestrated by the **PocketFlow** framework ‚Äî a lightweight, agentic workflow engine.\n\nHere‚Äôs the end-to-end flow:\n\n```mermaid\ngraph TD\n    A[Input: GitHub URL or Local Dir] --\u003e B(FetchRepo)\n    B --\u003e C[IdentifyAbstractions]\n    C --\u003e D[AnalyzeRelationships]\n    D --\u003e E[OrderChapters]\n    E --\u003e F[WriteChapters]\n    F --\u003e G[CombineTutorial]\n    G --\u003e H[Tutorial Output: Markdown/HTML]\n```\n\n### üîç Key Components at a Glance\n\n| Component | Role |\n|--------|------|\n| **`FetchRepo`** | Downloads and filters files (with `.gitignore`, size limits, patterns). |\n| **`IdentifyAbstractions`** | Asks: *\"What are the key concepts in this code?\"* ‚Äî returns classes, patterns, functions. |\n| **`AnalyzeRelationships`** | Asks: *\"How do these concepts interact?\"* ‚Äî builds a dependency map. |\n| **`OrderChapters`** | Asks: *\"What should a beginner learn first?\"* ‚Äî creates a logical sequence. |\n| **`WriteChapters`** | Generates **per-chapter explanations** with code snippets and analogies. |\n| **`CombineTutorial`** | Merges chapters into a single, polished document (Markdown/HTML). |\n| **`CallLLM`** | Unified interface to",
  "f6a1168695cb5678d44690c58ad4fe1d69b893ac": "# crawl_github_files.py - File Summary\n\n### one_line  \nA utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.\n\n---\n\n### Purpose  \nThis script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).\n\n---\n\n### Major Functions/Classes\n\n#### `crawl_github_files(...)`\nMain function that orchestrates crawling:\n- Parses GitHub URL (including commit/branch and subdirectory).\n- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.\n- Downloads files recursively from a specified subdirectory.\n- Applies **include/exclude glob patterns** to filter files.\n- Enforces **maximum file size** to avoid memory issues.\n- Returns a dictionary with file contents and crawl statistics.\n\n#### `should_include_file(file_path, file_name)`\nHelper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.\n\n#### `fetch_contents(path)` (nested)\nRecursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.\n\n#### `fetch_branches()` and `check_tree()`\nUsed to validate and extract commit/branch reference from the URL when not explicitly provided.\n\n---\n\n### Key Technical Details \u0026 TODOs\n\n#### ‚úÖ **Features**\n- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).\n- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.\n- **Two URL modes**:\n  - HTTPS: Uses GitHub API (supports branch/commit + path).\n  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).\n- **Flexible filtering**:\n  - `include_patterns`: e.g., `{\"*.py\", \"*.md\"}`\n  - `exclude_patterns`: e.g., `{\"*test*\", \"*.log\"}`\n- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.\n- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.\n\n#### ‚ö†Ô∏è **Technical Notes**\n- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.\n- **File size is checked twice**: once from metadata, once from `download_url` headers.\n- **Base64 decoding** used when `download_url` is missing (fallback).\n- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).\n- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).\n\n#### üîß **TODOs / Improvements**\n- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).\n- [ ] **Add progress bar or logging** for large repositories.\n- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.\n- [ ] **Caching mechanism** to avoid re-downloading unchanged files.\n- [ ] **Better handling of symlinks and binary files** (currently skipped silently).\n- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.\n- [ ] **Add retry logic** for transient network failures.\n\n---\n\n### Short Usage Example\n\n```python\nimport os\nfrom crawl_github_files import crawl_github_files\n\n# Optional: Set token for private repos or to avoid rate limits\nos.environ[\"GITHUB_TOKEN\"] = \"your_token_here\"\n\nresult = crawl_github_files(\n    repo_url=\"https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core\",\n    token=None,  # uses GITHUB_TOKEN env\n    max_file_size=500 * 1024,  # 500 KB\n    use_relative_paths=True,\n    include_patterns={\"*.py\", \"*.md\"},\n    exclude_patterns={\"*test*\"}\n)\n\nprint(f\"Downloaded {result['stats']['downloaded_count']} files\")\nfor path, content in result[\"files\"].items():\n    print(f\"{path}: {len(content)} chars\")\n```\n\n\u003e üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.",
  "ff823db841d10426d3fc3104181c6abdb25dab6c": "# flow.py - File Summary\n\n**one_line**:  \nOrchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.\n\n**Purpose**:  \nThis file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.\n\n**Major functions/classes**:  \n- `create_tutorial_flow()`:  \n  The main function that:\n  - Instantiates all node classes (processing units) for each stage of tutorial generation.\n  - Chains them together in a directed sequence using the `\u003e\u003e` operator (PocketFlow's flow syntax).\n  - Returns a `Flow` object starting at `FetchRepo`.\n\n  **Key Nodes Used** (imported from `nodes.py`):\n  - `FetchRepo`: Retrieves/clones the target repository.\n  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.\n  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.\n  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.\n  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).\n  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.\n\n**Key technical details \u0026 TODOs**:  \n- **PocketFlow Framework**: Uses `Flow` and node chaining (`\u003e\u003e`) to define a directed acyclic graph (DAG) of tasks.\n- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).\n- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.\n- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.\n- **TODO (implied)**:  \n  - Error handling or fallback logic is not visible here (likely handled in nodes).  \n  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  \n  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.\n\n**Short usage example**:  \n```python\nfrom flow import create_tutorial_flow\n\n# Create and run the tutorial generation pipeline\nflow = create_tutorial_flow()\nflow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine\n```  \n\n\u003e **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file."
}