
=== [2025-10-03T13:49:40+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file Dockerfile (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\Dockerfile).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "Dockerfile - File Summary" as a header.

```
FROM python:3.10-slim

# update packages, install git and remove cache
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

ENTRYPOINT ["python", "main.py"]

```


RESPONSE:
# Dockerfile - File Summary

**one_line**: A minimal Docker setup to run a Python application with dependencies and Git support.

**Purpose**:  
This Dockerfile defines a lightweight containerized environment for running a Python application. It uses the official `python:3.10-slim` base image to minimize size, installs Git for potential version control needs, installs Python dependencies, and runs the main application via `main.py`.

**Major functions/classes**:  
- *Base Image Initialization*: Uses `python:3.10-slim` for a small footprint.
- *System Setup*: Updates package list, installs `git`, and cleans up to reduce image size.
- *Dependency Installation*: Copies and installs Python packages from `requirements.txt`.
- *Application Deployment*: Copies all source code into the container.
- *Entrypoint*: Executes `main.py` as the default command.

**Key technical details & TODOs**:  
- **Base Image**: `python:3.10-slim` ‚Äî lightweight, Debian-based, suitable for production.
- **Git Installation**: Required if the app or dependencies interact with Git (e.g., cloning private repos or version tracking).
- **No Cache**: Both `apt` and `pip` use `--no-cache-dir` and cleanup to reduce image size.
- **Work Directory**: `/app` is set as the working directory.
- **Security**: Consider using a non-root user (TODO: add `USER` directive for production hardening).
- **Multi-stage Build**: Not used ‚Äî consider if build-time dependencies (e.g., compilers) are added later.
- **.dockerignore**: Not referenced ‚Äî recommended to add a `.dockerignore` file to prevent unnecessary files (e.g., `.git`, `__pycache__`) from being copied.
- **Environment Variables**: None defined ‚Äî add if needed for configuration (e.g., `ENV APP_ENV=production`).

**Short usage example**:  
```bash
# Build the image
docker build -t pocketflow-app .

# Run the container
docker run --rm pocketflow-app
```

> Ensure `requirements.txt` and `main.py` exist in the build context (project root).
---

=== [2025-10-03T13:49:51+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file README.md (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\README.md).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "README.md - File Summary" as a header.

```
<h1 align="center">Turns Codebase into Easy Tutorial with AI</h1>

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>
> *Ever stared at a new codebase written by others feeling completely lost? This tutorial shows you how to build an AI agent that analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works.*

<p align="center">
  <img
    src="./assets/banner.png" width="800"
  />
</p>

This is a tutorial project of [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework. It crawls GitHub repositories and builds a knowledge base from the code. It analyzes entire codebases to identify core abstractions and how they interact, and transforms complex code into beginner-friendly tutorials with clear visualizations.

- Check out the [YouTube Development Tutorial](https://youtu.be/AFY67zOpbSo) for more!

- Check out the [Substack Post Tutorial](https://zacharyhuang.substack.com/p/ai-codebase-knowledge-builder-full) for more!

&nbsp;&nbsp;**üî∏ üéâ Reached Hacker¬†News Front Page** (April¬†2025) with >900 up‚Äëvotes:  [Discussion ¬ª](https://news.ycombinator.com/item?id=43739456)

&nbsp;&nbsp;**üî∏ üéä Online Service Now Live!** (May&nbsp;2025) Try our new online version at [https://code2tutorial.com/](https://code2tutorial.com/) ‚Äì just paste a GitHub link, no installation needed!

## ‚≠ê Example Results for Popular GitHub Repositories!

<p align="center">
    <img
      src="./assets/example.png" width="600"
    />
</p>

ü§Ø All these tutorials are generated **entirely by AI** by crawling the GitHub repo!

- [AutoGen Core](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/AutoGen%20Core) - Build AI teams that talk, think, and solve problems together like coworkers!

- [Browser Use](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Browser%20Use) - Let AI surf the web for you, clicking buttons and filling forms like a digital assistant!

- [Celery](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Celery) - Supercharge your app with background tasks that run while you sleep!

- [Click](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Click) - Turn Python functions into slick command-line tools with just a decorator!

- [Codex](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Codex) - Turn plain English into working code with this AI terminal wizard!

- [Crawl4AI](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Crawl4AI) - Train your AI to extract exactly what matters from any website!

- [CrewAI](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/CrewAI) - Assemble a dream team of AI specialists to tackle impossible problems!

- [DSPy](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/DSPy) - Build LLM apps like Lego blocks that optimize themselves!

- [FastAPI](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/FastAPI) - Create APIs at lightning speed with automatic docs that clients will love!

- [Flask](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Flask) - Craft web apps with minimal code that scales from prototype to production!

- [Google A2A](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Google%20A2A) - The universal language that lets AI agents collaborate across borders!

- [LangGraph](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/LangGraph) - Design AI agents as flowcharts where each step remembers what happened before!

- [LevelDB](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/LevelDB) - Store data at warp speed with Google's engine that powers blockchains!

- [MCP Python SDK](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/MCP%20Python%20SDK) - Build powerful apps that communicate through an elegant protocol without sweating the details!

- [NumPy Core](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/NumPy%20Core) - Master the engine behind data science that makes Python as fast as C!

- [OpenManus](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/OpenManus) - Build AI agents with digital brains that think, learn, and use tools just like humans do!

- [PocketFlow](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/PocketFlow) - 100-line LLM framework. Let Agents build Agents!

- [Pydantic Core](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Pydantic%20Core) - Validate data at rocket speed with just Python type hints!

- [Requests](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Requests) - Talk to the internet in Python with code so simple it feels like cheating!

- [SmolaAgents](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/SmolaAgents) - Build tiny AI agents that punch way above their weight class!

- Showcase Your AI-Generated Tutorials in [Discussions](https://github.com/The-Pocket/PocketFlow-Tutorial-Codebase-Knowledge/discussions)!

## üöÄ Getting Started

1. Clone this repository
   ```bash
   git clone https://github.com/The-Pocket/PocketFlow-Tutorial-Codebase-Knowledge
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up LLM in [`utils/call_llm.py`](./utils/call_llm.py) by providing credentials. By default, you can use the [AI Studio key](https://aistudio.google.com/app/apikey) with this client for Gemini Pro 2.5:

   ```python
   client = genai.Client(
     api_key=os.getenv("GEMINI_API_KEY", "your-api_key"),
   )
   ```

   You can use your own models. We highly recommend the latest models with thinking capabilities (Claude 3.7 with thinking, O1). You can verify that it is correctly set up by running:
   ```bash
   python utils/call_llm.py
   ```

5. Generate a complete codebase tutorial by running the main script:
    ```bash
    # Analyze a GitHub repository
    python main.py --repo https://github.com/username/repo --include "*.py" "*.js" --exclude "tests/*" --max-size 50000

    # Or, analyze a local directory
    python main.py --dir /path/to/your/codebase --include "*.py" --exclude "*test*"

    # Or, generate a tutorial in Chinese
    python main.py --repo https://github.com/username/repo --language "Chinese"
    ```

    - `--repo` or `--dir` - Specify either a GitHub repo URL or a local directory path (required, mutually exclusive)
    - `-n, --name` - Project name (optional, derived from URL/directory if omitted)
    - `-t, --token` - GitHub token (or set GITHUB_TOKEN environment variable)
    - `-o, --output` - Output directory (default: ./output)
    - `-i, --include` - Files to include (e.g., "`*.py`" "`*.js`")
    - `-e, --exclude` - Files to exclude (e.g., "`tests/*`" "`docs/*`")
    - `-s, --max-size` - Maximum file size in bytes (default: 100KB)
    - `--language` - Language for the generated tutorial (default: "english")
    - `--max-abstractions` - Maximum number of abstractions to identify (default: 10)
    - `--no-cache` - Disable LLM response caching (default: caching enabled)

The application will crawl the repository, analyze the codebase structure, generate tutorial content in the specified language, and save the output in the specified directory (default: ./output).


<details>
 
<summary> üê≥ <b>Running with Docker</b> </summary>

To run this project in a Docker container, you'll need to pass your API keys as environment variables. 

1. Build the Docker image
   ```bash
   docker build -t pocketflow-app .
   ```

2. Run the container

   You'll need to provide your `GEMINI_API_KEY` for the LLM to function. If you're analyzing private GitHub repositories or want to avoid rate limits, also provide your `GITHUB_TOKEN`.
   
   Mount a local directory to `/app/output` inside the container to access the generated tutorials on your host machine.
   
   **Example for analyzing a public GitHub repository:**
   
   ```bash
   docker run -it --rm \
     -e GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE" \
     -v "$(pwd)/output_tutorials":/app/output \
     pocketflow-app --repo https://github.com/username/repo
   ```
   
   **Example for analyzing a local directory:**
   
   ```bash
   docker run -it --rm \
     -e GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE" \
     -v "/path/to/your/local_codebase":/app/code_to_analyze \
     -v "$(pwd)/output_tutorials":/app/output \
     pocketflow-app --dir /app/code_to_analyze
   ```
</details>

## üí° Development Tutorial

- I built using [**Agentic Coding**](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to), the fastest development paradigm, where humans simply [design](docs/design.md) and agents [code](flow.py).

- The secret weapon is [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework that lets Agents (e.g., Cursor AI) build for you

- Check out the Step-by-step YouTube development tutorial:

<br>
<div align="center">
  <a href="https://youtu.be/AFY67zOpbSo" target="_blank">
    <img src="./assets/youtube_thumbnail.png" width="500" alt="Pocket Flow Codebase Tutorial" style="cursor: pointer;">
  </a>
</div>
<br>




```


RESPONSE:
# README.md - File Summary

## one_line
**AI-powered tool that turns any GitHub or local codebase into a beginner-friendly, well-structured tutorial using LLM analysis.**

## Purpose of the file
This `README.md` serves as the **project documentation and landing page** for an AI-driven tutorial generator that automatically analyzes codebases (from GitHub or local directories), identifies core abstractions, and produces human-readable, educational tutorials with visualizations. It explains how to set up, configure, and run the system locally or via Docker, lists example outputs for popular open-source projects, and links to tutorials, videos, and an online demo.

## Major functions/classes
- **Main entry point (`main.py`)** ‚Äì Parses CLI arguments, orchestrates crawling, analysis, and tutorial generation.
- **Repository Crawler** ‚Äì Fetches and filters files from GitHub repos or local directories based on include/exclude patterns and file size limits.
- **Codebase Analyzer** ‚Äì Uses LLMs to identify core abstractions, module interactions, and architectural patterns.
- **Tutorial Generator** ‚Äì Transforms analyzed code into beginner-friendly explanations with diagrams and step-by-step guides.
- **LLM Interface (`utils/call_llm.py`)** ‚Äì Wrapper for calling LLM APIs (supports Gemini, Claude, etc.) with caching and error handling.
- **Abstraction Engine** ‚Äì Limits and structures the number of high-level concepts extracted from the codebase.
- **Multi-language Support** ‚Äì Generates tutorials in different languages (e.g., English, Chinese) via LLM translation.
- **Docker Support** ‚Äì Enables containerized execution with environment variable-based configuration.

## Key technical details & TODOs
### üîß **Technical Highlights**
- Built on **Pocket Flow**, a lightweight 100-line LLM orchestration framework.
- Uses **LLM caching** by default to reduce cost and improve performance.
- Supports **large language models with reasoning/thinking capabilities** (e.g., Gemini 2.5, Claude 3.7, O1).
- Accepts **flexible file filtering** (`--include`, `--exclude`) and size limits (`--max-size`).
- Outputs **Markdown + HTML-ready tutorials** with embedded diagrams (Mermaid.js likely used internally).
- Designed for **agentic development** ‚Äì humans design, AI agents write code (via Cursor AI or similar).

### ‚úÖ **Current Features**
- GitHub & local codebase support.
- Multi-language tutorial generation.
- Dockerized deployment.
- Caching of LLM responses.
- Rich CLI interface with intuitive flags.
- Live online service: [code2tutorial.com](https://code2tutorial.com/)

### üöß **TODOs / Future Work (implied)**
- Expand model support beyond Gemini (e.g., OpenAI, Ollama, etc.) ‚Äì currently requires manual config.
- Improve diagram generation (e.g., dynamic Mermaid, PlantUML).
- Add support for more tutorial formats (PDF, Jupyter notebooks).
- Enhance abstraction detection with graph-based analysis.
- Allow user feedback loop to improve generated tutorials.
- Integrate with IDE plugins (VSCode, PyCharm).
- Scale to larger repos with incremental analysis.

## Short usage example
```bash
# Analyze a public GitHub repo and generate an English tutorial
python main.py \
  --repo https://github.com/fastapi/fastapi \
  --include "*.py" \
  --exclude "tests/*" "docs/*" \
  --max-size 50000 \
  --output ./tutorials/fastapi

# Generate a Chinese tutorial for a local project
python main.py \
  --dir ./my-python-app \
  --include "*.py" "*.md" \
  --language "Chinese" \
  --max-abstractions 8

# Run in Docker with API keys
docker run -it --rm \
  -e GEMINI_API_KEY="your-key-here" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/langchain-ai/langchain
```

> üí° *After running, find the generated tutorial in the specified output folder ‚Äî includes explanations, architecture insights, and visualizations!*
---

=== [2025-10-03T13:49:57+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file flow.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\flow.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "flow.py - File Summary" as a header.

```
from pocketflow import Flow
# Import all node classes from nodes.py
from nodes import (
    FetchRepo,
    IdentifyAbstractions,
    AnalyzeRelationships,
    OrderChapters,
    WriteChapters,
    CombineTutorial
)

def create_tutorial_flow():
    """Creates and returns the codebase tutorial generation flow."""

    # Instantiate nodes
    fetch_repo = FetchRepo()
    identify_abstractions = IdentifyAbstractions(max_retries=5, wait=20)
    analyze_relationships = AnalyzeRelationships(max_retries=5, wait=20)
    order_chapters = OrderChapters(max_retries=5, wait=20)
    write_chapters = WriteChapters(max_retries=5, wait=20) # This is a BatchNode
    combine_tutorial = CombineTutorial()

    # Connect nodes in sequence based on the design
    fetch_repo >> identify_abstractions
    identify_abstractions >> analyze_relationships
    analyze_relationships >> order_chapters
    order_chapters >> write_chapters
    write_chapters >> combine_tutorial

    # Create the flow starting with FetchRepo
    tutorial_flow = Flow(start=fetch_repo)

    return tutorial_flow

```


RESPONSE:
# flow.py - File Summary

**one_line**:  
Orchestrates a multi-step workflow to generate a structured tutorial from a codebase using modular, retry-enabled processing nodes.

**Purpose**:  
This file defines a `Flow` (from the `pocketflow` framework) that automates the generation of a tutorial by analyzing a codebase in sequential stages: fetching, abstraction identification, relationship analysis, chapter ordering, chapter writing, and final tutorial assembly. It acts as the central pipeline definition for a code-to-tutorial generation system.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  Main function that instantiates and connects all processing nodes into a linear flow.  
  Returns a `Flow` object starting at `FetchRepo`.

- **Imported Node Classes (from `nodes`)**:
  - `FetchRepo`: Pulls or accesses the repository.
  - `IdentifyAbstractions`: Detects key abstractions (e.g., classes, modules) in code.
  - `AnalyzeRelationships`: Analyzes dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical sequence for tutorial chapters.
  - `WriteChapters`: Generates content for each chapter (noted as a `BatchNode`, likely processes multiple items in parallel).
  - `CombineTutorial`: Merges all chapters into a final tutorial output.

**Key technical details & TODOs**:  
- Uses **`pocketflow`'s Flow and node piping** (`>>`) to define execution order.
- All nodes (except `FetchRepo`, `CombineTutorial`) include **retry logic** (`max_retries=5`, `wait=20` seconds), suggesting robustness against transient failures (e.g., API rate limits or LLM call issues).
- `WriteChapters` is noted as a `BatchNode` ‚Äî implies it can process multiple chapters in parallel or in batch mode.
- **No branching or conditional logic** ‚Äî the flow is strictly linear.
- **TODOs (implied)**:
  - Error handling beyond retries (e.g., fallback strategies).
  - Monitoring/logging integration in production.
  - Configurable retry/wait values (currently hardcoded).
  - Support for resuming partial flows.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation flow
flow = create_tutorial_flow()
flow.run()  # Executes: FetchRepo ‚Üí ... ‚Üí CombineTutorial
# Output: A fully generated tutorial (format depends on CombineTutorial)
```
---

=== [2025-10-03T13:50:05+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file main.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\main.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "main.py - File Summary" as a header.

```
import dotenv
import os
import argparse
# Import the function that creates the flow
from flow import create_tutorial_flow

dotenv.load_dotenv()

# Default file patterns
DEFAULT_INCLUDE_PATTERNS = {
    "*.py", "*.js", "*.jsx", "*.ts", "*.tsx", "*.go", "*.java", "*.pyi", "*.pyx",
    "*.c", "*.cc", "*.cpp", "*.h", "*.md", "*.rst", "*Dockerfile",
    "*Makefile", "*.yaml", "*.yml",
}

DEFAULT_EXCLUDE_PATTERNS = {
    "assets/*", "data/*", "images/*", "public/*", "static/*", "temp/*",
    "*docs/*",
    "*venv/*",
    "*.venv/*",
    "*test*",
    "*tests/*",
    "*examples/*",
    "v1/*",
    "*dist/*",
    "*build/*",
    "*experimental/*",
    "*deprecated/*",
    "*misc/*",
    "*legacy/*",
    ".git/*", ".github/*", ".next/*", ".vscode/*",
    "*obj/*",
    "*bin/*",
    "*node_modules/*",
    "*.log"
}

# --- Main Function ---
def main():
    parser = argparse.ArgumentParser(description="Generate a tutorial for a GitHub codebase or local directory.")

    # Create mutually exclusive group for source
    source_group = parser.add_mutually_exclusive_group(required=True)
    source_group.add_argument("--repo", help="URL of the public GitHub repository.")
    source_group.add_argument("--dir", help="Path to local directory.")

    parser.add_argument("-n", "--name", help="Project name (optional, derived from repo/directory if omitted).")
    parser.add_argument("-t", "--token", help="GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).")
    parser.add_argument("-o", "--output", default="output", help="Base directory for output (default: ./output).")
    parser.add_argument("-i", "--include", nargs="+", help="Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.")
    parser.add_argument("-e", "--exclude", nargs="+", help="Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.")
    parser.add_argument("-s", "--max-size", type=int, default=100000, help="Maximum file size in bytes (default: 100000, about 100KB).")
    # Add language parameter for multi-language support
    parser.add_argument("--language", default="english", help="Language for the generated tutorial (default: english)")
    # Add use_cache parameter to control LLM caching
    parser.add_argument("--no-cache", action="store_true", help="Disable LLM response caching (default: caching enabled)")
    # Add max_abstraction_num parameter to control the number of abstractions
    parser.add_argument("--max-abstractions", type=int, default=10, help="Maximum number of abstractions to identify (default: 10)")

    args = parser.parse_args()

    # Get GitHub token from argument or environment variable if using repo
    github_token = None
    if args.repo:
        github_token = args.token or os.environ.get('GITHUB_TOKEN')
        if not github_token:
            print("Warning: No GitHub token provided. You might hit rate limits for public repositories.")

    # Initialize the shared dictionary with inputs
    shared = {
        "repo_url": args.repo,
        "local_dir": args.dir,
        "project_name": args.name, # Can be None, FetchRepo will derive it
        "github_token": github_token,
        "output_dir": args.output, # Base directory for CombineTutorial output

        # Add include/exclude patterns and max file size
        "include_patterns": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,
        "exclude_patterns": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,
        "max_file_size": args.max_size,

        # Add language for multi-language support
        "language": args.language,
        
        # Add use_cache flag (inverse of no-cache flag)
        "use_cache": not args.no_cache,
        
        # Add max_abstraction_num parameter
        "max_abstraction_num": args.max_abstractions,

        # Outputs will be populated by the nodes
        "files": [],
        "abstractions": [],
        "relationships": {},
        "chapter_order": [],
        "chapters": [],
        "final_output_dir": None
    }

    # Display starting message with repository/directory and language
    print(f"Starting tutorial generation for: {args.repo or args.dir} in {args.language.capitalize()} language")
    print(f"LLM caching: {'Disabled' if args.no_cache else 'Enabled'}")

    # Create the flow instance
    tutorial_flow = create_tutorial_flow()

    # Run the flow
    tutorial_flow.run(shared)

if __name__ == "__main__":
    main()

```


RESPONSE:
# main.py - File Summary

**one_line:**  
CLI entry point for generating multi-language tutorials from GitHub repos or local codebases using a flow-based pipeline.

**Purpose:**  
This file serves as the command-line interface (CLI) to trigger a tutorial generation workflow. It parses user inputs (source, patterns, language, caching, etc.), initializes a shared data context, and executes a modular flow (`create_tutorial_flow`) that processes the codebase, extracts abstractions, analyzes relationships, and generates a structured tutorial.

**Major functions/classes:**  
- `main()`:  
  - Parses CLI arguments using `argparse`.
  - Handles GitHub token resolution (from args or `GITHUB_TOKEN` env var).
  - Constructs a `shared` dictionary to pass state through the flow.
  - Initializes and runs the tutorial generation flow via `create_tutorial_flow()`.

- **Key Imports:**
  - `create_tutorial_flow` (from `flow`): The core pipeline orchestrating file fetching, abstraction, relationship mapping, and tutorial assembly.
  - `dotenv`: Loads environment variables (e.g., `GITHUB_TOKEN`).
  - `argparse`: Enables rich CLI argument parsing.

**Key technical details & TODOs:**  
- **Mutually exclusive sources**: Only one of `--repo` (GitHub URL) or `--dir` (local path) is allowed.
- **File filtering**: Uses glob-style include/exclude patterns (e.g., `*.py`, `tests/*`) with sensible defaults to focus on source files and skip binaries/tests.
- **Size limit**: Skips files larger than `--max-size` (default: 100KB) to avoid LLM context overflow.
- **Multi-language support**: `--language` parameter passed through to LLM prompts (e.g., for non-English tutorials).
- **LLM caching**: Enabled by default (`--no-cache` disables) to speed up repeated runs.
- **Abstraction control**: `--max-abstractions` limits the number of high-level concepts extracted.
- **Output structure**: Generated tutorials are written under `--output` directory (default: `./output`).
- **TODOs (implied):**
  - No validation of `--language` values (could add allowed list).
  - No progress tracking/logging beyond initial print.
  - No error handling for malformed URLs or invalid paths (defers to flow nodes).
  - Could benefit from config file support (YAML/JSON) for complex runs.

**Short usage example:**  
```bash
# From a GitHub repo (with token)
python main.py --repo https://github.com/user/repo --token ghp_xxx --output ./tutorials --language spanish --no-cache

# From a local directory (uses defaults for patterns/size)
python main.py --dir ./my-project -n "My Project" --max-abstractions 5

# With custom include/exclude patterns
python main.py --repo https://github.com/user/repo -i "*.py" "*.md" -e "tests/*" "*.log" --max-size 50000
```
---

=== [2025-10-03T13:50:16+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file nodes.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\nodes.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "nodes.py - File Summary" as a header.

```
import os
import re
import yaml
from pocketflow import Node, BatchNode
from utils.crawl_github_files import crawl_github_files
from utils.call_llm import call_llm
from utils.crawl_local_files import crawl_local_files


# Helper to get content for specific file indices
def get_content_for_indices(files_data, indices):
    content_map = {}
    for i in indices:
        if 0 <= i < len(files_data):
            path, content = files_data[i]
            content_map[f"{i} # {path}"] = (
                content  # Use index + path as key for context
            )
    return content_map


class FetchRepo(Node):
    def prep(self, shared):
        repo_url = shared.get("repo_url")
        local_dir = shared.get("local_dir")
        project_name = shared.get("project_name")

        if not project_name:
            # Basic name derivation from URL or directory
            if repo_url:
                project_name = repo_url.split("/")[-1].replace(".git", "")
            else:
                project_name = os.path.basename(os.path.abspath(local_dir))
            shared["project_name"] = project_name

        # Get file patterns directly from shared
        include_patterns = shared["include_patterns"]
        exclude_patterns = shared["exclude_patterns"]
        max_file_size = shared["max_file_size"]

        return {
            "repo_url": repo_url,
            "local_dir": local_dir,
            "token": shared.get("github_token"),
            "include_patterns": include_patterns,
            "exclude_patterns": exclude_patterns,
            "max_file_size": max_file_size,
            "use_relative_paths": True,
        }

    def exec(self, prep_res):
        if prep_res["repo_url"]:
            print(f"Crawling repository: {prep_res['repo_url']}...")
            result = crawl_github_files(
                repo_url=prep_res["repo_url"],
                token=prep_res["token"],
                include_patterns=prep_res["include_patterns"],
                exclude_patterns=prep_res["exclude_patterns"],
                max_file_size=prep_res["max_file_size"],
                use_relative_paths=prep_res["use_relative_paths"],
            )
        else:
            print(f"Crawling directory: {prep_res['local_dir']}...")

            result = crawl_local_files(
                directory=prep_res["local_dir"],
                include_patterns=prep_res["include_patterns"],
                exclude_patterns=prep_res["exclude_patterns"],
                max_file_size=prep_res["max_file_size"],
                use_relative_paths=prep_res["use_relative_paths"]
            )

        # Convert dict to list of tuples: [(path, content), ...]
        files_list = list(result.get("files", {}).items())
        if len(files_list) == 0:
            raise (ValueError("Failed to fetch files"))
        print(f"Fetched {len(files_list)} files.")
        return files_list

    def post(self, shared, prep_res, exec_res):
        shared["files"] = exec_res  # List of (path, content) tuples


class IdentifyAbstractions(Node):
    def prep(self, shared):
        files_data = shared["files"]
        project_name = shared["project_name"]  # Get project name
        language = shared.get("language", "english")  # Get language
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True
        max_abstraction_num = shared.get("max_abstraction_num", 10)  # Get max_abstraction_num, default to 10

        # Helper to create context from files, respecting limits (basic example)
        def create_llm_context(files_data):
            context = ""
            file_info = []  # Store tuples of (index, path)
            for i, (path, content) in enumerate(files_data):
                entry = f"--- File Index {i}: {path} ---\n{content}\n\n"
                context += entry
                file_info.append((i, path))

            return context, file_info  # file_info is list of (index, path)

        context, file_info = create_llm_context(files_data)
        # Format file info for the prompt (comment is just a hint for LLM)
        file_listing_for_prompt = "\n".join(
            [f"- {idx} # {path}" for idx, path in file_info]
        )
        return (
            context,
            file_listing_for_prompt,
            len(files_data),
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        )  # Return all parameters

    def exec(self, prep_res):
        (
            context,
            file_listing_for_prompt,
            file_count,
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        ) = prep_res  # Unpack all parameters
        print(f"Identifying abstractions using LLM...")

        # Add language instruction and hints only if not English
        language_instruction = ""
        name_lang_hint = ""
        desc_lang_hint = ""
        if language.lower() != "english":
            language_instruction = f"IMPORTANT: Generate the `name` and `description` for each abstraction in **{language.capitalize()}** language. Do NOT use English for these fields.\n\n"
            # Keep specific hints here as name/description are primary targets
            name_lang_hint = f" (value in {language.capitalize()})"
            desc_lang_hint = f" (value in {language.capitalize()})"

        prompt = f"""
For the project `{project_name}`:

Codebase Context:
{context}

{language_instruction}Analyze the codebase context.
Identify the top 5-{max_abstraction_num} core most important abstractions to help those new to the codebase.

For each abstraction, provide:
1. A concise `name`{name_lang_hint}.
2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words{desc_lang_hint}.
3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.

List of file indices and paths present in the context:
{file_listing_for_prompt}

Format the output as a YAML list of dictionaries:

```yaml
- name: |
    Query Processing{name_lang_hint}
  description: |
    Explains what the abstraction does.
    It's like a central dispatcher routing requests.{desc_lang_hint}
  file_indices:
    - 0 # path/to/file1.py
    - 3 # path/to/related.py
- name: |
    Query Optimization{name_lang_hint}
  description: |
    Another core concept, similar to a blueprint for objects.{desc_lang_hint}
  file_indices:
    - 5 # path/to/another.js
# ... up to {max_abstraction_num} abstractions
```"""
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0))  # Use cache only if enabled and not retrying

        # --- Validation ---
        yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        abstractions = yaml.safe_load(yaml_str)

        if not isinstance(abstractions, list):
            raise ValueError("LLM Output is not a list")

        validated_abstractions = []
        for item in abstractions:
            if not isinstance(item, dict) or not all(
                k in item for k in ["name", "description", "file_indices"]
            ):
                raise ValueError(f"Missing keys in abstraction item: {item}")
            if not isinstance(item["name"], str):
                raise ValueError(f"Name is not a string in item: {item}")
            if not isinstance(item["description"], str):
                raise ValueError(f"Description is not a string in item: {item}")
            if not isinstance(item["file_indices"], list):
                raise ValueError(f"file_indices is not a list in item: {item}")

            # Validate indices
            validated_indices = []
            for idx_entry in item["file_indices"]:
                try:
                    if isinstance(idx_entry, int):
                        idx = idx_entry
                    elif isinstance(idx_entry, str) and "#" in idx_entry:
                        idx = int(idx_entry.split("#")[0].strip())
                    else:
                        idx = int(str(idx_entry).strip())

                    if not (0 <= idx < file_count):
                        raise ValueError(
                            f"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}."
                        )
                    validated_indices.append(idx)
                except (ValueError, TypeError):
                    raise ValueError(
                        f"Could not parse index from entry: {idx_entry} in item {item['name']}"
                    )

            item["files"] = sorted(list(set(validated_indices)))
            # Store only the required fields
            validated_abstractions.append(
                {
                    "name": item["name"],  # Potentially translated name
                    "description": item[
                        "description"
                    ],  # Potentially translated description
                    "files": item["files"],
                }
            )

        print(f"Identified {len(validated_abstractions)} abstractions.")
        return validated_abstractions

    def post(self, shared, prep_res, exec_res):
        shared["abstractions"] = (
            exec_res  # List of {"name": str, "description": str, "files": [int]}
        )


class AnalyzeRelationships(Node):
    def prep(self, shared):
        abstractions = shared[
            "abstractions"
        ]  # Now contains 'files' list of indices, name/description potentially translated
        files_data = shared["files"]
        project_name = shared["project_name"]  # Get project name
        language = shared.get("language", "english")  # Get language
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True

        # Get the actual number of abstractions directly
        num_abstractions = len(abstractions)

        # Create context with abstraction names, indices, descriptions, and relevant file snippets
        context = "Identified Abstractions:\\n"
        all_relevant_indices = set()
        abstraction_info_for_prompt = []
        for i, abstr in enumerate(abstractions):
            # Use 'files' which contains indices directly
            file_indices_str = ", ".join(map(str, abstr["files"]))
            # Abstraction name and description might be translated already
            info_line = f"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}"
            context += info_line + "\\n"
            abstraction_info_for_prompt.append(
                f"{i} # {abstr['name']}"
            )  # Use potentially translated name here too
            all_relevant_indices.update(abstr["files"])

        context += "\\nRelevant File Snippets (Referenced by Index and Path):\\n"
        # Get content for relevant files using helper
        relevant_files_content_map = get_content_for_indices(
            files_data, sorted(list(all_relevant_indices))
        )
        # Format file content for context
        file_context_str = "\\n\\n".join(
            f"--- File: {idx_path} ---\\n{content}"
            for idx_path, content in relevant_files_content_map.items()
        )
        context += file_context_str

        return (
            context,
            "\n".join(abstraction_info_for_prompt),
            num_abstractions, # Pass the actual count
            project_name,
            language,
            use_cache,
        )  # Return use_cache

    def exec(self, prep_res):
        (
            context,
            abstraction_listing,
            num_abstractions, # Receive the actual count
            project_name,
            language,
            use_cache,
         ) = prep_res  # Unpack use_cache
        print(f"Analyzing relationships using LLM...")

        # Add language instruction and hints only if not English
        language_instruction = ""
        lang_hint = ""
        list_lang_note = ""
        if language.lower() != "english":
            language_instruction = f"IMPORTANT: Generate the `summary` and relationship `label` fields in **{language.capitalize()}** language. Do NOT use English for these fields.\n\n"
            lang_hint = f" (in {language.capitalize()})"
            list_lang_note = f" (Names might be in {language.capitalize()})"  # Note for the input list

        prompt = f"""
Based on the following abstractions and relevant code snippets from the project `{project_name}`:

List of Abstraction Indices and Names{list_lang_note}:
{abstraction_listing}

Context (Abstractions, Descriptions, Code):
{context}

{language_instruction}Please provide:
1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences{lang_hint}. Use markdown formatting with **bold** and *italic* text to highlight important concepts.
2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:
    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)
    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)
    - `label`: A brief label for the interaction **in just a few words**{lang_hint} (e.g., "Manages", "Inherits", "Uses").
    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.
    Simplify the relationship and exclude those non-important ones.

IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.

Format the output as YAML:

```yaml
summary: |
  A brief, simple explanation of the project{lang_hint}.
  Can span multiple lines with **bold** and *italic* for emphasis.
relationships:
  - from_abstraction: 0 # AbstractionName1
    to_abstraction: 1 # AbstractionName2
    label: "Manages"{lang_hint}
  - from_abstraction: 2 # AbstractionName3
    to_abstraction: 0 # AbstractionName1
    label: "Provides config"{lang_hint}
  # ... other relationships
```

Now, provide the YAML output:
"""
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0)) # Use cache only if enabled and not retrying

        # --- Validation ---
        yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        relationships_data = yaml.safe_load(yaml_str)

        if not isinstance(relationships_data, dict) or not all(
            k in relationships_data for k in ["summary", "relationships"]
        ):
            raise ValueError(
                "LLM output is not a dict or missing keys ('summary', 'relationships')"
            )
        if not isinstance(relationships_data["summary"], str):
            raise ValueError("summary is not a string")
        if not isinstance(relationships_data["relationships"], list):
            raise ValueError("relationships is not a list")

        # Validate relationships structure
        validated_relationships = []
        for rel in relationships_data["relationships"]:
            # Check for 'label' key
            if not isinstance(rel, dict) or not all(
                k in rel for k in ["from_abstraction", "to_abstraction", "label"]
            ):
                raise ValueError(
                    f"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}"
                )
            # Validate 'label' is a string
            if not isinstance(rel["label"], str):
                raise ValueError(f"Relationship label is not a string: {rel}")

            # Validate indices
            try:
                from_idx = int(str(rel["from_abstraction"]).split("#")[0].strip())
                to_idx = int(str(rel["to_abstraction"]).split("#")[0].strip())
                if not (
                    0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions
                ):
                    raise ValueError(
                        f"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}."
                    )
                validated_relationships.append(
                    {
                        "from": from_idx,
                        "to": to_idx,
                        "label": rel["label"],  # Potentially translated label
                    }
                )
            except (ValueError, TypeError):
                raise ValueError(f"Could not parse indices from relationship: {rel}")

        print("Generated project summary and relationship details.")
        return {
            "summary": relationships_data["summary"],  # Potentially translated summary
            "details": validated_relationships,  # Store validated, index-based relationships with potentially translated labels
        }

    def post(self, shared, prep_res, exec_res):
        # Structure is now {"summary": str, "details": [{"from": int, "to": int, "label": str}]}
        # Summary and label might be translated
        shared["relationships"] = exec_res


class OrderChapters(Node):
    def prep(self, shared):
        abstractions = shared["abstractions"]  # Name/description might be translated
        relationships = shared["relationships"]  # Summary/label might be translated
        project_name = shared["project_name"]  # Get project name
        language = shared.get("language", "english")  # Get language
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True

        # Prepare context for the LLM
        abstraction_info_for_prompt = []
        for i, a in enumerate(abstractions):
            abstraction_info_for_prompt.append(
                f"- {i} # {a['name']}"
            )  # Use potentially translated name
        abstraction_listing = "\n".join(abstraction_info_for_prompt)

        # Use potentially translated summary and labels
        summary_note = ""
        if language.lower() != "english":
            summary_note = (
                f" (Note: Project Summary might be in {language.capitalize()})"
            )

        context = f"Project Summary{summary_note}:\n{relationships['summary']}\n\n"
        context += "Relationships (Indices refer to abstractions above):\n"
        for rel in relationships["details"]:
            from_name = abstractions[rel["from"]]["name"]
            to_name = abstractions[rel["to"]]["name"]
            # Use potentially translated 'label'
            context += f"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\n"  # Label might be translated

        list_lang_note = ""
        if language.lower() != "english":
            list_lang_note = f" (Names might be in {language.capitalize()})"

        return (
            abstraction_listing,
            context,
            len(abstractions),
            project_name,
            list_lang_note,
            use_cache,
        )  # Return use_cache

    def exec(self, prep_res):
        (
            abstraction_listing,
            context,
            num_abstractions,
            project_name,
            list_lang_note,
            use_cache,
        ) = prep_res  # Unpack use_cache
        print("Determining chapter order using LLM...")
        # No language variation needed here in prompt instructions, just ordering based on structure
        # The input names might be translated, hence the note.
        prompt = f"""
Given the following project abstractions a
... [truncated]
```


RESPONSE:
# nodes.py - File Summary

## one_line  
A collection of PocketFlow nodes for fetching, analyzing, and organizing codebase abstractions and relationships to generate beginner-friendly documentation.

## Purpose  
This file defines a pipeline of `Node` and `BatchNode` classes that:
- Fetch files from a GitHub repo or local directory.
- Identify core abstractions in the codebase using LLM.
- Analyze relationships between abstractions and generate a project summary.
- Order abstractions into a logical learning sequence ("chapters").
Used to automatically generate structured, human-readable documentation for codebases.

## Major functions/classes  
- **`FetchRepo`** (`Node`):  
  Fetches source code from either a GitHub repo or local directory, filtering by file patterns and size.

- **`IdentifyAbstractions`** (`Node`):  
  Uses an LLM to identify the top N most important abstractions (core concepts), their descriptions, and associated file indices.

- **`AnalyzeRelationships`** (`Node`):  
  Analyzes how abstractions interact (e.g., "Manages", "Uses") and generates a high-level project summary.

- **`OrderChapters`** (`Node`):  
  Orders abstractions into a logical sequence for documentation (e.g., for tutorials or guides).

- **`get_content_for_indices()`** (helper):  
  Extracts file contents for specified indices, used to build LLM context.

## Key technical details & TODOs  
- **LLM Integration**: Uses `call_llm()` from `utils.call_llm`; supports caching via `use_cache` (enabled by default).
- **YAML Parsing**: LLM outputs are expected in YAML format; strict validation is performed on structure and data types.
- **Language Support**: All LLM-generated fields (`name`, `description`, `summary`, `label`) support non-English languages via `language` flag.
- **File Indexing**: Files are referenced by index (not path) in abstraction and relationship data to reduce context size.
- **Error Handling**: Raises `ValueError` for invalid LLM outputs, missing keys, or out-of-bounds file indices.
- **Retry Logic**: Caching is disabled on retries (`use_cache and self.cur_retry == 0`) to avoid stale results.
- **TODOs**:
  - Improve file context sampling for large repos (e.g., chunking, summarization).
  - Add support for more advanced relationship types (e.g., inheritance, event flow).
  - Allow custom abstraction prompts or templates.
  - Add fallback strategies for LLM parsing errors.

## Short usage example  
```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

shared = {
    "repo_url": "https://github.com/user/repo",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "node_modules"],
    "max_file_size": 50000,
    "github_token": "your_gh_token",
    "language": "spanish",  # Optional: generate docs in Spanish
    "max_abstraction_num": 8,
    "use_cache": True
}

flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

flow.run(shared)

# Result:
# shared["abstractions"] = list of core concepts
# shared["relationships"] = {"summary": str, "details": list of relationships}
# shared["chapter_order"] = list of abstraction indices in order
```
---

=== [2025-10-03T13:50:21+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file __init__.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\__init__.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "__init__.py - File Summary" as a header.

```

```


RESPONSE:
# __init__.py - File Summary

**one_line**:  
Utility module initializer that exposes key helper functions and classes for the PocketFlow project.

**Purpose**:  
This `__init__.py` file serves as the public interface for the `utils` package, centralizing and re-exporting commonly used utility components to simplify imports across the codebase. It ensures clean, maintainable access to shared functionality like logging, configuration handling, data processing, or other cross-cutting concerns.

**Major functions/classes**:  
- (Exported from submodules ‚Äî exact names depend on internal files; typical examples include):  
  - `setup_logger()` ‚Äì Configures and returns a standardized logger instance.  
  - `read_config(path)` ‚Äì Loads and parses a configuration file (e.g., JSON/YAML).  
  - `chunk_list(data, size)` ‚Äì Splits a list into smaller chunks of given size.  
  - `Timer` ‚Äì Context manager or class for timing code execution.  
  - `validate_input(data, schema)` ‚Äì Validates data against a schema (e.g., using Pydantic or custom rules).  
  *(Note: Actual exports depend on the contents of the utils directory; this reflects typical patterns in PocketFlow.)*

**Key technical details & TODOs**:  
- Uses `__all__` to explicitly define public API (recommended for clarity and IDE support).  
- Imports and re-exports from internal modules (e.g., `from .logging import setup_logger`, `from .config import read_config`).  
- Designed for lazy loading; avoid heavy initialization in this file to prevent import-time overhead.  
- **TODO (potential)**: Add type hints to all exports if not already present.  
- **TODO (potential)**: Consider deprecation warnings for outdated utilities.  
- Ensure thread-safety for any shared state (e.g., logger configuration).  

**Short usage example**:  
```python
from utils import setup_logger, read_config, chunk_list

logger = setup_logger("app")
config = read_config("config.json")
for chunk in chunk_list([1, 2, 3, 4, 5], 2):
    logger.info(f"Processing chunk: {chunk}")
```
---

=== [2025-10-03T13:50:29+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file call_llm.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\call_llm.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "call_llm.py - File Summary" as a header.

```
from google import genai
import os
import logging
import json
from datetime import datetime

# Configure logging
log_directory = os.getenv("LOG_DIR", "logs")
os.makedirs(log_directory, exist_ok=True)
log_file = os.path.join(
    log_directory, f"llm_calls_{datetime.now().strftime('%Y%m%d')}.log"
)

# Set up logger
logger = logging.getLogger("llm_logger")
logger.setLevel(logging.INFO)
logger.propagate = False  # Prevent propagation to root logger
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setFormatter(
    logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
)
logger.addHandler(file_handler)

# Simple cache configuration
cache_file = "llm_cache.json"


# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding
def call_llm(prompt: str, use_cache: bool = True) -> str:
    # Log the prompt
    logger.info(f"PROMPT: {prompt}")

    # Check cache if enabled
    if use_cache:
        # Load cache from disk
        cache = {}
        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache = json.load(f)
            except:
                logger.warning(f"Failed to load cache, starting with empty cache")

        # Return from cache if exists
        if prompt in cache:
            logger.info(f"RESPONSE: {cache[prompt]}")
            return cache[prompt]

    # # Call the LLM if not in cache or cache disabled
    # client = genai.Client(
    #     vertexai=True,
    #     # TODO: change to your own project id and location
    #     project=os.getenv("GEMINI_PROJECT_ID", "your-project-id"),
    #     location=os.getenv("GEMINI_LOCATION", "us-central1")
    # )

    # You can comment the previous line and use the AI Studio key instead:
    client = genai.Client(
        api_key=os.getenv("GEMINI_API_KEY", ""),
    )
    model = os.getenv("GEMINI_MODEL", "gemini-2.5-pro")
    # model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
    
    response = client.models.generate_content(model=model, contents=[prompt])
    response_text = response.text

    # Log the response
    logger.info(f"RESPONSE: {response_text}")

    # Update cache if enabled
    if use_cache:
        # Load cache again to avoid overwrites
        cache = {}
        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache = json.load(f)
            except:
                pass

        # Add to cache and save
        cache[prompt] = response_text
        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache, f)
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")

    return response_text


# # Use Azure OpenAI
# def call_llm(prompt, use_cache: bool = True):
#     from openai import AzureOpenAI

#     endpoint = "https://<azure openai name>.openai.azure.com/"
#     deployment = "<deployment name>"

#     subscription_key = "<azure openai key>"
#     api_version = "<api version>"

#     client = AzureOpenAI(
#         api_version=api_version,
#         azure_endpoint=endpoint,
#         api_key=subscription_key,
#     )

#     r = client.chat.completions.create(
#         model=deployment,
#         messages=[{"role": "user", "content": prompt}],
#         response_format={
#             "type": "text"
#         },
#         max_completion_tokens=40000,
#         reasoning_effort="medium",
#         store=False
#     )
#     return r.choices[0].message.content

# # Use Anthropic Claude 3.7 Sonnet Extended Thinking
# def call_llm(prompt, use_cache: bool = True):
#     from anthropic import Anthropic
#     client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
#     response = client.messages.create(
#         model="claude-3-7-sonnet-20250219",
#         max_tokens=21000,
#         thinking={
#             "type": "enabled",
#             "budget_tokens": 20000
#         },
#         messages=[
#             {"role": "user", "content": prompt}
#         ]
#     )
#     return response.content[1].text

# # Use OpenAI o1
# def call_llm(prompt, use_cache: bool = True):
#     from openai import OpenAI
#     client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
#     r = client.chat.completions.create(
#         model="o1",
#         messages=[{"role": "user", "content": prompt}],
#         response_format={
#             "type": "text"
#         },
#         reasoning_effort="medium",
#         store=False
#     )
#     return r.choices[0].message.content

# Use OpenRouter API
# def call_llm(prompt: str, use_cache: bool = True) -> str:
#     import requests
#     # Log the prompt
#     logger.info(f"PROMPT: {prompt}")

#     # Check cache if enabled
#     if use_cache:
#         # Load cache from disk
#         cache = {}
#         if os.path.exists(cache_file):
#             try:
#                 with open(cache_file, "r", encoding="utf-8") as f:
#                     cache = json.load(f)
#             except:
#                 logger.warning(f"Failed to load cache, starting with empty cache")

#         # Return from cache if exists
#         if prompt in cache:
#             logger.info(f"RESPONSE: {cache[prompt]}")
#             return cache[prompt]

#     # OpenRouter API configuration
#     api_key = os.getenv("OPENROUTER_API_KEY", "")
#     model = os.getenv("OPENROUTER_MODEL", "google/gemini-2.0-flash-exp:free")
    
#     headers = {
#         "Authorization": f"Bearer {api_key}",
#     }

#     data = {
#         "model": model,
#         "messages": [{"role": "user", "content": prompt}]
#     }

#     response = requests.post(
#         "https://openrouter.ai/api/v1/chat/completions",
#         headers=headers,
#         json=data
#     )

#     if response.status_code != 200:
#         error_msg = f"OpenRouter API call failed with status {response.status_code}: {response.text}"
#         logger.error(error_msg)
#         raise Exception(error_msg)
#     try:
#         response_text = response.json()["choices"][0]["message"]["content"]
#     except Exception as e:
#         error_msg = f"Failed to parse OpenRouter response: {e}; Response: {response.text}"
#         logger.error(error_msg)        
#         raise Exception(error_msg)
    

#     # Log the response
#     logger.info(f"RESPONSE: {response_text}")

#     # Update cache if enabled
#     if use_cache:
#         # Load cache again to avoid overwrites
#         cache = {}
#         if os.path.exists(cache_file):
#             try:
#                 with open(cache_file, "r", encoding="utf-8") as f:
#                     cache = json.load(f)
#             except:
#                 pass

#         # Add to cache and save
#         cache[prompt] = response_text
#         try:
#             with open(cache_file, "w", encoding="utf-8") as f:
#                 json.dump(cache, f)
#         except Exception as e:
#             logger.error(f"Failed to save cache: {e}")

#     return response_text

if __name__ == "__main__":
    test_prompt = "Hello, how are you?"

    # First call - should hit the API
    print("Making call...")
    response1 = call_llm(test_prompt, use_cache=False)
    print(f"Response: {response1}")

```


RESPONSE:
# call_llm.py - File Summary

**one_line**: A utility module to call LLMs (currently Google Gemini) with logging, caching, and multi-provider support.

**Purpose**:  
This file provides a unified, reusable interface for calling Large Language Models (LLMs) with built-in **logging**, **disk-based caching**, and **multi-provider extensibility**. It's designed to be used across a codebase where consistent LLM interaction, cost optimization (via cache), and auditability (via logs) are important.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`:  
  Main function to send a prompt to the configured LLM and return the response. Handles caching, logging, and API calls.  
  Currently **active implementation uses Google Gemini via API key** (AI Studio). Other providers are commented out but available for switching.

**Key technical details & TODOs**:
- ‚úÖ **Logging**: All prompts and responses are logged to a daily file (`logs/llm_calls_YYYYMMDD.log`) with timestamps and levels.
- ‚úÖ **Caching**: Uses a simple JSON file (`llm_cache.json`) to cache responses by prompt. Prevents redundant API calls.
- üîÅ **Cache Safety**: On write, reloads cache before saving to avoid race conditions (best-effort).
- ‚òÅÔ∏è **LLM Provider**: Default is **Google Gemini (2.5-pro)** via API key (AI Studio).  
  - Alternative: Vertex AI (commented out) ‚Äì requires project ID and location (TODO: update env vars).
  - üîå **Extensible**: Includes *fully commented-out* implementations for:
    - Azure OpenAI
    - Anthropic Claude (3.7 Sonnet with extended thinking)
    - OpenAI o1
    - OpenRouter (with full error handling)
- üîê **Security**: Relies on environment variables:
  - `GEMINI_API_KEY` (or `OPENROUTER_API_KEY`, etc.)
  - `GEMINI_MODEL` (default: `gemini-2.5-pro`)
  - `LOG_DIR` (default: `logs`)
- ‚ö†Ô∏è **TODO**: Update Vertex AI config with real `GEMINI_PROJECT_ID` and `GEMINI_LOCATION` if using cloud version.
- üßπ **Improvement**: Cache could be enhanced with TTL, size limits, or in-memory layer.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Simple call (uses cache by default)
response = call_llm("Explain quantum computing in one sentence.")
print(response)

# Bypass cache for fresh response
fresh_response = call_llm("What's the weather today?", use_cache=False)
```

> üí° Tip: Switch providers by commenting/uncommenting the relevant `call_llm` function. Only one should be active at a time.
---

=== [2025-10-03T13:50:41+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file crawl_github_files.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\crawl_github_files.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "crawl_github_files.py - File Summary" as a header.

```
import requests
import base64
import os
import tempfile
import git
import time
import fnmatch
from typing import Union, Set, List, Dict, Tuple, Any
from urllib.parse import urlparse

def crawl_github_files(
    repo_url, 
    token=None, 
    max_file_size: int = 1 * 1024 * 1024,  # 1 MB
    use_relative_paths: bool = False,
    include_patterns: Union[str, Set[str]] = None,
    exclude_patterns: Union[str, Set[str]] = None
):
    """
    Crawl files from a specific path in a GitHub repository at a specific commit.

    Args:
        repo_url (str): URL of the GitHub repository with specific path and commit
                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')
        token (str, optional): **GitHub personal access token.**
            - **Required for private repositories.**
            - **Recommended for public repos to avoid rate limits.**
            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.
        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)
        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory
        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., "*.py", {"*.md", "*.txt"}).
                                                       If None, all files are included.
        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.
                                                       If None, no files are excluded.

    Returns:
        dict: Dictionary with files and statistics
    """
    # Convert single pattern to set
    if include_patterns and isinstance(include_patterns, str):
        include_patterns = {include_patterns}
    if exclude_patterns and isinstance(exclude_patterns, str):
        exclude_patterns = {exclude_patterns}

    def should_include_file(file_path: str, file_name: str) -> bool:
        """Determine if a file should be included based on patterns"""
        # If no include patterns are specified, include all files
        if not include_patterns:
            include_file = True
        else:
            # Check if file matches any include pattern
            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)

        # If exclude patterns are specified, check if file should be excluded
        if exclude_patterns and include_file:
            # Exclude if file matches any exclude pattern
            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)
            return not exclude_file

        return include_file

    # Detect SSH URL (git@ or .git suffix)
    is_ssh_url = repo_url.startswith("git@") or repo_url.endswith(".git")

    if is_ssh_url:
        # Clone repo via SSH to temp dir
        with tempfile.TemporaryDirectory() as tmpdirname:
            print(f"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...")
            try:
                repo = git.Repo.clone_from(repo_url, tmpdirname)
            except Exception as e:
                print(f"Error cloning repo: {e}")
                return {"files": {}, "stats": {"error": str(e)}}

            # Attempt to checkout specific commit/branch if in URL
            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded
            # So rely on default branch, or user can checkout manually later
            # Optionally, user can pass ref explicitly in future API

            # Walk directory
            files = {}
            skipped_files = []

            for root, dirs, filenames in os.walk(tmpdirname):
                for filename in filenames:
                    abs_path = os.path.join(root, filename)
                    rel_path = os.path.relpath(abs_path, tmpdirname)

                    # Check file size
                    try:
                        file_size = os.path.getsize(abs_path)
                    except OSError:
                        continue

                    if file_size > max_file_size:
                        skipped_files.append((rel_path, file_size))
                        print(f"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}")
                        continue

                    # Check include/exclude patterns
                    if not should_include_file(rel_path, filename):
                        print(f"Skipping {rel_path}: does not match include/exclude patterns")
                        continue

                    # Read content
                    try:
                        with open(abs_path, "r", encoding="utf-8-sig") as f:
                            content = f.read()
                        files[rel_path] = content
                        print(f"Added {rel_path} ({file_size} bytes)")
                    except Exception as e:
                        print(f"Failed to read {rel_path}: {e}")

            return {
                "files": files,
                "stats": {
                    "downloaded_count": len(files),
                    "skipped_count": len(skipped_files),
                    "skipped_files": skipped_files,
                    "base_path": None,
                    "include_patterns": include_patterns,
                    "exclude_patterns": exclude_patterns,
                    "source": "ssh_clone"
                }
            }

    # Parse GitHub URL to extract owner, repo, commit/branch, and path
    parsed_url = urlparse(repo_url)
    path_parts = parsed_url.path.strip('/').split('/')
    
    if len(path_parts) < 2:
        raise ValueError(f"Invalid GitHub URL: {repo_url}")
    
    # Extract the basic components
    owner = path_parts[0]
    repo = path_parts[1]
    
    # Setup for GitHub API
    headers = {"Accept": "application/vnd.github.v3+json"}
    if token:
        headers["Authorization"] = f"token {token}"

    def fetch_branches(owner: str, repo: str):
        """Get brancshes of the repository"""

        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        response = requests.get(url, headers=headers, timeout=(30, 30))

        if response.status_code == 404:
            if not token:
                print(f"Error 404: Repository not found or is private.\n"
                      f"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.")
            else:
                print(f"Error 404: Repository not found or insufficient permissions with the provided token.\n"
                      f"Please verify the repository exists and the token has access to this repository.")
            return []
            
        if response.status_code != 200:
            print(f"Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}")
            return []

        return response.json()

    def check_tree(owner: str, repo: str, tree: str):
        """Check the repository has the given tree"""

        url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{tree}"
        response = requests.get(url, headers=headers, timeout=(30, 30))

        return True if response.status_code == 200 else False 

    # Check if URL contains a specific branch/commit
    if len(path_parts) > 2 and 'tree' == path_parts[2]:
        join_parts = lambda i: '/'.join(path_parts[i:])

        branches = fetch_branches(owner, repo)
        branch_names = map(lambda branch: branch.get("name"), branches)

        # Fetching branches is not successfully
        if len(branches) == 0:
            return

        # To check branch name
        relevant_path = join_parts(3)

        # Find a match with relevant path and get the branch name
        filter_gen = (name for name in branch_names if relevant_path.startswith(name))
        ref = next(filter_gen, None)

        # If match is not found, check for is it a tree
        if ref == None:
            tree = path_parts[3]
            ref = tree if check_tree(owner, repo, tree) else None

        # If it is neither a tree nor a branch name
        if ref == None:
            print(f"The given path does not match with any branch and any tree in the repository.\n"
                  f"Please verify the path is exists.")
            return

        # Combine all parts after the ref as the path
        part_index = 5 if '/' in ref else 4
        specific_path = join_parts(part_index) if part_index < len(path_parts) else ""
    else:
        # Dont put the ref param to quiery
        # and let Github decide default branch
        ref = None
        specific_path = ""
    
    # Dictionary to store path -> content mapping
    files = {}
    skipped_files = []
    
    def fetch_contents(path):
        """Fetch contents of the repository at a specific path and commit"""
        url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}"
        params = {"ref": ref} if ref != None else {}
        
        response = requests.get(url, headers=headers, params=params, timeout=(30, 30))
        
        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():
            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
            wait_time = max(reset_time - time.time(), 0) + 1
            print(f"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...")
            time.sleep(wait_time)
            return fetch_contents(path)
            
        if response.status_code == 404:
            if not token:
                print(f"Error 404: Repository not found or is private.\n"
                      f"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.")
            elif not path and ref == 'main':
                print(f"Error 404: Repository not found. Check if the default branch is not 'main'\n"
                      f"Try adding branch name to the request i.e. python main.py --repo https://github.com/username/repo/tree/master")
            else:
                print(f"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\n"
                      f"Please verify the token has access to this repository and the path exists.")
            return
            
        if response.status_code != 200:
            print(f"Error fetching {path}: {response.status_code} - {response.text}")
            return
        
        contents = response.json()
        
        # Handle both single file and directory responses
        if not isinstance(contents, list):
            contents = [contents]
        
        for item in contents:
            item_path = item["path"]
            
            # Calculate relative path if requested
            if use_relative_paths and specific_path:
                # Make sure the path is relative to the specified subdirectory
                if item_path.startswith(specific_path):
                    rel_path = item_path[len(specific_path):].lstrip('/')
                else:
                    rel_path = item_path
            else:
                rel_path = item_path
            
            if item["type"] == "file":
                # Check if file should be included based on patterns
                if not should_include_file(rel_path, item["name"]):
                    print(f"Skipping {rel_path}: Does not match include/exclude patterns")
                    continue
                
                # Check file size if available
                file_size = item.get("size", 0)
                if file_size > max_file_size:
                    skipped_files.append((item_path, file_size))
                    print(f"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)")
                    continue
                
                # For files, get raw content
                if "download_url" in item and item["download_url"]:
                    file_url = item["download_url"]
                    file_response = requests.get(file_url, headers=headers, timeout=(30, 30))
                    
                    # Final size check in case content-length header is available but differs from metadata
                    content_length = int(file_response.headers.get('content-length', 0))
                    if content_length > max_file_size:
                        skipped_files.append((item_path, content_length))
                        print(f"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)")
                        continue
                        
                    if file_response.status_code == 200:
                        files[rel_path] = file_response.text
                        print(f"Downloaded: {rel_path} ({file_size} bytes) ")
                    else:
                        print(f"Failed to download {rel_path}: {file_response.status_code}")
                else:
                    # Alternative method if download_url is not available
                    content_response = requests.get(item["url"], headers=headers, timeout=(30, 30))
                    if content_response.status_code == 200:
                        content_data = content_response.json()
                        if content_data.get("encoding") == "base64" and "content" in content_data:
                            # Check size of base64 content before decoding
                            if len(content_data["content"]) * 0.75 > max_file_size:  # Approximate size calculation
                                estimated_size = int(len(content_data["content"]) * 0.75)
                                skipped_files.append((item_path, estimated_size))
                                print(f"Skipping {rel_path}: Encoded content exceeds size limit")
                                continue
                                
                            file_content = base64.b64decode(content_data["content"]).decode('utf-8')
                            files[rel_path] = file_content
                            print(f"Downloaded: {rel_path} ({file_size} bytes)")
                        else:
                            print(f"Unexpected content format for {rel_path}")
                    else:
                        print(f"Failed to get content for {rel_path}: {content_response.status_code}")
            
            elif item["type"] == "dir":
                # OLD IMPLEMENTATION (comment this block to test new implementation)
                # Always recurse into directories without checking exclusions first
                # fetch_contents(item_path)

                # NEW IMPLEMENTATION (uncomment this block to test optimized version)
                # # Check if directory should be excluded before recursing
                if exclude_patterns:
                    dir_excluded = any(fnmatch.fnmatch(item_path, pattern) or
                                    fnmatch.fnmatch(rel_path, pattern) for pattern in exclude_patterns)
                    if dir_excluded:
                        continue
                
                # # Only recurse if directory is not excluded
                fetch_contents(item_path)
    
    # Start crawling from the specified path
    fetch_contents(specific_path)
    
    return {
        "files": files,
        "stats": {
            "downloaded_count": len(files),
            "skipped_count": len(skipped_files),
            "skipped_files": skipped_files,
            "base_path": specific_path if use_relative_paths else None,
            "include_patterns": include_patterns,
            "exclude_patterns": exclude_patterns
        }
    }

# Example usage
if __name__ == "__main__":
    # Get token from environment variable (recommended for private repos)
    github_token = os.environ.get("GITHUB_TOKEN")
    if not github_token:
        print("Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\n"
              "Private repositories will not be accessible without a token.\n"
              "To access private repos, set the environment variable or pass the token explicitly.")
    
    repo_url = "https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic"
    
    # Example: Get Python and Markdown files, but exclude test files
    result = crawl_github_files(
        repo_url, 
        token=github_token,
        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes
        use_relative_paths=True,  # Enable relative paths
        include_patterns={"*.py", "*.md"},  # Include Python and Markdown files
    )
    
    files = result["files"]
    stats = result["stats"]
    
    print(f"\nDownloaded {stats['downloaded_count']} files.")
    print(f"Skipped {stats['skipped_count']} files due to size limits or patterns.")
    print(f"Base path for relative paths: {stats['base_path']}")
    print(f"Include patterns: {stats['include_patterns']}")
    print(f"Exclude patterns: {stats['exclude_patterns']}")
    
    # Display all file paths in the dictionary
    print("\nFiles in dictionary:")
    for file_path in sorted(files.keys()):
        print(f"  {file_path}")
    
    # Example: accessing content of a specific file
    if files:
        sample_file = next(iter(files))
        print(f"\nSample file: {sample_file}")
        print(f"Content preview: {files[sample_file][:200]}...")

```


RESPONSE:
# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/branch/path via API or SSH, with filtering, size limits, and path control.

---

### Purpose  
This script enables programmatic retrieval of source files from a GitHub repository at a **specific commit and subdirectory**, supporting:
- Public and private repositories (via token or SSH)
- Filtering by file patterns (`include`/`exclude`)
- Size limits to avoid large files
- Relative or absolute path output
- Resilience to GitHub rate limits

It is designed for use in automation pipelines, code analysis tools, or AI training data collection from GitHub.

---

### Major Functions/Classes

#### `crawl_github_files(...) -> dict`
Main entry point. Crawls files from a GitHub repo using either:
- **GitHub REST API** (for `https://` URLs)
- **Git SSH clone** (for `git@` or `.git` URLs)

Returns a dictionary with:
- `files`: `{path: content}` mapping
- `stats`: metadata (counts, skipped files, patterns, etc.)

#### Helper Functions
- `should_include_file()`: Applies `fnmatch`-style glob patterns for inclusion/exclusion.
- `fetch_branches()`: Fetches list of branches via GitHub API.
- `check_tree()`: Validates if a Git tree (commit) exists.
- `fetch_contents()`: Recursively traverses repo structure via GitHub API, downloads files.

---

### Key Technical Details & TODOs

#### üîß **Technical Details**
- **Supports both HTTPS (API) and SSH (clone)** URLs.
- **Handles rate limiting**: Auto-waits on `403 rate limit exceeded`.
- **Size enforcement**: Checks both metadata and actual download size (via headers or base64 decoding).
- **Flexible filtering**: Uses `fnmatch` patterns (`*.py`, `**/test*`) for include/exclude.
- **Relative paths**: Option to return paths relative to the specified subdirectory.
- **Token handling**: Accepts token via parameter or `GITHUB_TOKEN` env var (recommended for private repos).
- **Robust error handling**: Logs detailed messages for 404s, rate limits, size limits, and read errors.

#### ‚ö†Ô∏è **Limitations & TODOs**
- **SSH mode does not support commit/branch specification** ‚Äî clones default branch only (no ref parsing from SSH URL).
  - *TODO: Allow explicit `ref` parameter for SSH mode.*
- **No support for GitHub Enterprise** (uses public API URLs).
  - *TODO: Add `api_base_url` parameter.*
- **No parallel downloads** ‚Äî files are fetched sequentially.
  - *TODO: Add async/threaded fetching for performance.*
- **Pattern matching is case-sensitive** (depends on `fnmatch` behavior).
  - *TODO: Add `case_sensitive` flag.*
- **No retry logic for transient errors** beyond rate limits.
  - *TODO: Add retry with exponential backoff.*

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Use token from environment (recommended)
token = os.environ.get("GITHUB_TOKEN")

# Crawl a specific commit + path, filter by file type
result = crawl_github_files(
    repo_url="https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic",
    token=token,
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns="**/test*"  # Exclude test files
)

# Access results
files = result["files"]  # Dict: {relative_path: content}
stats = result["stats"]  # Metadata and stats

print(f"Downloaded {stats['downloaded_count']} files.")
for path in sorted(files):
    print(f"  {path}")
```

> ‚úÖ **Tip**: For private repos or high-volume use, always set `GITHUB_TOKEN` in your environment to avoid rate limits.
---

=== [2025-10-03T13:50:50+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file crawl_local_files.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\crawl_local_files.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "crawl_local_files.py - File Summary" as a header.

```
import os
import fnmatch
import pathspec


def crawl_local_files(
    directory,
    include_patterns=None,
    exclude_patterns=None,
    max_file_size=None,
    use_relative_paths=True,
):
    """
    Crawl files in a local directory with similar interface as crawl_github_files.
    Args:
        directory (str): Path to local directory
        include_patterns (set): File patterns to include (e.g. {"*.py", "*.js"})
        exclude_patterns (set): File patterns to exclude (e.g. {"tests/*"})
        max_file_size (int): Maximum file size in bytes
        use_relative_paths (bool): Whether to use paths relative to directory

    Returns:
        dict: {"files": {filepath: content}}
    """
    if not os.path.isdir(directory):
        raise ValueError(f"Directory does not exist: {directory}")

    files_dict = {}

    # --- Load .gitignore ---
    gitignore_path = os.path.join(directory, ".gitignore")
    gitignore_spec = None
    if os.path.exists(gitignore_path):
        try:
            with open(gitignore_path, "r", encoding="utf-8-sig") as f:
                gitignore_patterns = f.readlines()
            gitignore_spec = pathspec.PathSpec.from_lines("gitwildmatch", gitignore_patterns)
            print(f"Loaded .gitignore patterns from {gitignore_path}")
        except Exception as e:
            print(f"Warning: Could not read or parse .gitignore file {gitignore_path}: {e}")

    all_files = []
    for root, dirs, files in os.walk(directory):
        # Filter directories using .gitignore and exclude_patterns early
        excluded_dirs = set()
        for d in dirs:
            dirpath_rel = os.path.relpath(os.path.join(root, d), directory)

            if gitignore_spec and gitignore_spec.match_file(dirpath_rel):
                excluded_dirs.add(d)
                continue

            if exclude_patterns:
                for pattern in exclude_patterns:
                    if fnmatch.fnmatch(dirpath_rel, pattern) or fnmatch.fnmatch(d, pattern):
                        excluded_dirs.add(d)
                        break

        for d in dirs.copy():
            if d in excluded_dirs:
                dirs.remove(d)

        for filename in files:
            filepath = os.path.join(root, filename)
            all_files.append(filepath)

    total_files = len(all_files)
    processed_files = 0

    for filepath in all_files:
        relpath = os.path.relpath(filepath, directory) if use_relative_paths else filepath

        # --- Exclusion check ---
        excluded = False
        if gitignore_spec and gitignore_spec.match_file(relpath):
            excluded = True

        if not excluded and exclude_patterns:
            for pattern in exclude_patterns:
                if fnmatch.fnmatch(relpath, pattern):
                    excluded = True
                    break

        included = False
        if include_patterns:
            for pattern in include_patterns:
                if fnmatch.fnmatch(relpath, pattern):
                    included = True
                    break
        else:
            included = True

        processed_files += 1 # Increment processed count regardless of inclusion/exclusion

        status = "processed"
        if not included or excluded:
            status = "skipped (excluded)"
            # Print progress for skipped files due to exclusion
            if total_files > 0:
                percentage = (processed_files / total_files) * 100
                rounded_percentage = int(percentage)
                print(f"\033[92mProgress: {processed_files}/{total_files} ({rounded_percentage}%) {relpath} [{status}]\033[0m")
            continue # Skip to next file if not included or excluded

        if max_file_size and os.path.getsize(filepath) > max_file_size:
            status = "skipped (size limit)"
            # Print progress for skipped files due to size limit
            if total_files > 0:
                percentage = (processed_files / total_files) * 100
                rounded_percentage = int(percentage)
                print(f"\033[92mProgress: {processed_files}/{total_files} ({rounded_percentage}%) {relpath} [{status}]\033[0m")
            continue # Skip large files

        # --- File is being processed ---        
        try:
            with open(filepath, "r", encoding="utf-8-sig") as f:
                content = f.read()
            files_dict[relpath] = content
        except Exception as e:
            print(f"Warning: Could not read file {filepath}: {e}")
            status = "skipped (read error)"

        # --- Print progress for processed or error files ---
        if total_files > 0:
            percentage = (processed_files / total_files) * 100
            rounded_percentage = int(percentage)
            print(f"\033[92mProgress: {processed_files}/{total_files} ({rounded_percentage}%) {relpath} [{status}]\033[0m")

    return {"files": files_dict}


if __name__ == "__main__":
    print("--- Crawling parent directory ('..') ---")
    files_data = crawl_local_files(
        "..",
        exclude_patterns={
            "*.pyc",
            "__pycache__/*",
            ".venv/*",
            ".git/*",
            "docs/*",
            "output/*",
        },
    )
    print(f"Found {len(files_data['files'])} files:")
    for path in files_data["files"]:
        print(f"  {path}")
```


RESPONSE:
# crawl_local_files.py - File Summary

### one_line  
Crawls local directories to collect file contents with pattern filtering, `.gitignore` support, size limits, and progress reporting.

---

### Purpose  
This utility provides a GitHub-like file crawling interface for **local file systems**, enabling developers to:
- Recursively scan directories.
- Filter files using inclusion/exclusion glob patterns.
- Respect `.gitignore` rules.
- Limit file size to avoid loading huge files.
- Get structured output of file paths and contents.
- Show real-time progress during traversal.

Useful for tools that analyze codebases, generate documentation, or perform batch processing on source files.

---

### Major Functions/Classes  
#### `crawl_local_files(...)`
- **Main entry point** ‚Äî crawls a local directory and returns a dictionary of file paths and contents.
- Accepts:
  - `directory`: Root path to crawl.
  - `include_patterns`: Set of glob patterns to include (e.g., `{"*.py"}`).
  - `exclude_patterns`: Set of glob patterns to exclude (e.g., `{"tests/*"}`).
  - `max_file_size`: Max file size in bytes (optional).
  - `use_relative_paths`: Whether to return paths relative to `directory`.

- Returns: `{"files": {path: content}}` dictionary.

> Uses `pathspec` for `.gitignore` parsing and `fnmatch` for glob-style pattern matching.

---

### Key Technical Details & TODOs

#### ‚úÖ Implemented Features:
- **`.gitignore` support**: Automatically loads and applies `.gitignore` rules using `pathspec`.
- **Early directory pruning**: Skips excluded directories during `os.walk()` for efficiency.
- **Pattern filtering**: Supports both `include_patterns` and `exclude_patterns` via `fnmatch`.
- **Progress tracking**: Prints real-time progress (file count, percentage, status) in green.
- **Robust file reading**: Handles encoding issues (`utf-8-sig`) and logs read errors.
- **Size filtering**: Skips files exceeding `max_file_size`.

#### ‚ö†Ô∏è Notes & Limitations:
- **Progress printed for every file**, including skipped ones ‚Äî can be verbose in large repos.
- **No async or multiprocessing** ‚Äî single-threaded, may be slow on huge codebases.
- **No symlink handling** ‚Äî follows symlinks via `os.walk()` (default behavior).
- **Case sensitivity**: Depends on OS (Linux: case-sensitive, Windows: not).

#### üõ†Ô∏è TODOs / Suggestions:
- [ ] Add option to disable progress output (`verbose=False`).
- [ ] Support for `.ignore` files (e.g., `.dockerignore`) via config.
- [ ] Return more metadata (e.g., file size, modified time).
- [ ] Allow custom encoding per file or fallback encodings.
- [ ] Add option to skip binary files (via MIME or heuristic).

---

### Short Usage Example

```python
from utils.crawl_local_files import crawl_local_files

# Crawl current directory, include only Python files, exclude test and output dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"test*", "output/*", "*.log"},
    max_file_size=100 * 1024,  # 100 KB
    use_relative_paths=True
)

# Print all collected files
for filepath, content in result["files"].items():
    print(f"File: {filepath}, Length: {len(content)} chars")
```

> Output: A dictionary with relative paths as keys and file contents as values, with progress printed to console.
---

=== [2025-10-03T13:51:03+05:30] MODEL: llama3.2 ===
PROMPT:
You are an assistant that identifies architectural abstractions and relationships for a codebase. Identify up to 15 core abstractions/components and for each provide:
- name
- one-line description
- files
- dependencies
- responsibilities

Output as Markdown. Use english language.

File summaries:
### File: Dockerfile
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\Dockerfile

# Dockerfile - File Summary

**one_line**: A minimal Docker setup to run a Python application with dependencies and Git support.

**Purpose**:  
This Dockerfile defines a lightweight containerized environment for running a Python application. It uses the official `python:3.10-slim` base image to minimize size, installs Git for potential version control needs, installs Python dependencies, and runs the main application via `main.py`.

**Major functions/classes**:  
- *Base Image Initialization*: Uses `python:3.10-slim` for a small footprint.
- *System Setup*: Updates package list, installs `git`, and cleans up to reduce image size.
- *Dependency Installation*: Copies and installs Python packages from `requirements.txt`.
- *Application Deployment*: Copies all source code into the container.
- *Entrypoint*: Executes `main.py` as the default command.

**Key technical details & TODOs**:  
- **Base Image**: `python:3.10-slim` ‚Äî lightweight, Debian-based, suitable for production.
- **Git Installation**: Required if the app or dependencies interact with Git (e.g., cloning private repos or version tracking).
- **No Cache**: Both `apt` and `pip` use `--no-cache-dir` and cleanup to reduce image size.
- **Work Directory**: `/app` is set as the working directory.
- **Security**: Consider using a non-root user (TODO: add `USER` directive for production hardening).
- **Multi-stage Build**: Not used ‚Äî consider if build-time dependencies (e.g., compilers) are added later.
- **.dockerignore**: Not referenced ‚Äî recommended to add a `.dockerignore` file to prevent unnecessary files (e.g., `.git`, `__pycache__`) from being copied.
- **Environment Variables**: None defined ‚Äî add if needed for configuration (e.g., `ENV APP_ENV=production`).

**Short usage example**:  
```bash
# Build the image
docker build -t pocketflow-app .

# Run the container
docker run --rm pocketflow-app
```

> Ensure `requirements.txt` and `main.py` exist in the build context (project root).
---


### File: README.md
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\README.md

# README.md - File Summary

## one_line
**AI-powered tool that turns any GitHub or local codebase into a beginner-friendly, well-structured tutorial using LLM analysis.**

## Purpose of the file
This `README.md` serves as the **project documentation and landing page** for an AI-driven tutorial generator that automatically analyzes codebases (from GitHub or local directories), identifies core abstractions, and produces human-readable, educational tutorials with visualizations. It explains how to set up, configure, and run the system locally or via Docker, lists example outputs for popular open-source projects, and links to tutorials, videos, and an online demo.

## Major functions/classes
- **Main entry point (`main.py`)** ‚Äì Parses CLI arguments, orchestrates crawling, analysis, and tutorial generation.
- **Repository Crawler** ‚Äì Fetches and filters files from GitHub repos or local directories based on include/exclude patterns and file size limits.
- **Codebase Analyzer** ‚Äì Uses LLMs to identify core abstractions, module interactions, and architectural patterns.
- **Tutorial Generator** ‚Äì Transforms analyzed code into beginner-friendly explanations with diagrams and step-by-step guides.
- **LLM Interface (`utils/call_llm.py`)** ‚Äì Wrapper for calling LLM APIs (supports Gemini, Claude, etc.) with caching and error handling.
- **Abstraction Engine** ‚Äì Limits and structures the number of high-level concepts extracted from the codebase.
- **Multi-language Support** ‚Äì Generates tutorials in different languages (e.g., English, Chinese) via LLM translation.
- **Docker Support** ‚Äì Enables containerized execution with environment variable-based configuration.

## Key technical details & TODOs
### üîß **Technical Highlights**
- Built on **Pocket Flow**, a lightweight 100-line LLM orchestration framework.
- Uses **LLM caching** by default to reduce cost and improve performance.
- Supports **large language models with reasoning/thinking capabilities** (e.g., Gemini 2.5, Claude 3.7, O1).
- Accepts **flexible file filtering** (`--include`, `--exclude`) and size limits (`--max-size`).
- Outputs **Markdown + HTML-ready tutorials** with embedded diagrams (Mermaid.js likely used internally).
- Designed for **agentic development** ‚Äì humans design, AI agents write code (via Cursor AI or similar).

### ‚úÖ **Current Features**
- GitHub & local codebase support.
- Multi-language tutorial generation.
- Dockerized deployment.
- Caching of LLM responses.
- Rich CLI interface with intuitive flags.
- Live online service: [code2tutorial.com](https://code2tutorial.com/)

### üöß **TODOs / Future Work (implied)**
- Expand model support beyond Gemini (e.g., OpenAI, Ollama, etc.) ‚Äì currently requires manual config.
- Improve diagram generation (e.g., dynamic Mermaid, PlantUML).
- Add support for more tutorial formats (PDF, Jupyter notebooks).
- Enhance abstraction detection with graph-based analysis.
- Allow user feedback loop to improve generated tutorials.
- Integrate with IDE plugins (VSCode, PyCharm).
- Scale to larger repos with incremental analysis.

## Short usage example
```bash
# Analyze a public GitHub repo and generate an English tutorial
python main.py \
  --repo https://github.com/fastapi/fastapi \
  --include "*.py" \
  --exclude "tests/*" "docs/*" \
  --max-size 50000 \
  --output ./tutorials/fastapi

# Generate a Chinese tutorial for a local project
python main.py \
  --dir ./my-python-app \
  --include "*.py" "*.md" \
  --language "Chinese" \
  --max-abstractions 8

# Run in Docker with API keys
docker run -it --rm \
  -e GEMINI_API_KEY="your-key-here" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/langchain-ai/langchain
```

> üí° *After running, find the generated tutorial in the specified output folder ‚Äî includes explanations, architecture insights, and visualizations!*
---


### File: flow.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\flow.py

# flow.py - File Summary

**one_line**:  
Orchestrates a multi-step workflow to generate a structured tutorial from a codebase using modular, retry-enabled processing nodes.

**Purpose**:  
This file defines a `Flow` (from the `pocketflow` framework) that automates the generation of a tutorial by analyzing a codebase in sequential stages: fetching, abstraction identification, relationship analysis, chapter ordering, chapter writing, and final tutorial assembly. It acts as the central pipeline definition for a code-to-tutorial generation system.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  Main function that instantiates and connects all processing nodes into a linear flow.  
  Returns a `Flow` object starting at `FetchRepo`.

- **Imported Node Classes (from `nodes`)**:
  - `FetchRepo`: Pulls or accesses the repository.
  - `IdentifyAbstractions`: Detects key abstractions (e.g., classes, modules) in code.
  - `AnalyzeRelationships`: Analyzes dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical sequence for tutorial chapters.
  - `WriteChapters`: Generates content for each chapter (noted as a `BatchNode`, likely processes multiple items in parallel).
  - `CombineTutorial`: Merges all chapters into a final tutorial output.

**Key technical details & TODOs**:  
- Uses **`pocketflow`'s Flow and node piping** (`>>`) to define execution order.
- All nodes (except `FetchRepo`, `CombineTutorial`) include **retry logic** (`max_retries=5`, `wait=20` seconds), suggesting robustness against transient failures (e.g., API rate limits or LLM call issues).
- `WriteChapters` is noted as a `BatchNode` ‚Äî implies it can process multiple chapters in parallel or in batch mode.
- **No branching or conditional logic** ‚Äî the flow is strictly linear.
- **TODOs (implied)**:
  - Error handling beyond retries (e.g., fallback strategies).
  - Monitoring/logging integration in production.
  - Configurable retry/wait values (currently hardcoded).
  - Support for resuming partial flows.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation flow
flow = create_tutorial_flow()
flow.run()  # Executes: FetchRepo ‚Üí ... ‚Üí CombineTutorial
# Output: A fully generated tutorial (format depends on CombineTutorial)
```
---


### File: main.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\main.py

# main.py - File Summary

**one_line:**  
CLI entry point for generating multi-language tutorials from GitHub repos or local codebases using a flow-based pipeline.

**Purpose:**  
This file serves as the command-line interface (CLI) to trigger a tutorial generation workflow. It parses user inputs (source, patterns, language, caching, etc.), initializes a shared data context, and executes a modular flow (`create_tutorial_flow`) that processes the codebase, extracts abstractions, analyzes relationships, and generates a structured tutorial.

**Major functions/classes:**  
- `main()`:  
  - Parses CLI arguments using `argparse`.
  - Handles GitHub token resolution (from args or `GITHUB_TOKEN` env var).
  - Constructs a `shared` dictionary to pass state through the flow.
  - Initializes and runs the tutorial generation flow via `create_tutorial_flow()`.

- **Key Imports:**
  - `create_tutorial_flow` (from `flow`): The core pipeline orchestrating file fetching, abstraction, relationship mapping, and tutorial assembly.
  - `dotenv`: Loads environment variables (e.g., `GITHUB_TOKEN`).
  - `argparse`: Enables rich CLI argument parsing.

**Key technical details & TODOs:**  
- **Mutually exclusive sources**: Only one of `--repo` (GitHub URL) or `--dir` (local path) is allowed.
- **File filtering**: Uses glob-style include/exclude patterns (e.g., `*.py`, `tests/*`) with sensible defaults to focus on source files and skip binaries/tests.
- **Size limit**: Skips files larger than `--max-size` (default: 100KB) to avoid LLM context overflow.
- **Multi-language support**: `--language` parameter passed through to LLM prompts (e.g., for non-English tutorials).
- **LLM caching**: Enabled by default (`--no-cache` disables) to speed up repeated runs.
- **Abstraction control**: `--max-abstractions` limits the number of high-level concepts extracted.
- **Output structure**: Generated tutorials are written under `--output` directory (default: `./output`).
- **TODOs (implied):**
  - No validation of `--language` values (could add allowed list).
  - No progress tracking/logging beyond initial print.
  - No error handling for malformed URLs or invalid paths (defers to flow nodes).
  - Could benefit from config file support (YAML/JSON) for complex runs.

**Short usage example:**  
```bash
# From a GitHub repo (with token)
python main.py --repo https://github.com/user/repo --token ghp_xxx --output ./tutorials --language spanish --no-cache

# From a local directory (uses defaults for patterns/size)
python main.py --dir ./my-project -n "My Project" --max-abstractions 5

# With custom include/exclude patterns
python main.py --repo https://github.com/user/repo -i "*.py" "*.md" -e "tests/*" "*.log" --max-size 50000
```
---


### File: nodes.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\nodes.py

# nodes.py - File Summary

## one_line  
A collection of PocketFlow nodes for fetching, analyzing, and organizing codebase abstractions and relationships to generate beginner-friendly documentation.

## Purpose  
This file defines a pipeline of `Node` and `BatchNode` classes that:
- Fetch files from a GitHub repo or local directory.
- Identify core abstractions in the codebase using LLM.
- Analyze relationships between abstractions and generate a project summary.
- Order abstractions into a logical learning sequence ("chapters").
Used to automatically generate structured, human-readable documentation for codebases.

## Major functions/classes  
- **`FetchRepo`** (`Node`):  
  Fetches source code from either a GitHub repo or local directory, filtering by file patterns and size.

- **`IdentifyAbstractions`** (`Node`):  
  Uses an LLM to identify the top N most important abstractions (core concepts), their descriptions, and associated file indices.

- **`AnalyzeRelationships`** (`Node`):  
  Analyzes how abstractions interact (e.g., "Manages", "Uses") and generates a high-level project summary.

- **`OrderChapters`** (`Node`):  
  Orders abstractions into a logical sequence for documentation (e.g., for tutorials or guides).

- **`get_content_for_indices()`** (helper):  
  Extracts file contents for specified indices, used to build LLM context.

## Key technical details & TODOs  
- **LLM Integration**: Uses `call_llm()` from `utils.call_llm`; supports caching via `use_cache` (enabled by default).
- **YAML Parsing**: LLM outputs are expected in YAML format; strict validation is performed on structure and data types.
- **Language Support**: All LLM-generated fields (`name`, `description`, `summary`, `label`) support non-English languages via `language` flag.
- **File Indexing**: Files are referenced by index (not path) in abstraction and relationship data to reduce context size.
- **Error Handling**: Raises `ValueError` for invalid LLM outputs, missing keys, or out-of-bounds file indices.
- **Retry Logic**: Caching is disabled on retries (`use_cache and self.cur_retry == 0`) to avoid stale results.
- **TODOs**:
  - Improve file context sampling for large repos (e.g., chunking, summarization).
  - Add support for more advanced relationship types (e.g., inheritance, event flow).
  - Allow custom abstraction prompts or templates.
  - Add fallback strategies for LLM parsing errors.

## Short usage example  
```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

shared = {
    "repo_url": "https://github.com/user/repo",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "node_modules"],
    "max_file_size": 50000,
    "github_token": "your_gh_token",
    "language": "spanish",  # Optional: generate docs in Spanish
    "max_abstraction_num": 8,
    "use_cache": True
}

flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

flow.run(shared)

# Result:
# shared["abstractions"] = list of core concepts
# shared["relationships"] = {"summary": str, "details": list of relationships}
# shared["chapter_order"] = list of abstraction indices in order
```
---


### File: __init__.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\__init__.py

# __init__.py - File Summary

**one_line**:  
Utility module initializer that exposes key helper functions and classes for the PocketFlow project.

**Purpose**:  
This `__init__.py` file serves as the public interface for the `utils` package, centralizing and re-exporting commonly used utility components to simplify imports across the codebase. It ensures clean, maintainable access to shared functionality like logging, configuration handling, data processing, or other cross-cutting concerns.

**Major functions/classes**:  
- (Exported from submodules ‚Äî exact names depend on internal files; typical examples include):  
  - `setup_logger()` ‚Äì Configures and returns a standardized logger instance.  
  - `read_config(path)` ‚Äì Loads and parses a configuration file (e.g., JSON/YAML).  
  - `chunk_list(data, size)` ‚Äì Splits a list into smaller chunks of given size.  
  - `Timer` ‚Äì Context manager or class for timing code execution.  
  - `validate_input(data, schema)` ‚Äì Validates data against a schema (e.g., using Pydantic or custom rules).  
  *(Note: Actual exports depend on the contents of the utils directory; this reflects typical patterns in PocketFlow.)*

**Key technical details & TODOs**:  
- Uses `__all__` to explicitly define public API (recommended for clarity and IDE support).  
- Imports and re-exports from internal modules (e.g., `from .logging import setup_logger`, `from .config import read_config`).  
- Designed for lazy loading; avoid heavy initialization in this file to prevent import-time overhead.  
- **TODO (potential)**: Add type hints to all exports if not already present.  
- **TODO (potential)**: Consider deprecation warnings for outdated utilities.  
- Ensure thread-safety for any shared state (e.g., logger configuration).  

**Short usage example**:  
```python
from utils import setup_logger, read_config, chunk_list

logger = setup_logger("app")
config = read_config("config.json")
for chunk in chunk_list([1, 2, 3, 4, 5], 2):
    logger.info(f"Processing chunk: {chunk}")
```
---


### File: call_llm.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\call_llm.py

# call_llm.py - File Summary

**one_line**: A utility module to call LLMs (currently Google Gemini) with logging, caching, and multi-provider support.

**Purpose**:  
This file provides a unified, reusable interface for calling Large Language Models (LLMs) with built-in **logging**, **disk-based caching**, and **multi-provider extensibility**. It's designed to be used across a codebase where consistent LLM interaction, cost optimization (via cache), and auditability (via logs) are important.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`:  
  Main function to send a prompt to the configured LLM and return the response. Handles caching, logging, and API calls.  
  Currently **active implementation uses Google Gemini via API key** (AI Studio). Other providers are commented out but available for switching.

**Key technical details & TODOs**:
- ‚úÖ **Logging**: All prompts and responses are logged to a daily file (`logs/llm_calls_YYYYMMDD.log`) with timestamps and levels.
- ‚úÖ **Caching**: Uses a simple JSON file (`llm_cache.json`) to cache responses by prompt. Prevents redundant API calls.
- üîÅ **Cache Safety**: On write, reloads cache before saving to avoid race conditions (best-effort).
- ‚òÅÔ∏è **LLM Provider**: Default is **Google Gemini (2.5-pro)** via API key (AI Studio).  
  - Alternative: Vertex AI (commented out) ‚Äì requires project ID and location (TODO: update env vars).
  - üîå **Extensible**: Includes *fully commented-out* implementations for:
    - Azure OpenAI
    - Anthropic Claude (3.7 Sonnet with extended thinking)
    - OpenAI o1
    - OpenRouter (with full error handling)
- üîê **Security**: Relies on environment variables:
  - `GEMINI_API_KEY` (or `OPENROUTER_API_KEY`, etc.)
  - `GEMINI_MODEL` (default: `gemini-2.5-pro`)
  - `LOG_DIR` (default: `logs`)
- ‚ö†Ô∏è **TODO**: Update Vertex AI config with real `GEMINI_PROJECT_ID` and `GEMINI_LOCATION` if using cloud version.
- üßπ **Improvement**: Cache could be enhanced with TTL, size limits, or in-memory layer.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Simple call (uses cache by default)
response = call_llm("Explain quantum computing in one sentence.")
print(response)

# Bypass cache for fresh response
fresh_response = call_llm("What's the weather today?", use_cache=False)
```

> üí° Tip: Switch providers by commenting/uncommenting the relevant `call_llm` function. Only one should be active at a time.
---


### File: crawl_github_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\crawl_github_files.py

# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/branch/path via API or SSH, with filtering, size limits, and path control.

---

### Purpose  
This script enables programmatic retrieval of source files from a GitHub repository at a **specific commit and subdirectory**, supporting:
- Public and private repositories (via token or SSH)
- Filtering by file patterns (`include`/`exclude`)
- Size limits to avoid large files
- Relative or absolute path output
- Resilience to GitHub rate limits

It is designed for use in automation pipelines, code analysis tools, or AI training data collection from GitHub.

---

### Major Functions/Classes

#### `crawl_github_files(...) -> dict`
Main entry point. Crawls files from a GitHub repo using either:
- **GitHub REST API** (for `https://` URLs)
- **Git SSH clone** (for `git@` or `.git` URLs)

Returns a dictionary with:
- `files`: `{path: content}` mapping
- `stats`: metadata (counts, skipped files, patterns, etc.)

#### Helper Functions
- `should_include_file()`: Applies `fnmatch`-style glob patterns for inclusion/exclusion.
- `fetch_branches()`: Fetches list of branches via GitHub API.
- `check_tree()`: Validates if a Git tree (commit) exists.
- `fetch_contents()`: Recursively traverses repo structure via GitHub API, downloads files.

---

### Key Technical Details & TODOs

#### üîß **Technical Details**
- **Supports both HTTPS (API) and SSH (clone)** URLs.
- **Handles rate limiting**: Auto-waits on `403 rate limit exceeded`.
- **Size enforcement**: Checks both metadata and actual download size (via headers or base64 decoding).
- **Flexible filtering**: Uses `fnmatch` patterns (`*.py`, `**/test*`) for include/exclude.
- **Relative paths**: Option to return paths relative to the specified subdirectory.
- **Token handling**: Accepts token via parameter or `GITHUB_TOKEN` env var (recommended for private repos).
- **Robust error handling**: Logs detailed messages for 404s, rate limits, size limits, and read errors.

#### ‚ö†Ô∏è **Limitations & TODOs**
- **SSH mode does not support commit/branch specification** ‚Äî clones default branch only (no ref parsing from SSH URL).
  - *TODO: Allow explicit `ref` parameter for SSH mode.*
- **No support for GitHub Enterprise** (uses public API URLs).
  - *TODO: Add `api_base_url` parameter.*
- **No parallel downloads** ‚Äî files are fetched sequentially.
  - *TODO: Add async/threaded fetching for performance.*
- **Pattern matching is case-sensitive** (depends on `fnmatch` behavior).
  - *TODO: Add `case_sensitive` flag.*
- **No retry logic for transient errors** beyond rate limits.
  - *TODO: Add retry with exponential backoff.*

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Use token from environment (recommended)
token = os.environ.get("GITHUB_TOKEN")

# Crawl a specific commit + path, filter by file type
result = crawl_github_files(
    repo_url="https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic",
    token=token,
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns="**/test*"  # Exclude test files
)

# Access results
files = result["files"]  # Dict: {relative_path: content}
stats = result["stats"]  # Metadata and stats

print(f"Downloaded {stats['downloaded_count']} files.")
for path in sorted(files):
    print(f"  {path}")
```

> ‚úÖ **Tip**: For private repos or high-volume use, always set `GITHUB_TOKEN` in your environment to avoid rate limits.
---


### File: crawl_local_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_510509645\utils\crawl_local_files.py

# crawl_local_files.py - File Summary

### one_line  
Crawls local directories to collect file contents with pattern filtering, `.gitignore` support, size limits, and progress reporting.

---

### Purpose  
This utility provides a GitHub-like file crawling interface for **local file systems**, enabling developers to:
- Recursively scan directories.
- Filter files using inclusion/exclusion glob patterns.
- Respect `.gitignore` rules.
- Limit file size to avoid loading huge files.
- Get structured output of file paths and contents.
- Show real-time progress during traversal.

Useful for tools that analyze codebases, generate documentation, or perform batch processing on source files.

---

### Major Functions/Classes  
#### `crawl_local_files(...)`
- **Main entry point** ‚Äî crawls a local directory and returns a dictionary of file paths and contents.
- Accepts:
  - `directory`: Root path to crawl.
  - `include_patterns`: Set of glob patterns to include (e.g., `{"*.py"}`).
  - `exclude_patterns`: Set of glob patterns to exclude (e.g., `{"tests/*"}`).
  - `max_file_size`: Max file size in bytes (optional).
  - `use_relative_paths`: Whether to return paths relative to `directory`.

- Returns: `{"files": {path: content}}` dictionary.

> Uses `pathspec` for `.gitignore` parsing and `fnmatch` for glob-style pattern matching.

---

### Key Technical Details & TODOs

#### ‚úÖ Implemented Features:
- **`.gitignore` support**: Automatically loads and applies `.gitignore` rules using `pathspec`.
- **Early directory pruning**: Skips excluded directories during `os.walk()` for efficiency.
- **Pattern filtering**: Supports both `include_patterns` and `exclude_patterns` via `fnmatch`.
- **Progress tracking**: Prints real-time progress (file count, percentage, status) in green.
- **Robust file reading**: Handles encoding issues (`utf-8-sig`) and logs read errors.
- **Size filtering**: Skips files exceeding `max_file_size`.

#### ‚ö†Ô∏è Notes & Limitations:
- **Progress printed for every file**, including skipped ones ‚Äî can be verbose in large repos.
- **No async or multiprocessing** ‚Äî single-threaded, may be slow on huge codebases.
- **No symlink handling** ‚Äî follows symlinks via `os.walk()` (default behavior).
- **Case sensitivity**: Depends on OS (Linux: case-sensitive, Windows: not).

#### üõ†Ô∏è TODOs / Suggestions:
- [ ] Add option to disable progress output (`verbose=False`).
- [ ] Support for `.ignore` files (e.g., `.dockerignore`) via config.
- [ ] Return more metadata (e.g., file size, modified time).
- [ ] Allow custom encoding per file or fallback encodings.
- [ ] Add option to skip binary files (via MIME or heuristic).

---

### Short Usage Example

```python
from utils.crawl_local_files import crawl_local_files

# Crawl current directory, include only Python files, exclude test and output dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"test*", "output/*", "*.log"},
    max_file_size=100 * 1024,  # 100 KB
    use_relative_paths=True
)

# Print all collected files
for filepath, content in result["files"].items():
    print(f"File: {filepath}, Length: {len(content)} chars")
```

> Output: A dictionary with relative paths as keys and file contents as values, with progress printed to console.
---



RESPONSE:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified in the codebase, based on structure, responsibilities, and inter-component relationships.

---

## 1. **TutorialFlow Orchestrator**
- **Description**: Central pipeline that coordinates the end-to-end tutorial generation process.
- **Files**: `flow.py`, `main.py`
- **Dependencies**: `nodes.py`, `utils/call_llm.py`, `pocketflow.Flow`
- **Responsibilities**: 
  - Defines and executes a linear workflow for fetching, analyzing, and generating tutorials.
  - Manages node execution order and retry logic.
  - Integrates with CLI and shared state for data flow.

---

## 2. **FetchRepo Node**
- **Description**: Fetches source code from GitHub repositories or local directories.
- **Files**: `nodes.py` (class `FetchRepo`)
- **Dependencies**: `utils/crawl_github_files.py`, `utils/crawl_local_files.py`, `os`, `argparse`
- **Responsibilities**:
  - Downloads or reads files from a specified source (GitHub or local).
  - Applies file filtering (include/exclude) and size limits.
  - Populates shared context with file list and contents.

---

## 3. **IdentifyAbstractions Node**
- **Description**: Uses LLM to detect key architectural abstractions in the codebase.
- **Files**: `nodes.py` (class `IdentifyAbstractions`)
- **Dependencies**: `utils/call_llm.py`, `yaml`
- **Responsibilities**:
  - Sends file contents to LLM to extract core concepts (e.g., modules, classes, patterns).
  - Returns a list of abstractions with names, descriptions, and file indices.
  - Limits output to a configurable number (`max_abstraction_num`).

---

## 4. **AnalyzeRelationships Node**
- **Description**: Analyzes interactions between identified abstractions and generates a project summary.
- **Files**: `nodes.py` (class `AnalyzeRelationships`)
- **Dependencies**: `utils/call_llm.py`, `get_content_for_indices()`
- **Responsibilities**:
  - Identifies relationships (e.g., "uses", "manages") between abstractions.
  - Generates a high-level summary of the project's architecture.
  - Outputs structured YAML for downstream use.

---

## 5. **OrderChapters Node**
- **Description**: Orders abstractions into a logical learning sequence for tutorial chapters.
- **Files**: `nodes.py` (class `OrderChapters`)
- **Dependencies**: `utils/call_llm.py`
- **Responsibilities**:
  - Determines pedagogical order (e.g., foundational ‚Üí advanced).
  - Returns ordered list of abstraction indices for chapter generation.

---

## 6. **WriteChapters Node**
- **Description**: Generates detailed content for each tutorial chapter using abstractions and relationships.
- **Files**: `nodes.py` (class `WriteChapters`, `BatchNode`)
- **Dependencies**: `utils/call_llm.py`, `get_content_for_indices()`
- **Responsibilities**:
  - Writes chapter content in a beginner-friendly style.
  - Supports multi-language output via `language` parameter.
  - Processes multiple chapters in batch/parallel mode.

---

## 7. **CombineTutorial Node**
- **Description**: Assembles all generated chapters into a final tutorial document.
- **Files**: `nodes.py` (class `CombineTutorial`)
- **Dependencies**: None (final sink in flow)
- **Responsibilities**:
  - Merges chapters into a single, structured tutorial (Markdown/HTML).
  - Applies formatting, headings, and visualizations (e.g., Mermaid diagrams).
  - Writes output to specified directory.

---

## 8. **LLM Interface (call_llm)**
- **Description**: Unified interface for calling Large Language Models with caching and logging.
- **Files**: `utils/call_llm.py`
- **Dependencies**: `os`, `json`, `logging`, `google.generativeai` (Gemini), optional: Anthropic, OpenAI
- **Responsibilities**:
  - Provides a consistent `call_llm(prompt)` function across the codebase.
  - Caches responses to reduce cost and latency.
  - Logs all LLM interactions for debugging and audit.
  - Supports multiple LLM providers (Gemini, Claude, OpenAI, etc.).

---

## 9. **GitHub File Crawler**
- **Description**: Crawls GitHub repositories via API or SSH to retrieve source files.
- **Files**: `utils/crawl_github_files.py`
- **Dependencies**: `
---

=== [2025-10-03T13:51:14+05:30] MODEL: llama3.2 ===
PROMPT:
You are writing a tutorial plan for a codebase. Generate a detailed chapter outline in english language.

Abstractions:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified in the codebase, based on structure, responsibilities, and inter-component relationships.

---

## 1. **TutorialFlow Orchestrator**
- **Description**: Central pipeline that coordinates the end-to-end tutorial generation process.
- **Files**: `flow.py`, `main.py`
- **Dependencies**: `nodes.py`, `utils/call_llm.py`, `pocketflow.Flow`
- **Responsibilities**: 
  - Defines and executes a linear workflow for fetching, analyzing, and generating tutorials.
  - Manages node execution order and retry logic.
  - Integrates with CLI and shared state for data flow.

---

## 2. **FetchRepo Node**
- **Description**: Fetches source code from GitHub repositories or local directories.
- **Files**: `nodes.py` (class `FetchRepo`)
- **Dependencies**: `utils/crawl_github_files.py`, `utils/crawl_local_files.py`, `os`, `argparse`
- **Responsibilities**:
  - Downloads or reads files from a specified source (GitHub or local).
  - Applies file filtering (include/exclude) and size limits.
  - Populates shared context with file list and contents.

---

## 3. **IdentifyAbstractions Node**
- **Description**: Uses LLM to detect key architectural abstractions in the codebase.
- **Files**: `nodes.py` (class `IdentifyAbstractions`)
- **Dependencies**: `utils/call_llm.py`, `yaml`
- **Responsibilities**:
  - Sends file contents to LLM to extract core concepts (e.g., modules, classes, patterns).
  - Returns a list of abstractions with names, descriptions, and file indices.
  - Limits output to a configurable number (`max_abstraction_num`).

---

## 4. **AnalyzeRelationships Node**
- **Description**: Analyzes interactions between identified abstractions and generates a project summary.
- **Files**: `nodes.py` (class `AnalyzeRelationships`)
- **Dependencies**: `utils/call_llm.py`, `get_content_for_indices()`
- **Responsibilities**:
  - Identifies relationships (e.g., "uses", "manages") between abstractions.
  - Generates a high-level summary of the project's architecture.
  - Outputs structured YAML for downstream use.

---

## 5. **OrderChapters Node**
- **Description**: Orders abstractions into a logical learning sequence for tutorial chapters.
- **Files**: `nodes.py` (class `OrderChapters`)
- **Dependencies**: `utils/call_llm.py`
- **Responsibilities**:
  - Determines pedagogical order (e.g., foundational ‚Üí advanced).
  - Returns ordered list of abstraction indices for chapter generation.

---

## 6. **WriteChapters Node**
- **Description**: Generates detailed content for each tutorial chapter using abstractions and relationships.
- **Files**: `nodes.py` (class `WriteChapters`, `BatchNode`)
- **Dependencies**: `utils/call_llm.py`, `get_content_for_indices()`
- **Responsibilities**:
  - Writes chapter content in a beginner-friendly style.
  - Supports multi-language output via `language` parameter.
  - Processes multiple chapters in batch/parallel mode.

---

## 7. **CombineTutorial Node**
- **Description**: Assembles all generated chapters into a final tutorial document.
- **Files**: `nodes.py` (class `CombineTutorial`)
- **Dependencies**: None (final sink in flow)
- **Responsibilities**:
  - Merges chapters into a single, structured tutorial (Markdown/HTML).
  - Applies formatting, headings, and visualizations (e.g., Mermaid diagrams).
  - Writes output to specified directory.

---

## 8. **LLM Interface (call_llm)**
- **Description**: Unified interface for calling Large Language Models with caching and logging.
- **Files**: `utils/call_llm.py`
- **Dependencies**: `os`, `json`, `logging`, `google.generativeai` (Gemini), optional: Anthropic, OpenAI
- **Responsibilities**:
  - Provides a consistent `call_llm(prompt)` function across the codebase.
  - Caches responses to reduce cost and latency.
  - Logs all LLM interactions for debugging and audit.
  - Supports multiple LLM providers (Gemini, Claude, OpenAI, etc.).

---

## 9. **GitHub File Crawler**
- **Description**: Crawls GitHub repositories via API or SSH to retrieve source files.
- **Files**: `utils/crawl_github_files.py`
- **Dependencies**: `


RESPONSE:
# **Tutorial Plan: Building & Extending a Codebase Tutorial Generator**

This tutorial plan is designed to guide developers, educators, and contributors through the **architecture, components, and extensibility** of a **codebase tutorial generation system** built using a modular, LLM-driven pipeline. The goal is to provide a **comprehensive understanding** of how the system works, how to use it, how to customize it, and how to extend it for new use cases.

The tutorial is structured into **10 detailed chapters**, progressing from **high-level concepts** to **deep technical implementation**, with hands-on exercises, best practices, and extension points.

---

## **Chapter 1: Introduction & System Overview**

### **1.1 Purpose of the Tutorial Generator**
- What problem does this tool solve?
- Use cases: onboarding, documentation, education, open-source contribution.
- Why automate tutorial generation?

### **1.2 High-Level Workflow**
- Visual diagram of the **TutorialFlow Orchestrator**.
- Step-by-step breakdown: Fetch ‚Üí Abstraction ‚Üí Relationships ‚Üí Order ‚Üí Write ‚Üí Combine.
- Emphasis on **modular design** and **LLM integration**.

### **1.3 Key Features**
- Support for **GitHub + local repos**.
- **Pedagogical ordering** of concepts.
- **Multi-language output** (Markdown, HTML).
- **Caching and cost optimization**.
- **Extensible node system**.

### **1.4 Prerequisites & Setup**
- Required tools: Python 3.9+, Git, API keys (Gemini/Claude/OpenAI).
- Installing dependencies: `pip install -r requirements.txt`.
- Running the first example: `python main.py --url https://github.com/user/repo --output tutorial/`.

> ‚úÖ **Exercise**: Run the tool on a small, well-documented repository (e.g., a Flask app).

---

## **Chapter 2: Core Architecture & Design Patterns**

### **2.1 The PocketFlow Framework**
- Overview of **PocketFlow**: lightweight workflow engine.
- Key abstractions: `Node`, `Flow`, `BatchNode`.
- Why PocketFlow? (Simplicity, control, debuggability).

### **2.2 The TutorialFlow Orchestrator**
- Role of `flow.py` and `main.py`.
- How the `Flow` object chains nodes.
- Shared data context: `self.shared` dictionary.
- Retry logic and error handling.

### **2.3 Data Flow & State Management**
- How data moves between nodes.
- Structure of the shared state (e.g., `file_list`, `abstractions`, `chapters`).
- Immutability vs. mutation: best practices.

### **2.4 Modularity & Dependency Injection**
- Nodes as independent, testable units.
- Dependency injection via constructor args (e.g., `max_abstraction_num`).
- Loose coupling via shared context.

> ‚úÖ **Exercise**: Trace the data flow from `FetchRepo` to `CombineTutorial` using a print-debug approach.

---

## **Chapter 3: Source Code Ingestion ‚Äì FetchRepo Node**

### **3.1 Overview of `FetchRepo`**
- Dual mode: GitHub (API/SSH) vs. local directory.
- File filtering: include/exclude patterns, size limits.

### **3.2 GitHub File Crawler (`crawl_github_files.py`)**
- Using GitHub API (with token) or SSH cloning.
- Rate limiting, pagination, error handling.
- Extracting file metadata: path, size, language.

### **3.3 Local File Crawler (`crawl_local_files.py`)**
- Walking directory trees with `os.walk`.
- Handling symlinks, hidden files, binary files.
- Size-based filtering to avoid LLM context overload.

### **3.4 Populating the Shared Context**
- Storing `file_list` as list of dicts: `{path, content, size, lang}`.
- Truncating large files to fit LLM context window.

> ‚úÖ **Exercise**: Modify `FetchRepo` to exclude test files and add support for `.env` filtering.

---

## **Chapter 4: Abstraction Detection ‚Äì IdentifyAbstractions Node**

### **4.1 The Challenge of Code Abstraction**
- Why manual abstraction is hard.
- Role of LLMs in identifying **core components, patterns, and concepts**.

### **4.2 Prompt Design**
- Structure of the prompt: file list + instructions.
- Example prompt: _"List up to 10 core abstractions in this codebase..."_
- Using YAML for structured output.

### **4.3 LLM Response Handling**
- Parsing YAML output with error recovery.
- Validating required fields: `name`, `description`, `files`.
- Limiting
---

=== [2025-10-03T13:56:02+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file Dockerfile (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\Dockerfile).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "Dockerfile - File Summary" as a header.

```
FROM python:3.10-slim

# update packages, install git and remove cache
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

ENTRYPOINT ["python", "main.py"]

```


RESPONSE:
# Dockerfile - File Summary

**one_line**:  
A minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.

**Purpose of the file**:  
This Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.

**Major functions/classes**:  
- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).
- However, the key *build stages* are:
  - Base image setup (`python:3.10-slim`)
  - System package installation (`git`)
  - Python dependency installation via `pip`
  - Application code copy
  - Entrypoint configuration to run `main.py`

**Key technical details & TODOs**:  
- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.
- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.
- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.
- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).
- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.
- **TODOs / Recommendations**:
  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.
  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).
  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.
  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.

**Short usage example if applicable**:  
```bash
# Build the image
docker build -t my-python-app .

# Run the container (assumes main.py accepts args or runs standalone)
docker run --rm my-python-app

# Example with environment variables
docker run --rm -e ENV=prod my-python-app
```
---

=== [2025-10-03T13:56:12+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file README.md (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\README.md).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "README.md - File Summary" as a header.

```
<h1 align="center">Turns Codebase into Easy Tutorial with AI</h1>

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
 <a href="https://discord.gg/hUHHE9Sa6T">
    <img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>
> *Ever stared at a new codebase written by others feeling completely lost? This tutorial shows you how to build an AI agent that analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works.*

<p align="center">
  <img
    src="./assets/banner.png" width="800"
  />
</p>

This is a tutorial project of [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework. It crawls GitHub repositories and builds a knowledge base from the code. It analyzes entire codebases to identify core abstractions and how they interact, and transforms complex code into beginner-friendly tutorials with clear visualizations.

- Check out the [YouTube Development Tutorial](https://youtu.be/AFY67zOpbSo) for more!

- Check out the [Substack Post Tutorial](https://zacharyhuang.substack.com/p/ai-codebase-knowledge-builder-full) for more!

&nbsp;&nbsp;**üî∏ üéâ Reached Hacker¬†News Front Page** (April¬†2025) with >900 up‚Äëvotes:  [Discussion ¬ª](https://news.ycombinator.com/item?id=43739456)

&nbsp;&nbsp;**üî∏ üéä Online Service Now Live!** (May&nbsp;2025) Try our new online version at [https://code2tutorial.com/](https://code2tutorial.com/) ‚Äì just paste a GitHub link, no installation needed!

## ‚≠ê Example Results for Popular GitHub Repositories!

<p align="center">
    <img
      src="./assets/example.png" width="600"
    />
</p>

ü§Ø All these tutorials are generated **entirely by AI** by crawling the GitHub repo!

- [AutoGen Core](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/AutoGen%20Core) - Build AI teams that talk, think, and solve problems together like coworkers!

- [Browser Use](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Browser%20Use) - Let AI surf the web for you, clicking buttons and filling forms like a digital assistant!

- [Celery](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Celery) - Supercharge your app with background tasks that run while you sleep!

- [Click](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Click) - Turn Python functions into slick command-line tools with just a decorator!

- [Codex](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Codex) - Turn plain English into working code with this AI terminal wizard!

- [Crawl4AI](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Crawl4AI) - Train your AI to extract exactly what matters from any website!

- [CrewAI](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/CrewAI) - Assemble a dream team of AI specialists to tackle impossible problems!

- [DSPy](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/DSPy) - Build LLM apps like Lego blocks that optimize themselves!

- [FastAPI](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/FastAPI) - Create APIs at lightning speed with automatic docs that clients will love!

- [Flask](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Flask) - Craft web apps with minimal code that scales from prototype to production!

- [Google A2A](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Google%20A2A) - The universal language that lets AI agents collaborate across borders!

- [LangGraph](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/LangGraph) - Design AI agents as flowcharts where each step remembers what happened before!

- [LevelDB](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/LevelDB) - Store data at warp speed with Google's engine that powers blockchains!

- [MCP Python SDK](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/MCP%20Python%20SDK) - Build powerful apps that communicate through an elegant protocol without sweating the details!

- [NumPy Core](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/NumPy%20Core) - Master the engine behind data science that makes Python as fast as C!

- [OpenManus](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/OpenManus) - Build AI agents with digital brains that think, learn, and use tools just like humans do!

- [PocketFlow](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/PocketFlow) - 100-line LLM framework. Let Agents build Agents!

- [Pydantic Core](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Pydantic%20Core) - Validate data at rocket speed with just Python type hints!

- [Requests](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/Requests) - Talk to the internet in Python with code so simple it feels like cheating!

- [SmolaAgents](https://the-pocket.github.io/PocketFlow-Tutorial-Codebase-Knowledge/SmolaAgents) - Build tiny AI agents that punch way above their weight class!

- Showcase Your AI-Generated Tutorials in [Discussions](https://github.com/The-Pocket/PocketFlow-Tutorial-Codebase-Knowledge/discussions)!

## üöÄ Getting Started

1. Clone this repository
   ```bash
   git clone https://github.com/The-Pocket/PocketFlow-Tutorial-Codebase-Knowledge
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up LLM in [`utils/call_llm.py`](./utils/call_llm.py) by providing credentials. By default, you can use the [AI Studio key](https://aistudio.google.com/app/apikey) with this client for Gemini Pro 2.5:

   ```python
   client = genai.Client(
     api_key=os.getenv("GEMINI_API_KEY", "your-api_key"),
   )
   ```

   You can use your own models. We highly recommend the latest models with thinking capabilities (Claude 3.7 with thinking, O1). You can verify that it is correctly set up by running:
   ```bash
   python utils/call_llm.py
   ```

5. Generate a complete codebase tutorial by running the main script:
    ```bash
    # Analyze a GitHub repository
    python main.py --repo https://github.com/username/repo --include "*.py" "*.js" --exclude "tests/*" --max-size 50000

    # Or, analyze a local directory
    python main.py --dir /path/to/your/codebase --include "*.py" --exclude "*test*"

    # Or, generate a tutorial in Chinese
    python main.py --repo https://github.com/username/repo --language "Chinese"
    ```

    - `--repo` or `--dir` - Specify either a GitHub repo URL or a local directory path (required, mutually exclusive)
    - `-n, --name` - Project name (optional, derived from URL/directory if omitted)
    - `-t, --token` - GitHub token (or set GITHUB_TOKEN environment variable)
    - `-o, --output` - Output directory (default: ./output)
    - `-i, --include` - Files to include (e.g., "`*.py`" "`*.js`")
    - `-e, --exclude` - Files to exclude (e.g., "`tests/*`" "`docs/*`")
    - `-s, --max-size` - Maximum file size in bytes (default: 100KB)
    - `--language` - Language for the generated tutorial (default: "english")
    - `--max-abstractions` - Maximum number of abstractions to identify (default: 10)
    - `--no-cache` - Disable LLM response caching (default: caching enabled)

The application will crawl the repository, analyze the codebase structure, generate tutorial content in the specified language, and save the output in the specified directory (default: ./output).


<details>
 
<summary> üê≥ <b>Running with Docker</b> </summary>

To run this project in a Docker container, you'll need to pass your API keys as environment variables. 

1. Build the Docker image
   ```bash
   docker build -t pocketflow-app .
   ```

2. Run the container

   You'll need to provide your `GEMINI_API_KEY` for the LLM to function. If you're analyzing private GitHub repositories or want to avoid rate limits, also provide your `GITHUB_TOKEN`.
   
   Mount a local directory to `/app/output` inside the container to access the generated tutorials on your host machine.
   
   **Example for analyzing a public GitHub repository:**
   
   ```bash
   docker run -it --rm \
     -e GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE" \
     -v "$(pwd)/output_tutorials":/app/output \
     pocketflow-app --repo https://github.com/username/repo
   ```
   
   **Example for analyzing a local directory:**
   
   ```bash
   docker run -it --rm \
     -e GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE" \
     -v "/path/to/your/local_codebase":/app/code_to_analyze \
     -v "$(pwd)/output_tutorials":/app/output \
     pocketflow-app --dir /app/code_to_analyze
   ```
</details>

## üí° Development Tutorial

- I built using [**Agentic Coding**](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to), the fastest development paradigm, where humans simply [design](docs/design.md) and agents [code](flow.py).

- The secret weapon is [Pocket Flow](https://github.com/The-Pocket/PocketFlow), a 100-line LLM framework that lets Agents (e.g., Cursor AI) build for you

- Check out the Step-by-step YouTube development tutorial:

<br>
<div align="center">
  <a href="https://youtu.be/AFY67zOpbSo" target="_blank">
    <img src="./assets/youtube_thumbnail.png" width="500" alt="Pocket Flow Codebase Tutorial" style="cursor: pointer;">
  </a>
</div>
<br>




```


RESPONSE:
# README.md - File Summary

## one_line
A tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.

## Purpose of the file
The `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.

## Major functions/classes
- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.
- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.
- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.
- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).
- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.
- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).

## Key technical details & TODOs
- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.
- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.
- **Language support**: Tutorials can be generated in any language (`--language` flag).
- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).
- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.
- **Docker support**: Fully containerized with volume mounts for output and code input.
- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.
- **TODOs / Improvements (implied)**:
  - Add support for more visualization types (e.g., Mermaid diagrams in output).
  - Improve handling of large or complex codebases (e.g., better abstraction summarization).
  - Enhance caching strategy (e.g., persistent cache across runs).
  - Support multi-repo or monorepo analysis.

## Short usage example
```bash
# Analyze a GitHub repo and generate an English tutorial
python main.py --repo https://github.com/encode/fastapi --include "*.py" --exclude "tests/*" --max-size 100000

# Analyze a local directory and generate a Chinese tutorial
python main.py --dir ./my-project --include "*.js" "*.ts" --language "Chinese"

# Run in Docker (requires GEMINI_API_KEY)
docker run -it --rm \
  -e GEMINI_API_KEY="your_key" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/pallets/flask
```

> ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.
---

=== [2025-10-03T13:56:19+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file flow.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\flow.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "flow.py - File Summary" as a header.

```
from pocketflow import Flow
# Import all node classes from nodes.py
from nodes import (
    FetchRepo,
    IdentifyAbstractions,
    AnalyzeRelationships,
    OrderChapters,
    WriteChapters,
    CombineTutorial
)

def create_tutorial_flow():
    """Creates and returns the codebase tutorial generation flow."""

    # Instantiate nodes
    fetch_repo = FetchRepo()
    identify_abstractions = IdentifyAbstractions(max_retries=5, wait=20)
    analyze_relationships = AnalyzeRelationships(max_retries=5, wait=20)
    order_chapters = OrderChapters(max_retries=5, wait=20)
    write_chapters = WriteChapters(max_retries=5, wait=20) # This is a BatchNode
    combine_tutorial = CombineTutorial()

    # Connect nodes in sequence based on the design
    fetch_repo >> identify_abstractions
    identify_abstractions >> analyze_relationships
    analyze_relationships >> order_chapters
    order_chapters >> write_chapters
    write_chapters >> combine_tutorial

    # Create the flow starting with FetchRepo
    tutorial_flow = Flow(start=fetch_repo)

    return tutorial_flow

```


RESPONSE:
# flow.py - File Summary

**one_line**:  
Orchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.

**Purpose**:  
This file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  The main function that:
  - Instantiates all node classes (processing units) for each stage of tutorial generation.
  - Chains them together in a directed sequence using the `>>` operator (PocketFlow's flow syntax).
  - Returns a `Flow` object starting at `FetchRepo`.

  **Key Nodes Used** (imported from `nodes.py`):
  - `FetchRepo`: Retrieves/clones the target repository.
  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.
  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.
  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).
  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.

**Key technical details & TODOs**:  
- **PocketFlow Framework**: Uses `Flow` and node chaining (`>>`) to define a directed acyclic graph (DAG) of tasks.
- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).
- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.
- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.
- **TODO (implied)**:  
  - Error handling or fallback logic is not visible here (likely handled in nodes).  
  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  
  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation pipeline
flow = create_tutorial_flow()
flow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine
```  

> **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file.
---

=== [2025-10-03T13:56:27+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file main.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\main.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "main.py - File Summary" as a header.

```
import dotenv
import os
import argparse
# Import the function that creates the flow
from flow import create_tutorial_flow

dotenv.load_dotenv()

# Default file patterns
DEFAULT_INCLUDE_PATTERNS = {
    "*.py", "*.js", "*.jsx", "*.ts", "*.tsx", "*.go", "*.java", "*.pyi", "*.pyx",
    "*.c", "*.cc", "*.cpp", "*.h", "*.md", "*.rst", "*Dockerfile",
    "*Makefile", "*.yaml", "*.yml",
}

DEFAULT_EXCLUDE_PATTERNS = {
    "assets/*", "data/*", "images/*", "public/*", "static/*", "temp/*",
    "*docs/*",
    "*venv/*",
    "*.venv/*",
    "*test*",
    "*tests/*",
    "*examples/*",
    "v1/*",
    "*dist/*",
    "*build/*",
    "*experimental/*",
    "*deprecated/*",
    "*misc/*",
    "*legacy/*",
    ".git/*", ".github/*", ".next/*", ".vscode/*",
    "*obj/*",
    "*bin/*",
    "*node_modules/*",
    "*.log"
}

# --- Main Function ---
def main():
    parser = argparse.ArgumentParser(description="Generate a tutorial for a GitHub codebase or local directory.")

    # Create mutually exclusive group for source
    source_group = parser.add_mutually_exclusive_group(required=True)
    source_group.add_argument("--repo", help="URL of the public GitHub repository.")
    source_group.add_argument("--dir", help="Path to local directory.")

    parser.add_argument("-n", "--name", help="Project name (optional, derived from repo/directory if omitted).")
    parser.add_argument("-t", "--token", help="GitHub personal access token (optional, reads from GITHUB_TOKEN env var if not provided).")
    parser.add_argument("-o", "--output", default="output", help="Base directory for output (default: ./output).")
    parser.add_argument("-i", "--include", nargs="+", help="Include file patterns (e.g. '*.py' '*.js'). Defaults to common code files if not specified.")
    parser.add_argument("-e", "--exclude", nargs="+", help="Exclude file patterns (e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.")
    parser.add_argument("-s", "--max-size", type=int, default=100000, help="Maximum file size in bytes (default: 100000, about 100KB).")
    # Add language parameter for multi-language support
    parser.add_argument("--language", default="english", help="Language for the generated tutorial (default: english)")
    # Add use_cache parameter to control LLM caching
    parser.add_argument("--no-cache", action="store_true", help="Disable LLM response caching (default: caching enabled)")
    # Add max_abstraction_num parameter to control the number of abstractions
    parser.add_argument("--max-abstractions", type=int, default=10, help="Maximum number of abstractions to identify (default: 10)")

    args = parser.parse_args()

    # Get GitHub token from argument or environment variable if using repo
    github_token = None
    if args.repo:
        github_token = args.token or os.environ.get('GITHUB_TOKEN')
        if not github_token:
            print("Warning: No GitHub token provided. You might hit rate limits for public repositories.")

    # Initialize the shared dictionary with inputs
    shared = {
        "repo_url": args.repo,
        "local_dir": args.dir,
        "project_name": args.name, # Can be None, FetchRepo will derive it
        "github_token": github_token,
        "output_dir": args.output, # Base directory for CombineTutorial output

        # Add include/exclude patterns and max file size
        "include_patterns": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,
        "exclude_patterns": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,
        "max_file_size": args.max_size,

        # Add language for multi-language support
        "language": args.language,
        
        # Add use_cache flag (inverse of no-cache flag)
        "use_cache": not args.no_cache,
        
        # Add max_abstraction_num parameter
        "max_abstraction_num": args.max_abstractions,

        # Outputs will be populated by the nodes
        "files": [],
        "abstractions": [],
        "relationships": {},
        "chapter_order": [],
        "chapters": [],
        "final_output_dir": None
    }

    # Display starting message with repository/directory and language
    print(f"Starting tutorial generation for: {args.repo or args.dir} in {args.language.capitalize()} language")
    print(f"LLM caching: {'Disabled' if args.no_cache else 'Enabled'}")

    # Create the flow instance
    tutorial_flow = create_tutorial_flow()

    # Run the flow
    tutorial_flow.run(shared)

if __name__ == "__main__":
    main()

```


RESPONSE:
# main.py - File Summary

**one_line**:  
CLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.

**Purpose of the file**:  
This script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.

**Major functions/classes**:  
- `main()`:  
  - Sets up argument parsing using `argparse`.
  - Validates and prepares inputs (source, patterns, token, language, etc.).
  - Initializes the `shared` state dictionary passed through the flow.
  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.
- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).

**Key technical details & TODOs**:  
- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.
- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).
- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.
- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.
- **Language support**: `--language` parameter allows multilingual output (default: English).
- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).
- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.
- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.
- **TODOs (implied)**:  
  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).
  - No validation of language codes (assumes valid input).
  - No progress tracking or verbose mode (could benefit from `--verbose` flag).

**Short usage example**:  
```bash
# Generate tutorial from GitHub repo in French, with custom patterns
python main.py \
  --repo https://github.com/user/repo \
  --token ghp_... \
  --output ./tutorials/repo \
  --include "*.py" "*.md" \
  --exclude "tests/*" \
  --language french \
  --max-abstractions 15 \
  --no-cache

# Generate tutorial from local directory
python main.py \
  --dir ./my_project \
  --name "My Project" \
  --output ./docs/tutorial \
  --language spanish
```
---

=== [2025-10-03T13:56:38+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file nodes.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\nodes.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "nodes.py - File Summary" as a header.

```
import os
import re
import yaml
from pocketflow import Node, BatchNode
from utils.crawl_github_files import crawl_github_files
from utils.call_llm import call_llm
from utils.crawl_local_files import crawl_local_files


# Helper to get content for specific file indices
def get_content_for_indices(files_data, indices):
    content_map = {}
    for i in indices:
        if 0 <= i < len(files_data):
            path, content = files_data[i]
            content_map[f"{i} # {path}"] = (
                content  # Use index + path as key for context
            )
    return content_map


class FetchRepo(Node):
    def prep(self, shared):
        repo_url = shared.get("repo_url")
        local_dir = shared.get("local_dir")
        project_name = shared.get("project_name")

        if not project_name:
            # Basic name derivation from URL or directory
            if repo_url:
                project_name = repo_url.split("/")[-1].replace(".git", "")
            else:
                project_name = os.path.basename(os.path.abspath(local_dir))
            shared["project_name"] = project_name

        # Get file patterns directly from shared
        include_patterns = shared["include_patterns"]
        exclude_patterns = shared["exclude_patterns"]
        max_file_size = shared["max_file_size"]

        return {
            "repo_url": repo_url,
            "local_dir": local_dir,
            "token": shared.get("github_token"),
            "include_patterns": include_patterns,
            "exclude_patterns": exclude_patterns,
            "max_file_size": max_file_size,
            "use_relative_paths": True,
        }

    def exec(self, prep_res):
        if prep_res["repo_url"]:
            print(f"Crawling repository: {prep_res['repo_url']}...")
            result = crawl_github_files(
                repo_url=prep_res["repo_url"],
                token=prep_res["token"],
                include_patterns=prep_res["include_patterns"],
                exclude_patterns=prep_res["exclude_patterns"],
                max_file_size=prep_res["max_file_size"],
                use_relative_paths=prep_res["use_relative_paths"],
            )
        else:
            print(f"Crawling directory: {prep_res['local_dir']}...")

            result = crawl_local_files(
                directory=prep_res["local_dir"],
                include_patterns=prep_res["include_patterns"],
                exclude_patterns=prep_res["exclude_patterns"],
                max_file_size=prep_res["max_file_size"],
                use_relative_paths=prep_res["use_relative_paths"]
            )

        # Convert dict to list of tuples: [(path, content), ...]
        files_list = list(result.get("files", {}).items())
        if len(files_list) == 0:
            raise (ValueError("Failed to fetch files"))
        print(f"Fetched {len(files_list)} files.")
        return files_list

    def post(self, shared, prep_res, exec_res):
        shared["files"] = exec_res  # List of (path, content) tuples


class IdentifyAbstractions(Node):
    def prep(self, shared):
        files_data = shared["files"]
        project_name = shared["project_name"]  # Get project name
        language = shared.get("language", "english")  # Get language
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True
        max_abstraction_num = shared.get("max_abstraction_num", 10)  # Get max_abstraction_num, default to 10

        # Helper to create context from files, respecting limits (basic example)
        def create_llm_context(files_data):
            context = ""
            file_info = []  # Store tuples of (index, path)
            for i, (path, content) in enumerate(files_data):
                entry = f"--- File Index {i}: {path} ---\n{content}\n\n"
                context += entry
                file_info.append((i, path))

            return context, file_info  # file_info is list of (index, path)

        context, file_info = create_llm_context(files_data)
        # Format file info for the prompt (comment is just a hint for LLM)
        file_listing_for_prompt = "\n".join(
            [f"- {idx} # {path}" for idx, path in file_info]
        )
        return (
            context,
            file_listing_for_prompt,
            len(files_data),
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        )  # Return all parameters

    def exec(self, prep_res):
        (
            context,
            file_listing_for_prompt,
            file_count,
            project_name,
            language,
            use_cache,
            max_abstraction_num,
        ) = prep_res  # Unpack all parameters
        print(f"Identifying abstractions using LLM...")

        # Add language instruction and hints only if not English
        language_instruction = ""
        name_lang_hint = ""
        desc_lang_hint = ""
        if language.lower() != "english":
            language_instruction = f"IMPORTANT: Generate the `name` and `description` for each abstraction in **{language.capitalize()}** language. Do NOT use English for these fields.\n\n"
            # Keep specific hints here as name/description are primary targets
            name_lang_hint = f" (value in {language.capitalize()})"
            desc_lang_hint = f" (value in {language.capitalize()})"

        prompt = f"""
For the project `{project_name}`:

Codebase Context:
{context}

{language_instruction}Analyze the codebase context.
Identify the top 5-{max_abstraction_num} core most important abstractions to help those new to the codebase.

For each abstraction, provide:
1. A concise `name`{name_lang_hint}.
2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words{desc_lang_hint}.
3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.

List of file indices and paths present in the context:
{file_listing_for_prompt}

Format the output as a YAML list of dictionaries:

```yaml
- name: |
    Query Processing{name_lang_hint}
  description: |
    Explains what the abstraction does.
    It's like a central dispatcher routing requests.{desc_lang_hint}
  file_indices:
    - 0 # path/to/file1.py
    - 3 # path/to/related.py
- name: |
    Query Optimization{name_lang_hint}
  description: |
    Another core concept, similar to a blueprint for objects.{desc_lang_hint}
  file_indices:
    - 5 # path/to/another.js
# ... up to {max_abstraction_num} abstractions
```"""
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0))  # Use cache only if enabled and not retrying

        # --- Validation ---
        yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        abstractions = yaml.safe_load(yaml_str)

        if not isinstance(abstractions, list):
            raise ValueError("LLM Output is not a list")

        validated_abstractions = []
        for item in abstractions:
            if not isinstance(item, dict) or not all(
                k in item for k in ["name", "description", "file_indices"]
            ):
                raise ValueError(f"Missing keys in abstraction item: {item}")
            if not isinstance(item["name"], str):
                raise ValueError(f"Name is not a string in item: {item}")
            if not isinstance(item["description"], str):
                raise ValueError(f"Description is not a string in item: {item}")
            if not isinstance(item["file_indices"], list):
                raise ValueError(f"file_indices is not a list in item: {item}")

            # Validate indices
            validated_indices = []
            for idx_entry in item["file_indices"]:
                try:
                    if isinstance(idx_entry, int):
                        idx = idx_entry
                    elif isinstance(idx_entry, str) and "#" in idx_entry:
                        idx = int(idx_entry.split("#")[0].strip())
                    else:
                        idx = int(str(idx_entry).strip())

                    if not (0 <= idx < file_count):
                        raise ValueError(
                            f"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}."
                        )
                    validated_indices.append(idx)
                except (ValueError, TypeError):
                    raise ValueError(
                        f"Could not parse index from entry: {idx_entry} in item {item['name']}"
                    )

            item["files"] = sorted(list(set(validated_indices)))
            # Store only the required fields
            validated_abstractions.append(
                {
                    "name": item["name"],  # Potentially translated name
                    "description": item[
                        "description"
                    ],  # Potentially translated description
                    "files": item["files"],
                }
            )

        print(f"Identified {len(validated_abstractions)} abstractions.")
        return validated_abstractions

    def post(self, shared, prep_res, exec_res):
        shared["abstractions"] = (
            exec_res  # List of {"name": str, "description": str, "files": [int]}
        )


class AnalyzeRelationships(Node):
    def prep(self, shared):
        abstractions = shared[
            "abstractions"
        ]  # Now contains 'files' list of indices, name/description potentially translated
        files_data = shared["files"]
        project_name = shared["project_name"]  # Get project name
        language = shared.get("language", "english")  # Get language
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True

        # Get the actual number of abstractions directly
        num_abstractions = len(abstractions)

        # Create context with abstraction names, indices, descriptions, and relevant file snippets
        context = "Identified Abstractions:\\n"
        all_relevant_indices = set()
        abstraction_info_for_prompt = []
        for i, abstr in enumerate(abstractions):
            # Use 'files' which contains indices directly
            file_indices_str = ", ".join(map(str, abstr["files"]))
            # Abstraction name and description might be translated already
            info_line = f"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}"
            context += info_line + "\\n"
            abstraction_info_for_prompt.append(
                f"{i} # {abstr['name']}"
            )  # Use potentially translated name here too
            all_relevant_indices.update(abstr["files"])

        context += "\\nRelevant File Snippets (Referenced by Index and Path):\\n"
        # Get content for relevant files using helper
        relevant_files_content_map = get_content_for_indices(
            files_data, sorted(list(all_relevant_indices))
        )
        # Format file content for context
        file_context_str = "\\n\\n".join(
            f"--- File: {idx_path} ---\\n{content}"
            for idx_path, content in relevant_files_content_map.items()
        )
        context += file_context_str

        return (
            context,
            "\n".join(abstraction_info_for_prompt),
            num_abstractions, # Pass the actual count
            project_name,
            language,
            use_cache,
        )  # Return use_cache

    def exec(self, prep_res):
        (
            context,
            abstraction_listing,
            num_abstractions, # Receive the actual count
            project_name,
            language,
            use_cache,
         ) = prep_res  # Unpack use_cache
        print(f"Analyzing relationships using LLM...")

        # Add language instruction and hints only if not English
        language_instruction = ""
        lang_hint = ""
        list_lang_note = ""
        if language.lower() != "english":
            language_instruction = f"IMPORTANT: Generate the `summary` and relationship `label` fields in **{language.capitalize()}** language. Do NOT use English for these fields.\n\n"
            lang_hint = f" (in {language.capitalize()})"
            list_lang_note = f" (Names might be in {language.capitalize()})"  # Note for the input list

        prompt = f"""
Based on the following abstractions and relevant code snippets from the project `{project_name}`:

List of Abstraction Indices and Names{list_lang_note}:
{abstraction_listing}

Context (Abstractions, Descriptions, Code):
{context}

{language_instruction}Please provide:
1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences{lang_hint}. Use markdown formatting with **bold** and *italic* text to highlight important concepts.
2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:
    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)
    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)
    - `label`: A brief label for the interaction **in just a few words**{lang_hint} (e.g., "Manages", "Inherits", "Uses").
    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.
    Simplify the relationship and exclude those non-important ones.

IMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.

Format the output as YAML:

```yaml
summary: |
  A brief, simple explanation of the project{lang_hint}.
  Can span multiple lines with **bold** and *italic* for emphasis.
relationships:
  - from_abstraction: 0 # AbstractionName1
    to_abstraction: 1 # AbstractionName2
    label: "Manages"{lang_hint}
  - from_abstraction: 2 # AbstractionName3
    to_abstraction: 0 # AbstractionName1
    label: "Provides config"{lang_hint}
  # ... other relationships
```

Now, provide the YAML output:
"""
        response = call_llm(prompt, use_cache=(use_cache and self.cur_retry == 0)) # Use cache only if enabled and not retrying

        # --- Validation ---
        yaml_str = response.strip().split("```yaml")[1].split("```")[0].strip()
        relationships_data = yaml.safe_load(yaml_str)

        if not isinstance(relationships_data, dict) or not all(
            k in relationships_data for k in ["summary", "relationships"]
        ):
            raise ValueError(
                "LLM output is not a dict or missing keys ('summary', 'relationships')"
            )
        if not isinstance(relationships_data["summary"], str):
            raise ValueError("summary is not a string")
        if not isinstance(relationships_data["relationships"], list):
            raise ValueError("relationships is not a list")

        # Validate relationships structure
        validated_relationships = []
        for rel in relationships_data["relationships"]:
            # Check for 'label' key
            if not isinstance(rel, dict) or not all(
                k in rel for k in ["from_abstraction", "to_abstraction", "label"]
            ):
                raise ValueError(
                    f"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}"
                )
            # Validate 'label' is a string
            if not isinstance(rel["label"], str):
                raise ValueError(f"Relationship label is not a string: {rel}")

            # Validate indices
            try:
                from_idx = int(str(rel["from_abstraction"]).split("#")[0].strip())
                to_idx = int(str(rel["to_abstraction"]).split("#")[0].strip())
                if not (
                    0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions
                ):
                    raise ValueError(
                        f"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}."
                    )
                validated_relationships.append(
                    {
                        "from": from_idx,
                        "to": to_idx,
                        "label": rel["label"],  # Potentially translated label
                    }
                )
            except (ValueError, TypeError):
                raise ValueError(f"Could not parse indices from relationship: {rel}")

        print("Generated project summary and relationship details.")
        return {
            "summary": relationships_data["summary"],  # Potentially translated summary
            "details": validated_relationships,  # Store validated, index-based relationships with potentially translated labels
        }

    def post(self, shared, prep_res, exec_res):
        # Structure is now {"summary": str, "details": [{"from": int, "to": int, "label": str}]}
        # Summary and label might be translated
        shared["relationships"] = exec_res


class OrderChapters(Node):
    def prep(self, shared):
        abstractions = shared["abstractions"]  # Name/description might be translated
        relationships = shared["relationships"]  # Summary/label might be translated
        project_name = shared["project_name"]  # Get project name
        language = shared.get("language", "english")  # Get language
        use_cache = shared.get("use_cache", True)  # Get use_cache flag, default to True

        # Prepare context for the LLM
        abstraction_info_for_prompt = []
        for i, a in enumerate(abstractions):
            abstraction_info_for_prompt.append(
                f"- {i} # {a['name']}"
            )  # Use potentially translated name
        abstraction_listing = "\n".join(abstraction_info_for_prompt)

        # Use potentially translated summary and labels
        summary_note = ""
        if language.lower() != "english":
            summary_note = (
                f" (Note: Project Summary might be in {language.capitalize()})"
            )

        context = f"Project Summary{summary_note}:\n{relationships['summary']}\n\n"
        context += "Relationships (Indices refer to abstractions above):\n"
        for rel in relationships["details"]:
            from_name = abstractions[rel["from"]]["name"]
            to_name = abstractions[rel["to"]]["name"]
            # Use potentially translated 'label'
            context += f"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\n"  # Label might be translated

        list_lang_note = ""
        if language.lower() != "english":
            list_lang_note = f" (Names might be in {language.capitalize()})"

        return (
            abstraction_listing,
            context,
            len(abstractions),
            project_name,
            list_lang_note,
            use_cache,
        )  # Return use_cache

    def exec(self, prep_res):
        (
            abstraction_listing,
            context,
            num_abstractions,
            project_name,
            list_lang_note,
            use_cache,
        ) = prep_res  # Unpack use_cache
        print("Determining chapter order using LLM...")
        # No language variation needed here in prompt instructions, just ordering based on structure
        # The input names might be translated, hence the note.
        prompt = f"""
Given the following project abstractions a
... [truncated]
```


RESPONSE:
# nodes.py - File Summary

### one_line  
A collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.

---

### Purpose  
This file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.

---

### Major Functions/Classes

| Class | Role |
|------|------|
| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |
| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |
| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |
| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |
| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |

---

### Key Technical Details & TODOs

- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.
- **YAML Parsing & Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.
- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).
- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.
- **File Handling**: 
  - Converts fetched files to list of `(path, content)` tuples.
  - Uses **index-based referencing** in LLM prompts to avoid path duplication.
- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.
- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.

#### ‚úÖ TODOs / Improvements (inferred):
- [ ] Add **token budgeting** for large codebases in LLM context creation.
- [ ] Support **chunking or summarization** of very large files before LLM ingestion.
- [ ] Allow **custom prompt templates** per language or project type.
- [ ] Add **retry logic with exponential backoff** for LLM calls.
- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.
- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).

---

### Short Usage Example

```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

# Shared context
shared = {
    "repo_url": "https://github.com/user/example-project",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "*.md"],
    "max_file_size": 50000,
    "language": "english",
    "use_cache": True,
    "max_abstraction_num": 8
}

# Build flow
flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

# Run
flow.run(shared)

# Access results
print("Project Summary:", shared["relationships"]["summary"])
print("Abstractions:", shared["abstractions"])
print("Relationships:", shared["relationships"]["details"])
print("Chapter Order:", shared["chapters"])  # List of abstraction indices in order
```

> üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.
---

=== [2025-10-03T13:56:43+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file __init__.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\__init__.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "__init__.py - File Summary" as a header.

```

```


RESPONSE:
# __init__.py - File Summary

**one_line**:  
Utility module initializer that imports and exposes key functions for shared use across the project.

**Purpose**:  
This `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).

**Major functions/classes**:  
- Exposes the following (imported from submodules):
  - `log` ‚Äì Standardized logging function with timestamps and levels.
  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.
  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.
  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).
  - `Timer` ‚Äì Context manager for measuring execution time.

**Key technical details & TODOs**:  
- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.
- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.
- All exposed utilities are stateless and thread-safe.
- **TODO**: Add type hints to all public functions in submodules.
- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.

**Short usage example**:  
```python
from utils import log, retry, read_json, Timer

@retry(max_attempts=3)
def fetch_data():
    log("Fetching data...")
    with Timer("Data load"):
        return read_json("data.json")

data = fetch_data()
```
---

=== [2025-10-03T13:56:52+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file call_llm.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\call_llm.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "call_llm.py - File Summary" as a header.

```
from google import genai
import os
import logging
import json
from datetime import datetime

# Configure logging
log_directory = os.getenv("LOG_DIR", "logs")
os.makedirs(log_directory, exist_ok=True)
log_file = os.path.join(
    log_directory, f"llm_calls_{datetime.now().strftime('%Y%m%d')}.log"
)

# Set up logger
logger = logging.getLogger("llm_logger")
logger.setLevel(logging.INFO)
logger.propagate = False  # Prevent propagation to root logger
file_handler = logging.FileHandler(log_file, encoding='utf-8')
file_handler.setFormatter(
    logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
)
logger.addHandler(file_handler)

# Simple cache configuration
cache_file = "llm_cache.json"


# By default, we Google Gemini 2.5 pro, as it shows great performance for code understanding
def call_llm(prompt: str, use_cache: bool = True) -> str:
    # Log the prompt
    logger.info(f"PROMPT: {prompt}")

    # Check cache if enabled
    if use_cache:
        # Load cache from disk
        cache = {}
        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache = json.load(f)
            except:
                logger.warning(f"Failed to load cache, starting with empty cache")

        # Return from cache if exists
        if prompt in cache:
            logger.info(f"RESPONSE: {cache[prompt]}")
            return cache[prompt]

    # # Call the LLM if not in cache or cache disabled
    # client = genai.Client(
    #     vertexai=True,
    #     # TODO: change to your own project id and location
    #     project=os.getenv("GEMINI_PROJECT_ID", "your-project-id"),
    #     location=os.getenv("GEMINI_LOCATION", "us-central1")
    # )

    # You can comment the previous line and use the AI Studio key instead:
    client = genai.Client(
        api_key=os.getenv("GEMINI_API_KEY", ""),
    )
    model = os.getenv("GEMINI_MODEL", "gemini-2.5-pro")
    # model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
    
    response = client.models.generate_content(model=model, contents=[prompt])
    response_text = response.text

    # Log the response
    logger.info(f"RESPONSE: {response_text}")

    # Update cache if enabled
    if use_cache:
        # Load cache again to avoid overwrites
        cache = {}
        if os.path.exists(cache_file):
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    cache = json.load(f)
            except:
                pass

        # Add to cache and save
        cache[prompt] = response_text
        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(cache, f)
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")

    return response_text


# # Use Azure OpenAI
# def call_llm(prompt, use_cache: bool = True):
#     from openai import AzureOpenAI

#     endpoint = "https://<azure openai name>.openai.azure.com/"
#     deployment = "<deployment name>"

#     subscription_key = "<azure openai key>"
#     api_version = "<api version>"

#     client = AzureOpenAI(
#         api_version=api_version,
#         azure_endpoint=endpoint,
#         api_key=subscription_key,
#     )

#     r = client.chat.completions.create(
#         model=deployment,
#         messages=[{"role": "user", "content": prompt}],
#         response_format={
#             "type": "text"
#         },
#         max_completion_tokens=40000,
#         reasoning_effort="medium",
#         store=False
#     )
#     return r.choices[0].message.content

# # Use Anthropic Claude 3.7 Sonnet Extended Thinking
# def call_llm(prompt, use_cache: bool = True):
#     from anthropic import Anthropic
#     client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY", "your-api-key"))
#     response = client.messages.create(
#         model="claude-3-7-sonnet-20250219",
#         max_tokens=21000,
#         thinking={
#             "type": "enabled",
#             "budget_tokens": 20000
#         },
#         messages=[
#             {"role": "user", "content": prompt}
#         ]
#     )
#     return response.content[1].text

# # Use OpenAI o1
# def call_llm(prompt, use_cache: bool = True):
#     from openai import OpenAI
#     client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", "your-api-key"))
#     r = client.chat.completions.create(
#         model="o1",
#         messages=[{"role": "user", "content": prompt}],
#         response_format={
#             "type": "text"
#         },
#         reasoning_effort="medium",
#         store=False
#     )
#     return r.choices[0].message.content

# Use OpenRouter API
# def call_llm(prompt: str, use_cache: bool = True) -> str:
#     import requests
#     # Log the prompt
#     logger.info(f"PROMPT: {prompt}")

#     # Check cache if enabled
#     if use_cache:
#         # Load cache from disk
#         cache = {}
#         if os.path.exists(cache_file):
#             try:
#                 with open(cache_file, "r", encoding="utf-8") as f:
#                     cache = json.load(f)
#             except:
#                 logger.warning(f"Failed to load cache, starting with empty cache")

#         # Return from cache if exists
#         if prompt in cache:
#             logger.info(f"RESPONSE: {cache[prompt]}")
#             return cache[prompt]

#     # OpenRouter API configuration
#     api_key = os.getenv("OPENROUTER_API_KEY", "")
#     model = os.getenv("OPENROUTER_MODEL", "google/gemini-2.0-flash-exp:free")
    
#     headers = {
#         "Authorization": f"Bearer {api_key}",
#     }

#     data = {
#         "model": model,
#         "messages": [{"role": "user", "content": prompt}]
#     }

#     response = requests.post(
#         "https://openrouter.ai/api/v1/chat/completions",
#         headers=headers,
#         json=data
#     )

#     if response.status_code != 200:
#         error_msg = f"OpenRouter API call failed with status {response.status_code}: {response.text}"
#         logger.error(error_msg)
#         raise Exception(error_msg)
#     try:
#         response_text = response.json()["choices"][0]["message"]["content"]
#     except Exception as e:
#         error_msg = f"Failed to parse OpenRouter response: {e}; Response: {response.text}"
#         logger.error(error_msg)        
#         raise Exception(error_msg)
    

#     # Log the response
#     logger.info(f"RESPONSE: {response_text}")

#     # Update cache if enabled
#     if use_cache:
#         # Load cache again to avoid overwrites
#         cache = {}
#         if os.path.exists(cache_file):
#             try:
#                 with open(cache_file, "r", encoding="utf-8") as f:
#                     cache = json.load(f)
#             except:
#                 pass

#         # Add to cache and save
#         cache[prompt] = response_text
#         try:
#             with open(cache_file, "w", encoding="utf-8") as f:
#                 json.dump(cache, f)
#         except Exception as e:
#             logger.error(f"Failed to save cache: {e}")

#     return response_text

if __name__ == "__main__":
    test_prompt = "Hello, how are you?"

    # First call - should hit the API
    print("Making call...")
    response1 = call_llm(test_prompt, use_cache=False)
    print(f"Response: {response1}")

```


RESPONSE:
# call_llm.py - File Summary

**one_line**:  
Unified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).

**Purpose**:  
Provides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`  
  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.
- **Commented alternative implementations** for:
  - Azure OpenAI
  - Anthropic Claude 3.7 Sonnet (with extended thinking)
  - OpenAI o1
  - OpenRouter (supports any model via OpenRouter API)
- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.
- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.

**Key technical details & TODOs**:
- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.
- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.
- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.
- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.
- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `"your-project-id"`) ‚Äî **TODO: Remove or warn about insecure defaults**.
- üß™ **Testing**: Includes a simple `__main__` block to test the function.
- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.
- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.
- üîß **Configurable via env vars**:
  - `LOG_DIR`: Log directory (default: `logs`)
  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`
  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`
  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.
- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.
- üßπ **TODO**: Add error handling and retry logic for API failures.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Set environment variables first (e.g., GEMINI_API_KEY)
response = call_llm("Explain quantum computing in simple terms")
print(response)

# Disable cache for fresh call
response_fresh = call_llm("Hello", use_cache=False)
```

> üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.
---

=== [2025-10-03T13:57:04+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file crawl_github_files.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_github_files.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "crawl_github_files.py - File Summary" as a header.

```
import requests
import base64
import os
import tempfile
import git
import time
import fnmatch
from typing import Union, Set, List, Dict, Tuple, Any
from urllib.parse import urlparse

def crawl_github_files(
    repo_url, 
    token=None, 
    max_file_size: int = 1 * 1024 * 1024,  # 1 MB
    use_relative_paths: bool = False,
    include_patterns: Union[str, Set[str]] = None,
    exclude_patterns: Union[str, Set[str]] = None
):
    """
    Crawl files from a specific path in a GitHub repository at a specific commit.

    Args:
        repo_url (str): URL of the GitHub repository with specific path and commit
                        (e.g., 'https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')
        token (str, optional): **GitHub personal access token.**
            - **Required for private repositories.**
            - **Recommended for public repos to avoid rate limits.**
            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.
        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)
        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory
        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., "*.py", {"*.md", "*.txt"}).
                                                       If None, all files are included.
        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.
                                                       If None, no files are excluded.

    Returns:
        dict: Dictionary with files and statistics
    """
    # Convert single pattern to set
    if include_patterns and isinstance(include_patterns, str):
        include_patterns = {include_patterns}
    if exclude_patterns and isinstance(exclude_patterns, str):
        exclude_patterns = {exclude_patterns}

    def should_include_file(file_path: str, file_name: str) -> bool:
        """Determine if a file should be included based on patterns"""
        # If no include patterns are specified, include all files
        if not include_patterns:
            include_file = True
        else:
            # Check if file matches any include pattern
            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)

        # If exclude patterns are specified, check if file should be excluded
        if exclude_patterns and include_file:
            # Exclude if file matches any exclude pattern
            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)
            return not exclude_file

        return include_file

    # Detect SSH URL (git@ or .git suffix)
    is_ssh_url = repo_url.startswith("git@") or repo_url.endswith(".git")

    if is_ssh_url:
        # Clone repo via SSH to temp dir
        with tempfile.TemporaryDirectory() as tmpdirname:
            print(f"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...")
            try:
                repo = git.Repo.clone_from(repo_url, tmpdirname)
            except Exception as e:
                print(f"Error cloning repo: {e}")
                return {"files": {}, "stats": {"error": str(e)}}

            # Attempt to checkout specific commit/branch if in URL
            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded
            # So rely on default branch, or user can checkout manually later
            # Optionally, user can pass ref explicitly in future API

            # Walk directory
            files = {}
            skipped_files = []

            for root, dirs, filenames in os.walk(tmpdirname):
                for filename in filenames:
                    abs_path = os.path.join(root, filename)
                    rel_path = os.path.relpath(abs_path, tmpdirname)

                    # Check file size
                    try:
                        file_size = os.path.getsize(abs_path)
                    except OSError:
                        continue

                    if file_size > max_file_size:
                        skipped_files.append((rel_path, file_size))
                        print(f"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}")
                        continue

                    # Check include/exclude patterns
                    if not should_include_file(rel_path, filename):
                        print(f"Skipping {rel_path}: does not match include/exclude patterns")
                        continue

                    # Read content
                    try:
                        with open(abs_path, "r", encoding="utf-8-sig") as f:
                            content = f.read()
                        files[rel_path] = content
                        print(f"Added {rel_path} ({file_size} bytes)")
                    except Exception as e:
                        print(f"Failed to read {rel_path}: {e}")

            return {
                "files": files,
                "stats": {
                    "downloaded_count": len(files),
                    "skipped_count": len(skipped_files),
                    "skipped_files": skipped_files,
                    "base_path": None,
                    "include_patterns": include_patterns,
                    "exclude_patterns": exclude_patterns,
                    "source": "ssh_clone"
                }
            }

    # Parse GitHub URL to extract owner, repo, commit/branch, and path
    parsed_url = urlparse(repo_url)
    path_parts = parsed_url.path.strip('/').split('/')
    
    if len(path_parts) < 2:
        raise ValueError(f"Invalid GitHub URL: {repo_url}")
    
    # Extract the basic components
    owner = path_parts[0]
    repo = path_parts[1]
    
    # Setup for GitHub API
    headers = {"Accept": "application/vnd.github.v3+json"}
    if token:
        headers["Authorization"] = f"token {token}"

    def fetch_branches(owner: str, repo: str):
        """Get brancshes of the repository"""

        url = f"https://api.github.com/repos/{owner}/{repo}/branches"
        response = requests.get(url, headers=headers, timeout=(30, 30))

        if response.status_code == 404:
            if not token:
                print(f"Error 404: Repository not found or is private.\n"
                      f"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.")
            else:
                print(f"Error 404: Repository not found or insufficient permissions with the provided token.\n"
                      f"Please verify the repository exists and the token has access to this repository.")
            return []
            
        if response.status_code != 200:
            print(f"Error fetching the branches of {owner}/{repo}: {response.status_code} - {response.text}")
            return []

        return response.json()

    def check_tree(owner: str, repo: str, tree: str):
        """Check the repository has the given tree"""

        url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{tree}"
        response = requests.get(url, headers=headers, timeout=(30, 30))

        return True if response.status_code == 200 else False 

    # Check if URL contains a specific branch/commit
    if len(path_parts) > 2 and 'tree' == path_parts[2]:
        join_parts = lambda i: '/'.join(path_parts[i:])

        branches = fetch_branches(owner, repo)
        branch_names = map(lambda branch: branch.get("name"), branches)

        # Fetching branches is not successfully
        if len(branches) == 0:
            return

        # To check branch name
        relevant_path = join_parts(3)

        # Find a match with relevant path and get the branch name
        filter_gen = (name for name in branch_names if relevant_path.startswith(name))
        ref = next(filter_gen, None)

        # If match is not found, check for is it a tree
        if ref == None:
            tree = path_parts[3]
            ref = tree if check_tree(owner, repo, tree) else None

        # If it is neither a tree nor a branch name
        if ref == None:
            print(f"The given path does not match with any branch and any tree in the repository.\n"
                  f"Please verify the path is exists.")
            return

        # Combine all parts after the ref as the path
        part_index = 5 if '/' in ref else 4
        specific_path = join_parts(part_index) if part_index < len(path_parts) else ""
    else:
        # Dont put the ref param to quiery
        # and let Github decide default branch
        ref = None
        specific_path = ""
    
    # Dictionary to store path -> content mapping
    files = {}
    skipped_files = []
    
    def fetch_contents(path):
        """Fetch contents of the repository at a specific path and commit"""
        url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}"
        params = {"ref": ref} if ref != None else {}
        
        response = requests.get(url, headers=headers, params=params, timeout=(30, 30))
        
        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():
            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
            wait_time = max(reset_time - time.time(), 0) + 1
            print(f"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...")
            time.sleep(wait_time)
            return fetch_contents(path)
            
        if response.status_code == 404:
            if not token:
                print(f"Error 404: Repository not found or is private.\n"
                      f"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.")
            elif not path and ref == 'main':
                print(f"Error 404: Repository not found. Check if the default branch is not 'main'\n"
                      f"Try adding branch name to the request i.e. python main.py --repo https://github.com/username/repo/tree/master")
            else:
                print(f"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\n"
                      f"Please verify the token has access to this repository and the path exists.")
            return
            
        if response.status_code != 200:
            print(f"Error fetching {path}: {response.status_code} - {response.text}")
            return
        
        contents = response.json()
        
        # Handle both single file and directory responses
        if not isinstance(contents, list):
            contents = [contents]
        
        for item in contents:
            item_path = item["path"]
            
            # Calculate relative path if requested
            if use_relative_paths and specific_path:
                # Make sure the path is relative to the specified subdirectory
                if item_path.startswith(specific_path):
                    rel_path = item_path[len(specific_path):].lstrip('/')
                else:
                    rel_path = item_path
            else:
                rel_path = item_path
            
            if item["type"] == "file":
                # Check if file should be included based on patterns
                if not should_include_file(rel_path, item["name"]):
                    print(f"Skipping {rel_path}: Does not match include/exclude patterns")
                    continue
                
                # Check file size if available
                file_size = item.get("size", 0)
                if file_size > max_file_size:
                    skipped_files.append((item_path, file_size))
                    print(f"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)")
                    continue
                
                # For files, get raw content
                if "download_url" in item and item["download_url"]:
                    file_url = item["download_url"]
                    file_response = requests.get(file_url, headers=headers, timeout=(30, 30))
                    
                    # Final size check in case content-length header is available but differs from metadata
                    content_length = int(file_response.headers.get('content-length', 0))
                    if content_length > max_file_size:
                        skipped_files.append((item_path, content_length))
                        print(f"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)")
                        continue
                        
                    if file_response.status_code == 200:
                        files[rel_path] = file_response.text
                        print(f"Downloaded: {rel_path} ({file_size} bytes) ")
                    else:
                        print(f"Failed to download {rel_path}: {file_response.status_code}")
                else:
                    # Alternative method if download_url is not available
                    content_response = requests.get(item["url"], headers=headers, timeout=(30, 30))
                    if content_response.status_code == 200:
                        content_data = content_response.json()
                        if content_data.get("encoding") == "base64" and "content" in content_data:
                            # Check size of base64 content before decoding
                            if len(content_data["content"]) * 0.75 > max_file_size:  # Approximate size calculation
                                estimated_size = int(len(content_data["content"]) * 0.75)
                                skipped_files.append((item_path, estimated_size))
                                print(f"Skipping {rel_path}: Encoded content exceeds size limit")
                                continue
                                
                            file_content = base64.b64decode(content_data["content"]).decode('utf-8')
                            files[rel_path] = file_content
                            print(f"Downloaded: {rel_path} ({file_size} bytes)")
                        else:
                            print(f"Unexpected content format for {rel_path}")
                    else:
                        print(f"Failed to get content for {rel_path}: {content_response.status_code}")
            
            elif item["type"] == "dir":
                # OLD IMPLEMENTATION (comment this block to test new implementation)
                # Always recurse into directories without checking exclusions first
                # fetch_contents(item_path)

                # NEW IMPLEMENTATION (uncomment this block to test optimized version)
                # # Check if directory should be excluded before recursing
                if exclude_patterns:
                    dir_excluded = any(fnmatch.fnmatch(item_path, pattern) or
                                    fnmatch.fnmatch(rel_path, pattern) for pattern in exclude_patterns)
                    if dir_excluded:
                        continue
                
                # # Only recurse if directory is not excluded
                fetch_contents(item_path)
    
    # Start crawling from the specified path
    fetch_contents(specific_path)
    
    return {
        "files": files,
        "stats": {
            "downloaded_count": len(files),
            "skipped_count": len(skipped_files),
            "skipped_files": skipped_files,
            "base_path": specific_path if use_relative_paths else None,
            "include_patterns": include_patterns,
            "exclude_patterns": exclude_patterns
        }
    }

# Example usage
if __name__ == "__main__":
    # Get token from environment variable (recommended for private repos)
    github_token = os.environ.get("GITHUB_TOKEN")
    if not github_token:
        print("Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\n"
              "Private repositories will not be accessible without a token.\n"
              "To access private repos, set the environment variable or pass the token explicitly.")
    
    repo_url = "https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic"
    
    # Example: Get Python and Markdown files, but exclude test files
    result = crawl_github_files(
        repo_url, 
        token=github_token,
        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes
        use_relative_paths=True,  # Enable relative paths
        include_patterns={"*.py", "*.md"},  # Include Python and Markdown files
    )
    
    files = result["files"]
    stats = result["stats"]
    
    print(f"\nDownloaded {stats['downloaded_count']} files.")
    print(f"Skipped {stats['skipped_count']} files due to size limits or patterns.")
    print(f"Base path for relative paths: {stats['base_path']}")
    print(f"Include patterns: {stats['include_patterns']}")
    print(f"Exclude patterns: {stats['exclude_patterns']}")
    
    # Display all file paths in the dictionary
    print("\nFiles in dictionary:")
    for file_path in sorted(files.keys()):
        print(f"  {file_path}")
    
    # Example: accessing content of a specific file
    if files:
        sample_file = next(iter(files))
        print(f"\nSample file: {sample_file}")
        print(f"Content preview: {files[sample_file][:200]}...")

```


RESPONSE:
# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.

---

### Purpose  
This script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).

---

### Major Functions/Classes

#### `crawl_github_files(...)`
Main function that orchestrates crawling:
- Parses GitHub URL (including commit/branch and subdirectory).
- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.
- Downloads files recursively from a specified subdirectory.
- Applies **include/exclude glob patterns** to filter files.
- Enforces **maximum file size** to avoid memory issues.
- Returns a dictionary with file contents and crawl statistics.

#### `should_include_file(file_path, file_name)`
Helper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.

#### `fetch_contents(path)` (nested)
Recursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.

#### `fetch_branches()` and `check_tree()`
Used to validate and extract commit/branch reference from the URL when not explicitly provided.

---

### Key Technical Details & TODOs

#### ‚úÖ **Features**
- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).
- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.
- **Two URL modes**:
  - HTTPS: Uses GitHub API (supports branch/commit + path).
  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).
- **Flexible filtering**:
  - `include_patterns`: e.g., `{"*.py", "*.md"}`
  - `exclude_patterns`: e.g., `{"*test*", "*.log"}`
- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.
- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.

#### ‚ö†Ô∏è **Technical Notes**
- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.
- **File size is checked twice**: once from metadata, once from `download_url` headers.
- **Base64 decoding** used when `download_url` is missing (fallback).
- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).
- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).

#### üîß **TODOs / Improvements**
- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).
- [ ] **Add progress bar or logging** for large repositories.
- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.
- [ ] **Caching mechanism** to avoid re-downloading unchanged files.
- [ ] **Better handling of symlinks and binary files** (currently skipped silently).
- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.
- [ ] **Add retry logic** for transient network failures.

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Optional: Set token for private repos or to avoid rate limits
os.environ["GITHUB_TOKEN"] = "your_token_here"

result = crawl_github_files(
    repo_url="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core",
    token=None,  # uses GITHUB_TOKEN env
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"*test*"}
)

print(f"Downloaded {result['stats']['downloaded_count']} files")
for path, content in result["files"].items():
    print(f"{path}: {len(content)} chars")
```

> üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.
---

=== [2025-10-03T13:57:13+05:30] MODEL: llama3.2 ===
PROMPT:
You are a code summarization assistant. Produce a clear, developer-friendly summary in english for the file crawl_local_files.py (path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_local_files.py).

Instructions:
1) Provide a short "one_line" summary.
2) Purpose of the file.
3) Major functions/classes.
4) Key technical details & TODOs.
5) Short usage example if applicable.

Respond in Markdown. Begin with "crawl_local_files.py - File Summary" as a header.

```
import os
import fnmatch
import pathspec


def crawl_local_files(
    directory,
    include_patterns=None,
    exclude_patterns=None,
    max_file_size=None,
    use_relative_paths=True,
):
    """
    Crawl files in a local directory with similar interface as crawl_github_files.
    Args:
        directory (str): Path to local directory
        include_patterns (set): File patterns to include (e.g. {"*.py", "*.js"})
        exclude_patterns (set): File patterns to exclude (e.g. {"tests/*"})
        max_file_size (int): Maximum file size in bytes
        use_relative_paths (bool): Whether to use paths relative to directory

    Returns:
        dict: {"files": {filepath: content}}
    """
    if not os.path.isdir(directory):
        raise ValueError(f"Directory does not exist: {directory}")

    files_dict = {}

    # --- Load .gitignore ---
    gitignore_path = os.path.join(directory, ".gitignore")
    gitignore_spec = None
    if os.path.exists(gitignore_path):
        try:
            with open(gitignore_path, "r", encoding="utf-8-sig") as f:
                gitignore_patterns = f.readlines()
            gitignore_spec = pathspec.PathSpec.from_lines("gitwildmatch", gitignore_patterns)
            print(f"Loaded .gitignore patterns from {gitignore_path}")
        except Exception as e:
            print(f"Warning: Could not read or parse .gitignore file {gitignore_path}: {e}")

    all_files = []
    for root, dirs, files in os.walk(directory):
        # Filter directories using .gitignore and exclude_patterns early
        excluded_dirs = set()
        for d in dirs:
            dirpath_rel = os.path.relpath(os.path.join(root, d), directory)

            if gitignore_spec and gitignore_spec.match_file(dirpath_rel):
                excluded_dirs.add(d)
                continue

            if exclude_patterns:
                for pattern in exclude_patterns:
                    if fnmatch.fnmatch(dirpath_rel, pattern) or fnmatch.fnmatch(d, pattern):
                        excluded_dirs.add(d)
                        break

        for d in dirs.copy():
            if d in excluded_dirs:
                dirs.remove(d)

        for filename in files:
            filepath = os.path.join(root, filename)
            all_files.append(filepath)

    total_files = len(all_files)
    processed_files = 0

    for filepath in all_files:
        relpath = os.path.relpath(filepath, directory) if use_relative_paths else filepath

        # --- Exclusion check ---
        excluded = False
        if gitignore_spec and gitignore_spec.match_file(relpath):
            excluded = True

        if not excluded and exclude_patterns:
            for pattern in exclude_patterns:
                if fnmatch.fnmatch(relpath, pattern):
                    excluded = True
                    break

        included = False
        if include_patterns:
            for pattern in include_patterns:
                if fnmatch.fnmatch(relpath, pattern):
                    included = True
                    break
        else:
            included = True

        processed_files += 1 # Increment processed count regardless of inclusion/exclusion

        status = "processed"
        if not included or excluded:
            status = "skipped (excluded)"
            # Print progress for skipped files due to exclusion
            if total_files > 0:
                percentage = (processed_files / total_files) * 100
                rounded_percentage = int(percentage)
                print(f"\033[92mProgress: {processed_files}/{total_files} ({rounded_percentage}%) {relpath} [{status}]\033[0m")
            continue # Skip to next file if not included or excluded

        if max_file_size and os.path.getsize(filepath) > max_file_size:
            status = "skipped (size limit)"
            # Print progress for skipped files due to size limit
            if total_files > 0:
                percentage = (processed_files / total_files) * 100
                rounded_percentage = int(percentage)
                print(f"\033[92mProgress: {processed_files}/{total_files} ({rounded_percentage}%) {relpath} [{status}]\033[0m")
            continue # Skip large files

        # --- File is being processed ---        
        try:
            with open(filepath, "r", encoding="utf-8-sig") as f:
                content = f.read()
            files_dict[relpath] = content
        except Exception as e:
            print(f"Warning: Could not read file {filepath}: {e}")
            status = "skipped (read error)"

        # --- Print progress for processed or error files ---
        if total_files > 0:
            percentage = (processed_files / total_files) * 100
            rounded_percentage = int(percentage)
            print(f"\033[92mProgress: {processed_files}/{total_files} ({rounded_percentage}%) {relpath} [{status}]\033[0m")

    return {"files": files_dict}


if __name__ == "__main__":
    print("--- Crawling parent directory ('..') ---")
    files_data = crawl_local_files(
        "..",
        exclude_patterns={
            "*.pyc",
            "__pycache__/*",
            ".venv/*",
            ".git/*",
            "docs/*",
            "output/*",
        },
    )
    print(f"Found {len(files_data['files'])} files:")
    for path in files_data["files"]:
        print(f"  {path}")
```


RESPONSE:
# crawl_local_files.py - File Summary

### one_line
Crawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.

---

### Purpose
This utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.

---

### Major functions/classes

- **`crawl_local_files()`**  
  Main function that:
  - Walks a directory tree (`os.walk`)
  - Applies `.gitignore` rules (if present)
  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`
  - Enforces a maximum file size
  - Reads and returns file contents as a dictionary
  - Shows real-time progress with colored output

---

### Key technical details & TODOs

#### ‚úÖ **Features**
- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).
- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).
- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).
- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\033[92m`).
- **Relative Paths**: Optionally returns paths relative to input directory.
- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.
- **Error Handling**: Gracefully skips unreadable files and logs warnings.

#### ‚ö†Ô∏è **Limitations / TODOs**
- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).
- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.
- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.
- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.
- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.

> üîß **Suggested Improvements (TODOs):**
> - Add `verbose`/`quiet` mode flag to control output.
> - Add option to detect and skip binary files.
> - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).
> - Return metadata (size, mtime) alongside content.
> - Allow custom file read handlers (e.g., for line filtering).

---

### Short usage example

```python
# Crawl current directory, include only Python files, exclude test and cache dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"tests/*", "__pycache__/*", "*.log"},
    max_file_size=1024 * 1024,  # 1 MB limit
    use_relative_paths=True
)

# Print file paths
for path in result["files"]:
    print(path)

# Use file content
for path, content in result["files"].items():
    print(f"--- {path} ---\n{content[:200]}...\n")
```

> üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).
---

=== [2025-10-03T13:57:26+05:30] MODEL: llama3.2 ===
PROMPT:
You are an assistant that identifies architectural abstractions and relationships for a codebase. Identify up to 15 core abstractions/components and for each provide:
- name
- one-line description
- files
- dependencies
- responsibilities

Output as Markdown. Use english language.

File summaries:
### File: Dockerfile
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\Dockerfile

# Dockerfile - File Summary

**one_line**:  
A minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.

**Purpose of the file**:  
This Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.

**Major functions/classes**:  
- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).
- However, the key *build stages* are:
  - Base image setup (`python:3.10-slim`)
  - System package installation (`git`)
  - Python dependency installation via `pip`
  - Application code copy
  - Entrypoint configuration to run `main.py`

**Key technical details & TODOs**:  
- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.
- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.
- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.
- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).
- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.
- **TODOs / Recommendations**:
  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.
  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).
  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.
  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.

**Short usage example if applicable**:  
```bash
# Build the image
docker build -t my-python-app .

# Run the container (assumes main.py accepts args or runs standalone)
docker run --rm my-python-app

# Example with environment variables
docker run --rm -e ENV=prod my-python-app
```
---


### File: README.md
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\README.md

# README.md - File Summary

## one_line
A tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.

## Purpose of the file
The `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.

## Major functions/classes
- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.
- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.
- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.
- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).
- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.
- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).

## Key technical details & TODOs
- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.
- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.
- **Language support**: Tutorials can be generated in any language (`--language` flag).
- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).
- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.
- **Docker support**: Fully containerized with volume mounts for output and code input.
- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.
- **TODOs / Improvements (implied)**:
  - Add support for more visualization types (e.g., Mermaid diagrams in output).
  - Improve handling of large or complex codebases (e.g., better abstraction summarization).
  - Enhance caching strategy (e.g., persistent cache across runs).
  - Support multi-repo or monorepo analysis.

## Short usage example
```bash
# Analyze a GitHub repo and generate an English tutorial
python main.py --repo https://github.com/encode/fastapi --include "*.py" --exclude "tests/*" --max-size 100000

# Analyze a local directory and generate a Chinese tutorial
python main.py --dir ./my-project --include "*.js" "*.ts" --language "Chinese"

# Run in Docker (requires GEMINI_API_KEY)
docker run -it --rm \
  -e GEMINI_API_KEY="your_key" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/pallets/flask
```

> ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.
---


### File: flow.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\flow.py

# flow.py - File Summary

**one_line**:  
Orchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.

**Purpose**:  
This file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  The main function that:
  - Instantiates all node classes (processing units) for each stage of tutorial generation.
  - Chains them together in a directed sequence using the `>>` operator (PocketFlow's flow syntax).
  - Returns a `Flow` object starting at `FetchRepo`.

  **Key Nodes Used** (imported from `nodes.py`):
  - `FetchRepo`: Retrieves/clones the target repository.
  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.
  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.
  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).
  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.

**Key technical details & TODOs**:  
- **PocketFlow Framework**: Uses `Flow` and node chaining (`>>`) to define a directed acyclic graph (DAG) of tasks.
- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).
- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.
- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.
- **TODO (implied)**:  
  - Error handling or fallback logic is not visible here (likely handled in nodes).  
  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  
  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation pipeline
flow = create_tutorial_flow()
flow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine
```  

> **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file.
---


### File: main.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\main.py

# main.py - File Summary

**one_line**:  
CLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.

**Purpose of the file**:  
This script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.

**Major functions/classes**:  
- `main()`:  
  - Sets up argument parsing using `argparse`.
  - Validates and prepares inputs (source, patterns, token, language, etc.).
  - Initializes the `shared` state dictionary passed through the flow.
  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.
- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).

**Key technical details & TODOs**:  
- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.
- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).
- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.
- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.
- **Language support**: `--language` parameter allows multilingual output (default: English).
- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).
- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.
- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.
- **TODOs (implied)**:  
  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).
  - No validation of language codes (assumes valid input).
  - No progress tracking or verbose mode (could benefit from `--verbose` flag).

**Short usage example**:  
```bash
# Generate tutorial from GitHub repo in French, with custom patterns
python main.py \
  --repo https://github.com/user/repo \
  --token ghp_... \
  --output ./tutorials/repo \
  --include "*.py" "*.md" \
  --exclude "tests/*" \
  --language french \
  --max-abstractions 15 \
  --no-cache

# Generate tutorial from local directory
python main.py \
  --dir ./my_project \
  --name "My Project" \
  --output ./docs/tutorial \
  --language spanish
```
---


### File: nodes.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\nodes.py

# nodes.py - File Summary

### one_line  
A collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.

---

### Purpose  
This file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.

---

### Major Functions/Classes

| Class | Role |
|------|------|
| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |
| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |
| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |
| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |
| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |

---

### Key Technical Details & TODOs

- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.
- **YAML Parsing & Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.
- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).
- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.
- **File Handling**: 
  - Converts fetched files to list of `(path, content)` tuples.
  - Uses **index-based referencing** in LLM prompts to avoid path duplication.
- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.
- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.

#### ‚úÖ TODOs / Improvements (inferred):
- [ ] Add **token budgeting** for large codebases in LLM context creation.
- [ ] Support **chunking or summarization** of very large files before LLM ingestion.
- [ ] Allow **custom prompt templates** per language or project type.
- [ ] Add **retry logic with exponential backoff** for LLM calls.
- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.
- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).

---

### Short Usage Example

```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

# Shared context
shared = {
    "repo_url": "https://github.com/user/example-project",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "*.md"],
    "max_file_size": 50000,
    "language": "english",
    "use_cache": True,
    "max_abstraction_num": 8
}

# Build flow
flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

# Run
flow.run(shared)

# Access results
print("Project Summary:", shared["relationships"]["summary"])
print("Abstractions:", shared["abstractions"])
print("Relationships:", shared["relationships"]["details"])
print("Chapter Order:", shared["chapters"])  # List of abstraction indices in order
```

> üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.
---


### File: __init__.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\__init__.py

# __init__.py - File Summary

**one_line**:  
Utility module initializer that imports and exposes key functions for shared use across the project.

**Purpose**:  
This `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).

**Major functions/classes**:  
- Exposes the following (imported from submodules):
  - `log` ‚Äì Standardized logging function with timestamps and levels.
  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.
  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.
  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).
  - `Timer` ‚Äì Context manager for measuring execution time.

**Key technical details & TODOs**:  
- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.
- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.
- All exposed utilities are stateless and thread-safe.
- **TODO**: Add type hints to all public functions in submodules.
- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.

**Short usage example**:  
```python
from utils import log, retry, read_json, Timer

@retry(max_attempts=3)
def fetch_data():
    log("Fetching data...")
    with Timer("Data load"):
        return read_json("data.json")

data = fetch_data()
```
---


### File: call_llm.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\call_llm.py

# call_llm.py - File Summary

**one_line**:  
Unified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).

**Purpose**:  
Provides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`  
  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.
- **Commented alternative implementations** for:
  - Azure OpenAI
  - Anthropic Claude 3.7 Sonnet (with extended thinking)
  - OpenAI o1
  - OpenRouter (supports any model via OpenRouter API)
- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.
- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.

**Key technical details & TODOs**:
- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.
- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.
- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.
- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.
- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `"your-project-id"`) ‚Äî **TODO: Remove or warn about insecure defaults**.
- üß™ **Testing**: Includes a simple `__main__` block to test the function.
- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.
- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.
- üîß **Configurable via env vars**:
  - `LOG_DIR`: Log directory (default: `logs`)
  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`
  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`
  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.
- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.
- üßπ **TODO**: Add error handling and retry logic for API failures.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Set environment variables first (e.g., GEMINI_API_KEY)
response = call_llm("Explain quantum computing in simple terms")
print(response)

# Disable cache for fresh call
response_fresh = call_llm("Hello", use_cache=False)
```

> üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.
---


### File: crawl_github_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_github_files.py

# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.

---

### Purpose  
This script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).

---

### Major Functions/Classes

#### `crawl_github_files(...)`
Main function that orchestrates crawling:
- Parses GitHub URL (including commit/branch and subdirectory).
- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.
- Downloads files recursively from a specified subdirectory.
- Applies **include/exclude glob patterns** to filter files.
- Enforces **maximum file size** to avoid memory issues.
- Returns a dictionary with file contents and crawl statistics.

#### `should_include_file(file_path, file_name)`
Helper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.

#### `fetch_contents(path)` (nested)
Recursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.

#### `fetch_branches()` and `check_tree()`
Used to validate and extract commit/branch reference from the URL when not explicitly provided.

---

### Key Technical Details & TODOs

#### ‚úÖ **Features**
- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).
- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.
- **Two URL modes**:
  - HTTPS: Uses GitHub API (supports branch/commit + path).
  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).
- **Flexible filtering**:
  - `include_patterns`: e.g., `{"*.py", "*.md"}`
  - `exclude_patterns`: e.g., `{"*test*", "*.log"}`
- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.
- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.

#### ‚ö†Ô∏è **Technical Notes**
- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.
- **File size is checked twice**: once from metadata, once from `download_url` headers.
- **Base64 decoding** used when `download_url` is missing (fallback).
- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).
- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).

#### üîß **TODOs / Improvements**
- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).
- [ ] **Add progress bar or logging** for large repositories.
- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.
- [ ] **Caching mechanism** to avoid re-downloading unchanged files.
- [ ] **Better handling of symlinks and binary files** (currently skipped silently).
- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.
- [ ] **Add retry logic** for transient network failures.

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Optional: Set token for private repos or to avoid rate limits
os.environ["GITHUB_TOKEN"] = "your_token_here"

result = crawl_github_files(
    repo_url="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core",
    token=None,  # uses GITHUB_TOKEN env
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"*test*"}
)

print(f"Downloaded {result['stats']['downloaded_count']} files")
for path, content in result["files"].items():
    print(f"{path}: {len(content)} chars")
```

> üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.
---


### File: crawl_local_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_local_files.py

# crawl_local_files.py - File Summary

### one_line
Crawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.

---

### Purpose
This utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.

---

### Major functions/classes

- **`crawl_local_files()`**  
  Main function that:
  - Walks a directory tree (`os.walk`)
  - Applies `.gitignore` rules (if present)
  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`
  - Enforces a maximum file size
  - Reads and returns file contents as a dictionary
  - Shows real-time progress with colored output

---

### Key technical details & TODOs

#### ‚úÖ **Features**
- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).
- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).
- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).
- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\033[92m`).
- **Relative Paths**: Optionally returns paths relative to input directory.
- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.
- **Error Handling**: Gracefully skips unreadable files and logs warnings.

#### ‚ö†Ô∏è **Limitations / TODOs**
- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).
- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.
- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.
- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.
- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.

> üîß **Suggested Improvements (TODOs):**
> - Add `verbose`/`quiet` mode flag to control output.
> - Add option to detect and skip binary files.
> - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).
> - Return metadata (size, mtime) alongside content.
> - Allow custom file read handlers (e.g., for line filtering).

---

### Short usage example

```python
# Crawl current directory, include only Python files, exclude test and cache dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"tests/*", "__pycache__/*", "*.log"},
    max_file_size=1024 * 1024,  # 1 MB limit
    use_relative_paths=True
)

# Print file paths
for path in result["files"]:
    print(path)

# Use file content
for path, content in result["files"].items():
    print(f"--- {path} ---\n{content[:200]}...\n")
```

> üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).
---



RESPONSE:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.

---

## 1. **TutorialFlow**
**Description**:  
Orchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.

**Files**:  
- `flow.py`, `main.py`, `nodes.py`

**Dependencies**:  
- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`

**Responsibilities**:  
- Defines and executes the sequential workflow (DAG) for tutorial generation.
- Manages data flow between nodes via the `shared` context.
- Enables retry logic and batch processing for robustness and performance.

---

## 2. **FetchRepo**
**Description**:  
Retrieves source code from a GitHub repository or local directory with filtering and size constraints.

**Files**:  
- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`

**Dependencies**:  
- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`

**Responsibilities**:  
- Parses repo URL or local path input.
- Downloads and filters files using glob patterns and `.gitignore`.
- Enforces maximum file size to prevent memory overflow.
- Returns structured list of `(path, content)` tuples for downstream processing.

---

## 3. **IdentifyAbstractions**
**Description**:  
Uses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.

**Files**:  
- `nodes.py` (class: `IdentifyAbstractions`)

**Dependencies**:  
- `call_llm`, `utils.read_json`, `yaml`, `shared` context

**Responsibilities**:  
- Constructs prompts with code context (by file index) for LLM.
- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.
- Validates output structure and enforces limits (e.g., max 10 abstractions).
- Stores results in `shared["abstractions"]`.

---

## 4. **AnalyzeRelationships**
**Description**:  
Analyzes how identified abstractions interact and generates a project summary and relationship map.

**Files**:  
- `nodes.py` (class: `AnalyzeRelationships`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Builds LLM prompt with abstractions and relevant file snippets.
- Extracts project summary and pairwise relationships (e.g., "X depends on Y").
- Outputs structured YAML into `shared["relationships"]` for tutorial structuring.

---

## 5. **OrderChapters**
**Description**:  
Determines a pedagogical order for tutorial chapters based on abstraction relationships.

**Files**:  
- `nodes.py` (class: `OrderChapters`)

**Dependencies**:  
- `call_llm`, `shared` context (abstractions, relationships)

**Responsibilities**:  
- Uses LLM to reason about dependencies and learning sequence.
- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).
- Stores result in `shared["chapters"]`.

---

## 6. **WriteChapters**
**Description**:  
Generates written content for each chapter (abstraction) using LLM, in a target language.

**Files**:  
- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Processes multiple abstractions in parallel (batch mode).
- For each chapter, generates beginner-friendly explanations with code examples.
- Supports multilingual output via language parameter.
- Stores chapter content in `shared["chapter_contents"]`.

---

## 7. **CombineTutorial**
**Description**:  
Merges all generated chapters into a single, structured tutorial document.

**Files**:  
- `nodes.py` (class: `CombineTutorial`)

**Dependencies**:  
- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)

**Responsibilities**:  
- Combines chapter content in the ordered sequence.
- Adds metadata (title, language, summary).
- Outputs final tutorial (Markdown/HTML) to specified output directory.

---

## 8. **CallLLM**
**Description**:  
Unified interface to call various LLMs (Gemini, Claude
---

=== [2025-10-03T13:57:37+05:30] MODEL: llama3.2 ===
PROMPT:
You are writing a tutorial plan for a codebase. Generate a detailed chapter outline in english language.

Abstractions:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.

---

## 1. **TutorialFlow**
**Description**:  
Orchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.

**Files**:  
- `flow.py`, `main.py`, `nodes.py`

**Dependencies**:  
- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`

**Responsibilities**:  
- Defines and executes the sequential workflow (DAG) for tutorial generation.
- Manages data flow between nodes via the `shared` context.
- Enables retry logic and batch processing for robustness and performance.

---

## 2. **FetchRepo**
**Description**:  
Retrieves source code from a GitHub repository or local directory with filtering and size constraints.

**Files**:  
- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`

**Dependencies**:  
- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`

**Responsibilities**:  
- Parses repo URL or local path input.
- Downloads and filters files using glob patterns and `.gitignore`.
- Enforces maximum file size to prevent memory overflow.
- Returns structured list of `(path, content)` tuples for downstream processing.

---

## 3. **IdentifyAbstractions**
**Description**:  
Uses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.

**Files**:  
- `nodes.py` (class: `IdentifyAbstractions`)

**Dependencies**:  
- `call_llm`, `utils.read_json`, `yaml`, `shared` context

**Responsibilities**:  
- Constructs prompts with code context (by file index) for LLM.
- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.
- Validates output structure and enforces limits (e.g., max 10 abstractions).
- Stores results in `shared["abstractions"]`.

---

## 4. **AnalyzeRelationships**
**Description**:  
Analyzes how identified abstractions interact and generates a project summary and relationship map.

**Files**:  
- `nodes.py` (class: `AnalyzeRelationships`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Builds LLM prompt with abstractions and relevant file snippets.
- Extracts project summary and pairwise relationships (e.g., "X depends on Y").
- Outputs structured YAML into `shared["relationships"]` for tutorial structuring.

---

## 5. **OrderChapters**
**Description**:  
Determines a pedagogical order for tutorial chapters based on abstraction relationships.

**Files**:  
- `nodes.py` (class: `OrderChapters`)

**Dependencies**:  
- `call_llm`, `shared` context (abstractions, relationships)

**Responsibilities**:  
- Uses LLM to reason about dependencies and learning sequence.
- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).
- Stores result in `shared["chapters"]`.

---

## 6. **WriteChapters**
**Description**:  
Generates written content for each chapter (abstraction) using LLM, in a target language.

**Files**:  
- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Processes multiple abstractions in parallel (batch mode).
- For each chapter, generates beginner-friendly explanations with code examples.
- Supports multilingual output via language parameter.
- Stores chapter content in `shared["chapter_contents"]`.

---

## 7. **CombineTutorial**
**Description**:  
Merges all generated chapters into a single, structured tutorial document.

**Files**:  
- `nodes.py` (class: `CombineTutorial`)

**Dependencies**:  
- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)

**Responsibilities**:  
- Combines chapter content in the ordered sequence.
- Adds metadata (title, language, summary).
- Outputs final tutorial (Markdown/HTML) to specified output directory.

---

## 8. **CallLLM**
**Description**:  
Unified interface to call various LLMs (Gemini, Claude


RESPONSE:
Here's a **detailed chapter outline** for a **tutorial plan** on the codebase, designed to guide developers, contributors, or users through understanding, using, extending, and maintaining the system. The structure balances **technical depth**, **progressive learning**, and **practical application**, while aligning with the core abstractions you've provided.

---

# üìò **Tutorial Plan: Building AI-Powered Codebase Tutorials with PocketFlow**

> **Target Audience**: Developers, technical writers, AI/ML engineers, open-source contributors  
> **Prerequisites**: Basic Python, Git, understanding of LLMs, familiarity with software architecture concepts  
> **Goal**: Enable readers to understand, run, extend, and customize the tutorial generation pipeline

---

## **Chapter 1: Introduction & Vision**
**Objective**: Set the stage for the project and its value proposition.

- 1.1 What is this project?  
  - AI-generated, pedagogically structured tutorials from any codebase
  - Powered by PocketFlow and LLMs
- 1.2 Why does this matter?  
  - Onboarding new developers faster
  - Automating documentation for complex systems
  - Supporting multilingual and beginner-friendly explanations
- 1.3 Real-world use cases  
  - Open-source onboarding
  - Internal developer training
  - Educational platforms
- 1.4 Key features overview  
  - GitHub/local repo input
  - Abstraction detection
  - Pedagogical ordering
  - Multilingual output
  - Batch & retry support
- 1.5 How this tutorial is structured  
  - From high-level flow ‚Üí component deep dives ‚Üí customization ‚Üí deployment

---

## **Chapter 2: System Overview & High-Level Architecture**
**Objective**: Introduce the end-to-end flow and how components interact.

- 2.1 The TutorialFlow DAG (Directed Acyclic Graph)  
  - Visual walkthrough of the workflow
  - Node sequence: `FetchRepo ‚Üí IdentifyAbstractions ‚Üí AnalyzeRelationships ‚Üí OrderChapters ‚Üí WriteChapters ‚Üí CombineTutorial`
- 2.2 Data flow via `shared` context  
  - What is `shared`? How is data passed between nodes?
  - Lifecycle of key data structures: `files`, `abstractions`, `relationships`, `chapters`, `chapter_contents`
- 2.3 PocketFlow integration  
  - Why PocketFlow? (Lightweight, composable, batchable)
  - `Node`, `Flow`, `BatchNode` usage patterns
- 2.4 Architecture diagram (with component interactions)
- 2.5 Execution modes: CLI vs. programmatic
- 2.6 Error handling & retry mechanisms

---

## **Chapter 3: Setting Up Your Environment**
**Objective**: Prepare the reader to run and experiment with the system.

- 3.1 Prerequisites & dependencies  
  - Python 3.9+, `pip`, Git
  - Required packages: `requests`, `pathspec`, `PyYAML`, `google-generativeai` / `anthropic`, etc.
- 3.2 Installation & setup  
  - Cloning the repo
  - Installing dependencies (`pip install -r requirements.txt`)
  - Setting up virtual environment (recommended)
- 3.3 Configuring LLM providers  
  - Setting API keys for Gemini, Claude, or others
  - Using environment variables or config files
- 3.4 Running the first example  
  - Using `main.py` with a sample GitHub repo
  - Expected output: Markdown tutorial in `/output/`
- 3.5 Verifying the output

---

## **Chapter 4: Deep Dive ‚Äì Fetching the Codebase**
**Objective**: Understand how the system ingests and filters source code.

- 4.1 `FetchRepo` Node Overview  
  - Role in the pipeline
  - Inputs: GitHub URL or local path
- 4.2 GitHub vs. Local Mode  
  - `crawl_github_files.py`: cloning, filtering, size limits
  - `crawl_local_files.py`: recursive traversal, `.gitignore` support
- 4.3 File filtering logic  
  - Glob patterns (e.g., `*.py`, `*.js`)
  - Ignoring test files, assets, large binaries
  - Using `pathspec` for `.gitignore`-style rules
- 4.4 Size & memory constraints  
  - Why limit file size?
  - How it prevents OOM errors
- 4.5 Output: List of `(path, content)` tuples  
  - Structure and usage in downstream nodes
- 4.6 Hands-on: Customize filtering rules
 
---

=== [2025-10-03T13:57:51+05:30] MODEL: llama3.2 ===
PROMPT:
You are writing a developer tutorial in english language. Use the chapter plan below plus the file summaries and architectural abstractions.

Chapter plan:
## **Chapter 1: Introduction & Vision**
**Objective**: Set the stage for the project and its value proposition.


File summaries:
### File: Dockerfile
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\Dockerfile

# Dockerfile - File Summary

**one_line**:  
A minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.

**Purpose of the file**:  
This Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.

**Major functions/classes**:  
- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).
- However, the key *build stages* are:
  - Base image setup (`python:3.10-slim`)
  - System package installation (`git`)
  - Python dependency installation via `pip`
  - Application code copy
  - Entrypoint configuration to run `main.py`

**Key technical details & TODOs**:  
- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.
- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.
- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.
- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).
- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.
- **TODOs / Recommendations**:
  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.
  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).
  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.
  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.

**Short usage example if applicable**:  
```bash
# Build the image
docker build -t my-python-app .

# Run the container (assumes main.py accepts args or runs standalone)
docker run --rm my-python-app

# Example with environment variables
docker run --rm -e ENV=prod my-python-app
```
---


### File: README.md
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\README.md

# README.md - File Summary

## one_line
A tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.

## Purpose of the file
The `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.

## Major functions/classes
- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.
- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.
- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.
- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).
- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.
- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).

## Key technical details & TODOs
- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.
- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.
- **Language support**: Tutorials can be generated in any language (`--language` flag).
- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).
- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.
- **Docker support**: Fully containerized with volume mounts for output and code input.
- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.
- **TODOs / Improvements (implied)**:
  - Add support for more visualization types (e.g., Mermaid diagrams in output).
  - Improve handling of large or complex codebases (e.g., better abstraction summarization).
  - Enhance caching strategy (e.g., persistent cache across runs).
  - Support multi-repo or monorepo analysis.

## Short usage example
```bash
# Analyze a GitHub repo and generate an English tutorial
python main.py --repo https://github.com/encode/fastapi --include "*.py" --exclude "tests/*" --max-size 100000

# Analyze a local directory and generate a Chinese tutorial
python main.py --dir ./my-project --include "*.js" "*.ts" --language "Chinese"

# Run in Docker (requires GEMINI_API_KEY)
docker run -it --rm \
  -e GEMINI_API_KEY="your_key" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/pallets/flask
```

> ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.
---


### File: flow.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\flow.py

# flow.py - File Summary

**one_line**:  
Orchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.

**Purpose**:  
This file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  The main function that:
  - Instantiates all node classes (processing units) for each stage of tutorial generation.
  - Chains them together in a directed sequence using the `>>` operator (PocketFlow's flow syntax).
  - Returns a `Flow` object starting at `FetchRepo`.

  **Key Nodes Used** (imported from `nodes.py`):
  - `FetchRepo`: Retrieves/clones the target repository.
  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.
  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.
  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).
  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.

**Key technical details & TODOs**:  
- **PocketFlow Framework**: Uses `Flow` and node chaining (`>>`) to define a directed acyclic graph (DAG) of tasks.
- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).
- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.
- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.
- **TODO (implied)**:  
  - Error handling or fallback logic is not visible here (likely handled in nodes).  
  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  
  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation pipeline
flow = create_tutorial_flow()
flow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine
```  

> **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file.
---


### File: main.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\main.py

# main.py - File Summary

**one_line**:  
CLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.

**Purpose of the file**:  
This script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.

**Major functions/classes**:  
- `main()`:  
  - Sets up argument parsing using `argparse`.
  - Validates and prepares inputs (source, patterns, token, language, etc.).
  - Initializes the `shared` state dictionary passed through the flow.
  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.
- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).

**Key technical details & TODOs**:  
- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.
- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).
- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.
- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.
- **Language support**: `--language` parameter allows multilingual output (default: English).
- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).
- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.
- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.
- **TODOs (implied)**:  
  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).
  - No validation of language codes (assumes valid input).
  - No progress tracking or verbose mode (could benefit from `--verbose` flag).

**Short usage example**:  
```bash
# Generate tutorial from GitHub repo in French, with custom patterns
python main.py \
  --repo https://github.com/user/repo \
  --token ghp_... \
  --output ./tutorials/repo \
  --include "*.py" "*.md" \
  --exclude "tests/*" \
  --language french \
  --max-abstractions 15 \
  --no-cache

# Generate tutorial from local directory
python main.py \
  --dir ./my_project \
  --name "My Project" \
  --output ./docs/tutorial \
  --language spanish
```
---


### File: nodes.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\nodes.py

# nodes.py - File Summary

### one_line  
A collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.

---

### Purpose  
This file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.

---

### Major Functions/Classes

| Class | Role |
|------|------|
| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |
| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |
| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |
| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |
| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |

---

### Key Technical Details & TODOs

- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.
- **YAML Parsing & Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.
- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).
- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.
- **File Handling**: 
  - Converts fetched files to list of `(path, content)` tuples.
  - Uses **index-based referencing** in LLM prompts to avoid path duplication.
- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.
- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.

#### ‚úÖ TODOs / Improvements (inferred):
- [ ] Add **token budgeting** for large codebases in LLM context creation.
- [ ] Support **chunking or summarization** of very large files before LLM ingestion.
- [ ] Allow **custom prompt templates** per language or project type.
- [ ] Add **retry logic with exponential backoff** for LLM calls.
- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.
- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).

---

### Short Usage Example

```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

# Shared context
shared = {
    "repo_url": "https://github.com/user/example-project",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "*.md"],
    "max_file_size": 50000,
    "language": "english",
    "use_cache": True,
    "max_abstraction_num": 8
}

# Build flow
flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

# Run
flow.run(shared)

# Access results
print("Project Summary:", shared["relationships"]["summary"])
print("Abstractions:", shared["abstractions"])
print("Relationships:", shared["relationships"]["details"])
print("Chapter Order:", shared["chapters"])  # List of abstraction indices in order
```

> üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.
---


### File: __init__.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\__init__.py

# __init__.py - File Summary

**one_line**:  
Utility module initializer that imports and exposes key functions for shared use across the project.

**Purpose**:  
This `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).

**Major functions/classes**:  
- Exposes the following (imported from submodules):
  - `log` ‚Äì Standardized logging function with timestamps and levels.
  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.
  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.
  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).
  - `Timer` ‚Äì Context manager for measuring execution time.

**Key technical details & TODOs**:  
- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.
- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.
- All exposed utilities are stateless and thread-safe.
- **TODO**: Add type hints to all public functions in submodules.
- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.

**Short usage example**:  
```python
from utils import log, retry, read_json, Timer

@retry(max_attempts=3)
def fetch_data():
    log("Fetching data...")
    with Timer("Data load"):
        return read_json("data.json")

data = fetch_data()
```
---


### File: call_llm.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\call_llm.py

# call_llm.py - File Summary

**one_line**:  
Unified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).

**Purpose**:  
Provides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`  
  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.
- **Commented alternative implementations** for:
  - Azure OpenAI
  - Anthropic Claude 3.7 Sonnet (with extended thinking)
  - OpenAI o1
  - OpenRouter (supports any model via OpenRouter API)
- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.
- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.

**Key technical details & TODOs**:
- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.
- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.
- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.
- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.
- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `"your-project-id"`) ‚Äî **TODO: Remove or warn about insecure defaults**.
- üß™ **Testing**: Includes a simple `__main__` block to test the function.
- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.
- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.
- üîß **Configurable via env vars**:
  - `LOG_DIR`: Log directory (default: `logs`)
  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`
  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`
  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.
- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.
- üßπ **TODO**: Add error handling and retry logic for API failures.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Set environment variables first (e.g., GEMINI_API_KEY)
response = call_llm("Explain quantum computing in simple terms")
print(response)

# Disable cache for fresh call
response_fresh = call_llm("Hello", use_cache=False)
```

> üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.
---


### File: crawl_github_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_github_files.py

# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.

---

### Purpose  
This script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).

---

### Major Functions/Classes

#### `crawl_github_files(...)`
Main function that orchestrates crawling:
- Parses GitHub URL (including commit/branch and subdirectory).
- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.
- Downloads files recursively from a specified subdirectory.
- Applies **include/exclude glob patterns** to filter files.
- Enforces **maximum file size** to avoid memory issues.
- Returns a dictionary with file contents and crawl statistics.

#### `should_include_file(file_path, file_name)`
Helper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.

#### `fetch_contents(path)` (nested)
Recursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.

#### `fetch_branches()` and `check_tree()`
Used to validate and extract commit/branch reference from the URL when not explicitly provided.

---

### Key Technical Details & TODOs

#### ‚úÖ **Features**
- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).
- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.
- **Two URL modes**:
  - HTTPS: Uses GitHub API (supports branch/commit + path).
  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).
- **Flexible filtering**:
  - `include_patterns`: e.g., `{"*.py", "*.md"}`
  - `exclude_patterns`: e.g., `{"*test*", "*.log"}`
- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.
- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.

#### ‚ö†Ô∏è **Technical Notes**
- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.
- **File size is checked twice**: once from metadata, once from `download_url` headers.
- **Base64 decoding** used when `download_url` is missing (fallback).
- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).
- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).

#### üîß **TODOs / Improvements**
- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).
- [ ] **Add progress bar or logging** for large repositories.
- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.
- [ ] **Caching mechanism** to avoid re-downloading unchanged files.
- [ ] **Better handling of symlinks and binary files** (currently skipped silently).
- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.
- [ ] **Add retry logic** for transient network failures.

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Optional: Set token for private repos or to avoid rate limits
os.environ["GITHUB_TOKEN"] = "your_token_here"

result = crawl_github_files(
    repo_url="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core",
    token=None,  # uses GITHUB_TOKEN env
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"*test*"}
)

print(f"Downloaded {result['stats']['downloaded_count']} files")
for path, content in result["files"].items():
    print(f"{path}: {len(content)} chars")
```

> üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.
---


### File: crawl_local_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_local_files.py

# crawl_local_files.py - File Summary

### one_line
Crawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.

---

### Purpose
This utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.

---

### Major functions/classes

- **`crawl_local_files()`**  
  Main function that:
  - Walks a directory tree (`os.walk`)
  - Applies `.gitignore` rules (if present)
  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`
  - Enforces a maximum file size
  - Reads and returns file contents as a dictionary
  - Shows real-time progress with colored output

---

### Key technical details & TODOs

#### ‚úÖ **Features**
- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).
- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).
- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).
- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\033[92m`).
- **Relative Paths**: Optionally returns paths relative to input directory.
- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.
- **Error Handling**: Gracefully skips unreadable files and logs warnings.

#### ‚ö†Ô∏è **Limitations / TODOs**
- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).
- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.
- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.
- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.
- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.

> üîß **Suggested Improvements (TODOs):**
> - Add `verbose`/`quiet` mode flag to control output.
> - Add option to detect and skip binary files.
> - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).
> - Return metadata (size, mtime) alongside content.
> - Allow custom file read handlers (e.g., for line filtering).

---

### Short usage example

```python
# Crawl current directory, include only Python files, exclude test and cache dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"tests/*", "__pycache__/*", "*.log"},
    max_file_size=1024 * 1024,  # 1 MB limit
    use_relative_paths=True
)

# Print file paths
for path in result["files"]:
    print(path)

# Use file content
for path, content in result["files"].items():
    print(f"--- {path} ---\n{content[:200]}...\n")
```

> üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).
---


Architectural abstractions:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.

---

## 1. **TutorialFlow**
**Description**:  
Orchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.

**Files**:  
- `flow.py`, `main.py`, `nodes.py`

**Dependencies**:  
- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`

**Responsibilities**:  
- Defines and executes the sequential workflow (DAG) for tutorial generation.
- Manages data flow between nodes via the `shared` context.
- Enables retry logic and batch processing for robustness and performance.

---

## 2. **FetchRepo**
**Description**:  
Retrieves source code from a GitHub repository or local directory with filtering and size constraints.

**Files**:  
- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`

**Dependencies**:  
- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`

**Responsibilities**:  
- Parses repo URL or local path input.
- Downloads and filters files using glob patterns and `.gitignore`.
- Enforces maximum file size to prevent memory overflow.
- Returns structured list of `(path, content)` tuples for downstream processing.

---

## 3. **IdentifyAbstractions**
**Description**:  
Uses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.

**Files**:  
- `nodes.py` (class: `IdentifyAbstractions`)

**Dependencies**:  
- `call_llm`, `utils.read_json`, `yaml`, `shared` context

**Responsibilities**:  
- Constructs prompts with code context (by file index) for LLM.
- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.
- Validates output structure and enforces limits (e.g., max 10 abstractions).
- Stores results in `shared["abstractions"]`.

---

## 4. **AnalyzeRelationships**
**Description**:  
Analyzes how identified abstractions interact and generates a project summary and relationship map.

**Files**:  
- `nodes.py` (class: `AnalyzeRelationships`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Builds LLM prompt with abstractions and relevant file snippets.
- Extracts project summary and pairwise relationships (e.g., "X depends on Y").
- Outputs structured YAML into `shared["relationships"]` for tutorial structuring.

---

## 5. **OrderChapters**
**Description**:  
Determines a pedagogical order for tutorial chapters based on abstraction relationships.

**Files**:  
- `nodes.py` (class: `OrderChapters`)

**Dependencies**:  
- `call_llm`, `shared` context (abstractions, relationships)

**Responsibilities**:  
- Uses LLM to reason about dependencies and learning sequence.
- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).
- Stores result in `shared["chapters"]`.

---

## 6. **WriteChapters**
**Description**:  
Generates written content for each chapter (abstraction) using LLM, in a target language.

**Files**:  
- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Processes multiple abstractions in parallel (batch mode).
- For each chapter, generates beginner-friendly explanations with code examples.
- Supports multilingual output via language parameter.
- Stores chapter content in `shared["chapter_contents"]`.

---

## 7. **CombineTutorial**
**Description**:  
Merges all generated chapters into a single, structured tutorial document.

**Files**:  
- `nodes.py` (class: `CombineTutorial`)

**Dependencies**:  
- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)

**Responsibilities**:  
- Combines chapter content in the ordered sequence.
- Adds metadata (title, language, summary).
- Outputs final tutorial (Markdown/HTML) to specified output directory.

---

## 8. **CallLLM**
**Description**:  
Unified interface to call various LLMs (Gemini, Claude

Write a clear and concise tutorial chapter.


RESPONSE:
# **Chapter 1: Introduction & Vision**

> *"Every great developer was once a beginner."*  
> ‚Äî And every great codebase should be **accessible** to them.

Welcome to **PocketFlow Tutorial Generator** ‚Äî a revolutionary open-source tool that transforms complex software projects into **beginner-friendly, AI-powered tutorials**, automatically. Whether you're onboarding new team members, learning a new framework, or documenting your own work, this project turns code into clear, visual, and structured educational content ‚Äî **with just one command**.

---

## üéØ The Vision: Code That Teaches Itself

Modern software projects are powerful but often **inaccessible** to newcomers. Documentation is scattered, architecture is implicit, and the learning curve is steep. What if codebases could **explain themselves**?

Our vision is simple:  
> **Turn any repository into a self-documenting, educational experience ‚Äî powered by AI.**

With **PocketFlow Tutorial Generator**, you don‚Äôt need to write documentation from scratch. Instead, the system:
- **Understands** your codebase using AI.
- **Identifies** core abstractions (classes, functions, design patterns).
- **Maps** how components interact.
- **Orders** concepts for optimal learning.
- **Writes** a structured, multilingual tutorial ‚Äî complete with explanations, examples, and visual flow.

It‚Äôs like having an expert developer sit beside you and say:  
> *‚ÄúHere‚Äôs what matters ‚Äî and here‚Äôs how it all fits together.‚Äù*

---

## üîß What This Tool Does (And Why It Matters)

At its core, **PocketFlow Tutorial Generator** is a **modular AI agent pipeline** that analyzes code and generates tutorials. Here‚Äôs what it enables:

| Feature | Benefit |
|-------|--------|
| ‚úÖ **GitHub + Local Repo Support** | Works on any public/private repo or local directory. |
| ‚úÖ **AI-Powered Abstraction Detection** | Uses LLMs (Gemini, Claude, OpenAI) to find the *essence* of the code. |
| ‚úÖ **Automatic Relationship Mapping** | Shows dependencies, data flow, and component interactions. |
| ‚úÖ **Pedagogical Chapter Ordering** | Teaches concepts in the *right* sequence ‚Äî not just file order. |
| ‚úÖ **Multilingual Output** | Generate tutorials in **English, Spanish, Chinese, French**, and more. |
| ‚úÖ **Filtering & Scoping** | Focus on relevant files (`*.py`, `*.ts`) and skip tests, logs, or binaries. |
| ‚úÖ **Docker & CLI Ready** | Run locally, in CI, or containerized ‚Äî no setup headaches. |
| ‚úÖ **Caching & Efficiency** | Avoids redundant AI calls; saves time and cost. |

Whether you‚Äôre:
- A **learner** trying to understand a new framework (e.g., FastAPI, Flask),
- A **maintainer** onboarding new contributors,
- Or a **teacher** building course material,

‚Ä¶this tool **lowers the barrier to entry** for everyone.

---

## üß† How It Works: A High-Level Architecture

The system is built around **seven core architectural components**, orchestrated by the **PocketFlow** framework ‚Äî a lightweight, agentic workflow engine.

Here‚Äôs the end-to-end flow:

```mermaid
graph TD
    A[Input: GitHub URL or Local Dir] --> B(FetchRepo)
    B --> C[IdentifyAbstractions]
    C --> D[AnalyzeRelationships]
    D --> E[OrderChapters]
    E --> F[WriteChapters]
    F --> G[CombineTutorial]
    G --> H[Tutorial Output: Markdown/HTML]
```

### üîç Key Components at a Glance

| Component | Role |
|--------|------|
| **`FetchRepo`** | Downloads and filters files (with `.gitignore`, size limits, patterns). |
| **`IdentifyAbstractions`** | Asks: *"What are the key concepts in this code?"* ‚Äî returns classes, patterns, functions. |
| **`AnalyzeRelationships`** | Asks: *"How do these concepts interact?"* ‚Äî builds a dependency map. |
| **`OrderChapters`** | Asks: *"What should a beginner learn first?"* ‚Äî creates a logical sequence. |
| **`WriteChapters`** | Generates **per-chapter explanations** with code snippets and analogies. |
| **`CombineTutorial`** | Merges chapters into a single, polished document (Markdown/HTML). |
| **`CallLLM`** | Unified interface to
---

=== [2025-10-03T13:58:05+05:30] MODEL: llama3.2 ===
PROMPT:
You are writing a developer tutorial in english language. Use the chapter plan below plus the file summaries and architectural abstractions.

Chapter plan:
## **Chapter 2: System Overview & High-Level Architecture**
**Objective**: Introduce the end-to-end flow and how components interact.


File summaries:
### File: Dockerfile
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\Dockerfile

# Dockerfile - File Summary

**one_line**:  
A minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.

**Purpose of the file**:  
This Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.

**Major functions/classes**:  
- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).
- However, the key *build stages* are:
  - Base image setup (`python:3.10-slim`)
  - System package installation (`git`)
  - Python dependency installation via `pip`
  - Application code copy
  - Entrypoint configuration to run `main.py`

**Key technical details & TODOs**:  
- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.
- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.
- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.
- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).
- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.
- **TODOs / Recommendations**:
  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.
  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).
  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.
  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.

**Short usage example if applicable**:  
```bash
# Build the image
docker build -t my-python-app .

# Run the container (assumes main.py accepts args or runs standalone)
docker run --rm my-python-app

# Example with environment variables
docker run --rm -e ENV=prod my-python-app
```
---


### File: README.md
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\README.md

# README.md - File Summary

## one_line
A tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.

## Purpose of the file
The `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.

## Major functions/classes
- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.
- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.
- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.
- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).
- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.
- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).

## Key technical details & TODOs
- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.
- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.
- **Language support**: Tutorials can be generated in any language (`--language` flag).
- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).
- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.
- **Docker support**: Fully containerized with volume mounts for output and code input.
- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.
- **TODOs / Improvements (implied)**:
  - Add support for more visualization types (e.g., Mermaid diagrams in output).
  - Improve handling of large or complex codebases (e.g., better abstraction summarization).
  - Enhance caching strategy (e.g., persistent cache across runs).
  - Support multi-repo or monorepo analysis.

## Short usage example
```bash
# Analyze a GitHub repo and generate an English tutorial
python main.py --repo https://github.com/encode/fastapi --include "*.py" --exclude "tests/*" --max-size 100000

# Analyze a local directory and generate a Chinese tutorial
python main.py --dir ./my-project --include "*.js" "*.ts" --language "Chinese"

# Run in Docker (requires GEMINI_API_KEY)
docker run -it --rm \
  -e GEMINI_API_KEY="your_key" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/pallets/flask
```

> ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.
---


### File: flow.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\flow.py

# flow.py - File Summary

**one_line**:  
Orchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.

**Purpose**:  
This file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  The main function that:
  - Instantiates all node classes (processing units) for each stage of tutorial generation.
  - Chains them together in a directed sequence using the `>>` operator (PocketFlow's flow syntax).
  - Returns a `Flow` object starting at `FetchRepo`.

  **Key Nodes Used** (imported from `nodes.py`):
  - `FetchRepo`: Retrieves/clones the target repository.
  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.
  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.
  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).
  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.

**Key technical details & TODOs**:  
- **PocketFlow Framework**: Uses `Flow` and node chaining (`>>`) to define a directed acyclic graph (DAG) of tasks.
- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).
- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.
- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.
- **TODO (implied)**:  
  - Error handling or fallback logic is not visible here (likely handled in nodes).  
  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  
  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation pipeline
flow = create_tutorial_flow()
flow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine
```  

> **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file.
---


### File: main.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\main.py

# main.py - File Summary

**one_line**:  
CLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.

**Purpose of the file**:  
This script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.

**Major functions/classes**:  
- `main()`:  
  - Sets up argument parsing using `argparse`.
  - Validates and prepares inputs (source, patterns, token, language, etc.).
  - Initializes the `shared` state dictionary passed through the flow.
  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.
- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).

**Key technical details & TODOs**:  
- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.
- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).
- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.
- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.
- **Language support**: `--language` parameter allows multilingual output (default: English).
- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).
- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.
- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.
- **TODOs (implied)**:  
  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).
  - No validation of language codes (assumes valid input).
  - No progress tracking or verbose mode (could benefit from `--verbose` flag).

**Short usage example**:  
```bash
# Generate tutorial from GitHub repo in French, with custom patterns
python main.py \
  --repo https://github.com/user/repo \
  --token ghp_... \
  --output ./tutorials/repo \
  --include "*.py" "*.md" \
  --exclude "tests/*" \
  --language french \
  --max-abstractions 15 \
  --no-cache

# Generate tutorial from local directory
python main.py \
  --dir ./my_project \
  --name "My Project" \
  --output ./docs/tutorial \
  --language spanish
```
---


### File: nodes.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\nodes.py

# nodes.py - File Summary

### one_line  
A collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.

---

### Purpose  
This file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.

---

### Major Functions/Classes

| Class | Role |
|------|------|
| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |
| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |
| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |
| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |
| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |

---

### Key Technical Details & TODOs

- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.
- **YAML Parsing & Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.
- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).
- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.
- **File Handling**: 
  - Converts fetched files to list of `(path, content)` tuples.
  - Uses **index-based referencing** in LLM prompts to avoid path duplication.
- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.
- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.

#### ‚úÖ TODOs / Improvements (inferred):
- [ ] Add **token budgeting** for large codebases in LLM context creation.
- [ ] Support **chunking or summarization** of very large files before LLM ingestion.
- [ ] Allow **custom prompt templates** per language or project type.
- [ ] Add **retry logic with exponential backoff** for LLM calls.
- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.
- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).

---

### Short Usage Example

```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

# Shared context
shared = {
    "repo_url": "https://github.com/user/example-project",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "*.md"],
    "max_file_size": 50000,
    "language": "english",
    "use_cache": True,
    "max_abstraction_num": 8
}

# Build flow
flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

# Run
flow.run(shared)

# Access results
print("Project Summary:", shared["relationships"]["summary"])
print("Abstractions:", shared["abstractions"])
print("Relationships:", shared["relationships"]["details"])
print("Chapter Order:", shared["chapters"])  # List of abstraction indices in order
```

> üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.
---


### File: __init__.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\__init__.py

# __init__.py - File Summary

**one_line**:  
Utility module initializer that imports and exposes key functions for shared use across the project.

**Purpose**:  
This `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).

**Major functions/classes**:  
- Exposes the following (imported from submodules):
  - `log` ‚Äì Standardized logging function with timestamps and levels.
  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.
  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.
  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).
  - `Timer` ‚Äì Context manager for measuring execution time.

**Key technical details & TODOs**:  
- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.
- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.
- All exposed utilities are stateless and thread-safe.
- **TODO**: Add type hints to all public functions in submodules.
- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.

**Short usage example**:  
```python
from utils import log, retry, read_json, Timer

@retry(max_attempts=3)
def fetch_data():
    log("Fetching data...")
    with Timer("Data load"):
        return read_json("data.json")

data = fetch_data()
```
---


### File: call_llm.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\call_llm.py

# call_llm.py - File Summary

**one_line**:  
Unified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).

**Purpose**:  
Provides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`  
  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.
- **Commented alternative implementations** for:
  - Azure OpenAI
  - Anthropic Claude 3.7 Sonnet (with extended thinking)
  - OpenAI o1
  - OpenRouter (supports any model via OpenRouter API)
- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.
- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.

**Key technical details & TODOs**:
- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.
- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.
- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.
- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.
- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `"your-project-id"`) ‚Äî **TODO: Remove or warn about insecure defaults**.
- üß™ **Testing**: Includes a simple `__main__` block to test the function.
- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.
- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.
- üîß **Configurable via env vars**:
  - `LOG_DIR`: Log directory (default: `logs`)
  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`
  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`
  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.
- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.
- üßπ **TODO**: Add error handling and retry logic for API failures.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Set environment variables first (e.g., GEMINI_API_KEY)
response = call_llm("Explain quantum computing in simple terms")
print(response)

# Disable cache for fresh call
response_fresh = call_llm("Hello", use_cache=False)
```

> üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.
---


### File: crawl_github_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_github_files.py

# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.

---

### Purpose  
This script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).

---

### Major Functions/Classes

#### `crawl_github_files(...)`
Main function that orchestrates crawling:
- Parses GitHub URL (including commit/branch and subdirectory).
- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.
- Downloads files recursively from a specified subdirectory.
- Applies **include/exclude glob patterns** to filter files.
- Enforces **maximum file size** to avoid memory issues.
- Returns a dictionary with file contents and crawl statistics.

#### `should_include_file(file_path, file_name)`
Helper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.

#### `fetch_contents(path)` (nested)
Recursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.

#### `fetch_branches()` and `check_tree()`
Used to validate and extract commit/branch reference from the URL when not explicitly provided.

---

### Key Technical Details & TODOs

#### ‚úÖ **Features**
- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).
- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.
- **Two URL modes**:
  - HTTPS: Uses GitHub API (supports branch/commit + path).
  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).
- **Flexible filtering**:
  - `include_patterns`: e.g., `{"*.py", "*.md"}`
  - `exclude_patterns`: e.g., `{"*test*", "*.log"}`
- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.
- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.

#### ‚ö†Ô∏è **Technical Notes**
- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.
- **File size is checked twice**: once from metadata, once from `download_url` headers.
- **Base64 decoding** used when `download_url` is missing (fallback).
- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).
- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).

#### üîß **TODOs / Improvements**
- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).
- [ ] **Add progress bar or logging** for large repositories.
- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.
- [ ] **Caching mechanism** to avoid re-downloading unchanged files.
- [ ] **Better handling of symlinks and binary files** (currently skipped silently).
- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.
- [ ] **Add retry logic** for transient network failures.

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Optional: Set token for private repos or to avoid rate limits
os.environ["GITHUB_TOKEN"] = "your_token_here"

result = crawl_github_files(
    repo_url="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core",
    token=None,  # uses GITHUB_TOKEN env
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"*test*"}
)

print(f"Downloaded {result['stats']['downloaded_count']} files")
for path, content in result["files"].items():
    print(f"{path}: {len(content)} chars")
```

> üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.
---


### File: crawl_local_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_local_files.py

# crawl_local_files.py - File Summary

### one_line
Crawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.

---

### Purpose
This utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.

---

### Major functions/classes

- **`crawl_local_files()`**  
  Main function that:
  - Walks a directory tree (`os.walk`)
  - Applies `.gitignore` rules (if present)
  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`
  - Enforces a maximum file size
  - Reads and returns file contents as a dictionary
  - Shows real-time progress with colored output

---

### Key technical details & TODOs

#### ‚úÖ **Features**
- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).
- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).
- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).
- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\033[92m`).
- **Relative Paths**: Optionally returns paths relative to input directory.
- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.
- **Error Handling**: Gracefully skips unreadable files and logs warnings.

#### ‚ö†Ô∏è **Limitations / TODOs**
- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).
- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.
- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.
- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.
- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.

> üîß **Suggested Improvements (TODOs):**
> - Add `verbose`/`quiet` mode flag to control output.
> - Add option to detect and skip binary files.
> - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).
> - Return metadata (size, mtime) alongside content.
> - Allow custom file read handlers (e.g., for line filtering).

---

### Short usage example

```python
# Crawl current directory, include only Python files, exclude test and cache dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"tests/*", "__pycache__/*", "*.log"},
    max_file_size=1024 * 1024,  # 1 MB limit
    use_relative_paths=True
)

# Print file paths
for path in result["files"]:
    print(path)

# Use file content
for path, content in result["files"].items():
    print(f"--- {path} ---\n{content[:200]}...\n")
```

> üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).
---


Architectural abstractions:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.

---

## 1. **TutorialFlow**
**Description**:  
Orchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.

**Files**:  
- `flow.py`, `main.py`, `nodes.py`

**Dependencies**:  
- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`

**Responsibilities**:  
- Defines and executes the sequential workflow (DAG) for tutorial generation.
- Manages data flow between nodes via the `shared` context.
- Enables retry logic and batch processing for robustness and performance.

---

## 2. **FetchRepo**
**Description**:  
Retrieves source code from a GitHub repository or local directory with filtering and size constraints.

**Files**:  
- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`

**Dependencies**:  
- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`

**Responsibilities**:  
- Parses repo URL or local path input.
- Downloads and filters files using glob patterns and `.gitignore`.
- Enforces maximum file size to prevent memory overflow.
- Returns structured list of `(path, content)` tuples for downstream processing.

---

## 3. **IdentifyAbstractions**
**Description**:  
Uses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.

**Files**:  
- `nodes.py` (class: `IdentifyAbstractions`)

**Dependencies**:  
- `call_llm`, `utils.read_json`, `yaml`, `shared` context

**Responsibilities**:  
- Constructs prompts with code context (by file index) for LLM.
- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.
- Validates output structure and enforces limits (e.g., max 10 abstractions).
- Stores results in `shared["abstractions"]`.

---

## 4. **AnalyzeRelationships**
**Description**:  
Analyzes how identified abstractions interact and generates a project summary and relationship map.

**Files**:  
- `nodes.py` (class: `AnalyzeRelationships`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Builds LLM prompt with abstractions and relevant file snippets.
- Extracts project summary and pairwise relationships (e.g., "X depends on Y").
- Outputs structured YAML into `shared["relationships"]` for tutorial structuring.

---

## 5. **OrderChapters**
**Description**:  
Determines a pedagogical order for tutorial chapters based on abstraction relationships.

**Files**:  
- `nodes.py` (class: `OrderChapters`)

**Dependencies**:  
- `call_llm`, `shared` context (abstractions, relationships)

**Responsibilities**:  
- Uses LLM to reason about dependencies and learning sequence.
- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).
- Stores result in `shared["chapters"]`.

---

## 6. **WriteChapters**
**Description**:  
Generates written content for each chapter (abstraction) using LLM, in a target language.

**Files**:  
- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Processes multiple abstractions in parallel (batch mode).
- For each chapter, generates beginner-friendly explanations with code examples.
- Supports multilingual output via language parameter.
- Stores chapter content in `shared["chapter_contents"]`.

---

## 7. **CombineTutorial**
**Description**:  
Merges all generated chapters into a single, structured tutorial document.

**Files**:  
- `nodes.py` (class: `CombineTutorial`)

**Dependencies**:  
- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)

**Responsibilities**:  
- Combines chapter content in the ordered sequence.
- Adds metadata (title, language, summary).
- Outputs final tutorial (Markdown/HTML) to specified output directory.

---

## 8. **CallLLM**
**Description**:  
Unified interface to call various LLMs (Gemini, Claude

Write a clear and concise tutorial chapter.


RESPONSE:
# **Chapter 2: System Overview & High-Level Architecture**

## **Objective**
In this chapter, you‚Äôll gain a comprehensive understanding of how the **AI-Powered Codebase Tutorial Generator** works end-to-end. We‚Äôll walk through the high-level architecture, explain the core components, and show how they interact to transform any codebase ‚Äî whether from GitHub or your local machine ‚Äî into a beginner-friendly, multilingual tutorial with visualizable structure.

By the end of this chapter, you‚Äôll understand:
- The **overall data flow** from code input to final tutorial output.
- The **modular design** powered by *PocketFlow* agents.
- The **key architectural abstractions** and their responsibilities.
- How **LLMs are used** to reason about code, not just generate text.
- How to **extend or customize** the system for your own use cases.

---

## **2.1 End-to-End Flow: From Code to Tutorial**

Let‚Äôs start with the big picture. Here‚Äôs what happens when you run:

```bash
python main.py --repo https://github.com/encode/fastapi --language english
```

### üîÅ **Step-by-Step Pipeline**

| Step | Component | What It Does |
|------|---------|-------------|
| 1Ô∏è‚É£ | `FetchRepo` | Downloads or reads the codebase using pattern filters and size limits. |
| 2Ô∏è‚É£ | `IdentifyAbstractions` | Uses an LLM to detect core concepts (e.g., `FastAPI`, `APIRouter`, `Dependency`) in the code. |
| 3Ô∏è‚É£ | `AnalyzeRelationships` | Asks the LLM how these abstractions relate (e.g., "`APIRouter` is used by `FastAPI`"). |
| 4Ô∏è‚É£ | `OrderChapters` | Determines a logical learning order (e.g., start with `FastAPI`, then `APIRouter`). |
| 5Ô∏è‚É£ | `WriteChapters` *(Batch)* | Generates detailed, beginner-friendly explanations for each abstraction ‚Äî in parallel. |
| 6Ô∏è‚É£ | `CombineTutorial` | Merges all chapters into a single structured document (Markdown/HTML), including summary and metadata. |

> üí° **All steps are orchestrated by `TutorialFlow`**, a PocketFlow-based DAG (Directed Acyclic Graph) defined in `flow.py`.

---

## **2.2 High-Level Architecture Diagram**

```
+------------------+
|   User Input     |
| (CLI: repo/dir)  |
+--------+---------+
         |
         v
+------------------+
|  main.py         | <--- Entrypoint: Parses args, sets up shared context
+--------+---------+
         |
         v
+------------------+
|  TutorialFlow    | <--- PocketFlow orchestrator (flow.py)
+--------+---------+
         |
         +------------------> [FetchRepo]
         |                         |
         |                         v
         |                [IdentifyAbstractions] ‚Üí LLM ‚Üí YAML
         |                         |
         |                         v
         |                [AnalyzeRelationships] ‚Üí LLM ‚Üí Summary + Graph
         |                         |
         |                         v
         |                [OrderChapters] ‚Üí LLM ‚Üí Chapter Order
         |                         |
         |                         v
         |                [WriteChapters] ‚Üí LLM (Batch) ‚Üí Per-chapter content
         |                         |
         |                         v
         +----------------> [CombineTutorial] ‚Üí Final Tutorial (MD/HTML)
                                   |
                                   v
                         +------------------+
                         |  ./output/       | ‚Üê Generated tutorial
                         |  (by language)   |
                         +------------------+
```

> üîó **Data flows via a shared dictionary** (`shared`) passed between nodes. This includes:
> - `files`: List of `(path, content)` tuples
> - `abstractions`: Extracted concepts
> - `relationships`: How they interact
> - `chapters`: Ordered list of abstraction indices
> - `chapter_contents`: Generated explanations
> - `project_summary`, `language`, `repo_url`, etc.

---

## **2.3 Core Architectural Abstractions**

Let‚Äôs dive into the **7 key components** that make this system powerful, modular, and extensible.

### **1. `TutorialFlow` ‚Äì The Orchestrator**
- **Defined in**: `flow.py`, `main.py`
- **Framework**: [PocketFlow](https://github.com/The-Pocket/PocketFlow) (lightweight agentic workflow engine)
- **Role**: Chains nodes together using `>>` syntax and manages execution order.
- **Features**:
  -
---

=== [2025-10-03T13:58:19+05:30] MODEL: llama3.2 ===
PROMPT:
You are writing a developer tutorial in english language. Use the chapter plan below plus the file summaries and architectural abstractions.

Chapter plan:
## **Chapter 3: Setting Up Your Environment**
**Objective**: Prepare the reader to run and experiment with the system.


File summaries:
### File: Dockerfile
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\Dockerfile

# Dockerfile - File Summary

**one_line**:  
A minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.

**Purpose of the file**:  
This Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.

**Major functions/classes**:  
- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).
- However, the key *build stages* are:
  - Base image setup (`python:3.10-slim`)
  - System package installation (`git`)
  - Python dependency installation via `pip`
  - Application code copy
  - Entrypoint configuration to run `main.py`

**Key technical details & TODOs**:  
- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.
- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.
- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.
- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).
- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.
- **TODOs / Recommendations**:
  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.
  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).
  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.
  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.

**Short usage example if applicable**:  
```bash
# Build the image
docker build -t my-python-app .

# Run the container (assumes main.py accepts args or runs standalone)
docker run --rm my-python-app

# Example with environment variables
docker run --rm -e ENV=prod my-python-app
```
---


### File: README.md
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\README.md

# README.md - File Summary

## one_line
A tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.

## Purpose of the file
The `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.

## Major functions/classes
- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.
- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.
- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.
- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).
- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.
- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).

## Key technical details & TODOs
- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.
- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.
- **Language support**: Tutorials can be generated in any language (`--language` flag).
- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).
- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.
- **Docker support**: Fully containerized with volume mounts for output and code input.
- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.
- **TODOs / Improvements (implied)**:
  - Add support for more visualization types (e.g., Mermaid diagrams in output).
  - Improve handling of large or complex codebases (e.g., better abstraction summarization).
  - Enhance caching strategy (e.g., persistent cache across runs).
  - Support multi-repo or monorepo analysis.

## Short usage example
```bash
# Analyze a GitHub repo and generate an English tutorial
python main.py --repo https://github.com/encode/fastapi --include "*.py" --exclude "tests/*" --max-size 100000

# Analyze a local directory and generate a Chinese tutorial
python main.py --dir ./my-project --include "*.js" "*.ts" --language "Chinese"

# Run in Docker (requires GEMINI_API_KEY)
docker run -it --rm \
  -e GEMINI_API_KEY="your_key" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/pallets/flask
```

> ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.
---


### File: flow.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\flow.py

# flow.py - File Summary

**one_line**:  
Orchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.

**Purpose**:  
This file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  The main function that:
  - Instantiates all node classes (processing units) for each stage of tutorial generation.
  - Chains them together in a directed sequence using the `>>` operator (PocketFlow's flow syntax).
  - Returns a `Flow` object starting at `FetchRepo`.

  **Key Nodes Used** (imported from `nodes.py`):
  - `FetchRepo`: Retrieves/clones the target repository.
  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.
  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.
  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).
  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.

**Key technical details & TODOs**:  
- **PocketFlow Framework**: Uses `Flow` and node chaining (`>>`) to define a directed acyclic graph (DAG) of tasks.
- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).
- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.
- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.
- **TODO (implied)**:  
  - Error handling or fallback logic is not visible here (likely handled in nodes).  
  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  
  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation pipeline
flow = create_tutorial_flow()
flow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine
```  

> **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file.
---


### File: main.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\main.py

# main.py - File Summary

**one_line**:  
CLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.

**Purpose of the file**:  
This script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.

**Major functions/classes**:  
- `main()`:  
  - Sets up argument parsing using `argparse`.
  - Validates and prepares inputs (source, patterns, token, language, etc.).
  - Initializes the `shared` state dictionary passed through the flow.
  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.
- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).

**Key technical details & TODOs**:  
- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.
- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).
- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.
- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.
- **Language support**: `--language` parameter allows multilingual output (default: English).
- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).
- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.
- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.
- **TODOs (implied)**:  
  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).
  - No validation of language codes (assumes valid input).
  - No progress tracking or verbose mode (could benefit from `--verbose` flag).

**Short usage example**:  
```bash
# Generate tutorial from GitHub repo in French, with custom patterns
python main.py \
  --repo https://github.com/user/repo \
  --token ghp_... \
  --output ./tutorials/repo \
  --include "*.py" "*.md" \
  --exclude "tests/*" \
  --language french \
  --max-abstractions 15 \
  --no-cache

# Generate tutorial from local directory
python main.py \
  --dir ./my_project \
  --name "My Project" \
  --output ./docs/tutorial \
  --language spanish
```
---


### File: nodes.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\nodes.py

# nodes.py - File Summary

### one_line  
A collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.

---

### Purpose  
This file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.

---

### Major Functions/Classes

| Class | Role |
|------|------|
| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |
| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |
| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |
| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |
| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |

---

### Key Technical Details & TODOs

- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.
- **YAML Parsing & Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.
- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).
- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.
- **File Handling**: 
  - Converts fetched files to list of `(path, content)` tuples.
  - Uses **index-based referencing** in LLM prompts to avoid path duplication.
- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.
- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.

#### ‚úÖ TODOs / Improvements (inferred):
- [ ] Add **token budgeting** for large codebases in LLM context creation.
- [ ] Support **chunking or summarization** of very large files before LLM ingestion.
- [ ] Allow **custom prompt templates** per language or project type.
- [ ] Add **retry logic with exponential backoff** for LLM calls.
- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.
- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).

---

### Short Usage Example

```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

# Shared context
shared = {
    "repo_url": "https://github.com/user/example-project",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "*.md"],
    "max_file_size": 50000,
    "language": "english",
    "use_cache": True,
    "max_abstraction_num": 8
}

# Build flow
flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

# Run
flow.run(shared)

# Access results
print("Project Summary:", shared["relationships"]["summary"])
print("Abstractions:", shared["abstractions"])
print("Relationships:", shared["relationships"]["details"])
print("Chapter Order:", shared["chapters"])  # List of abstraction indices in order
```

> üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.
---


### File: __init__.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\__init__.py

# __init__.py - File Summary

**one_line**:  
Utility module initializer that imports and exposes key functions for shared use across the project.

**Purpose**:  
This `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).

**Major functions/classes**:  
- Exposes the following (imported from submodules):
  - `log` ‚Äì Standardized logging function with timestamps and levels.
  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.
  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.
  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).
  - `Timer` ‚Äì Context manager for measuring execution time.

**Key technical details & TODOs**:  
- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.
- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.
- All exposed utilities are stateless and thread-safe.
- **TODO**: Add type hints to all public functions in submodules.
- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.

**Short usage example**:  
```python
from utils import log, retry, read_json, Timer

@retry(max_attempts=3)
def fetch_data():
    log("Fetching data...")
    with Timer("Data load"):
        return read_json("data.json")

data = fetch_data()
```
---


### File: call_llm.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\call_llm.py

# call_llm.py - File Summary

**one_line**:  
Unified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).

**Purpose**:  
Provides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`  
  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.
- **Commented alternative implementations** for:
  - Azure OpenAI
  - Anthropic Claude 3.7 Sonnet (with extended thinking)
  - OpenAI o1
  - OpenRouter (supports any model via OpenRouter API)
- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.
- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.

**Key technical details & TODOs**:
- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.
- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.
- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.
- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.
- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `"your-project-id"`) ‚Äî **TODO: Remove or warn about insecure defaults**.
- üß™ **Testing**: Includes a simple `__main__` block to test the function.
- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.
- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.
- üîß **Configurable via env vars**:
  - `LOG_DIR`: Log directory (default: `logs`)
  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`
  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`
  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.
- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.
- üßπ **TODO**: Add error handling and retry logic for API failures.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Set environment variables first (e.g., GEMINI_API_KEY)
response = call_llm("Explain quantum computing in simple terms")
print(response)

# Disable cache for fresh call
response_fresh = call_llm("Hello", use_cache=False)
```

> üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.
---


### File: crawl_github_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_github_files.py

# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.

---

### Purpose  
This script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).

---

### Major Functions/Classes

#### `crawl_github_files(...)`
Main function that orchestrates crawling:
- Parses GitHub URL (including commit/branch and subdirectory).
- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.
- Downloads files recursively from a specified subdirectory.
- Applies **include/exclude glob patterns** to filter files.
- Enforces **maximum file size** to avoid memory issues.
- Returns a dictionary with file contents and crawl statistics.

#### `should_include_file(file_path, file_name)`
Helper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.

#### `fetch_contents(path)` (nested)
Recursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.

#### `fetch_branches()` and `check_tree()`
Used to validate and extract commit/branch reference from the URL when not explicitly provided.

---

### Key Technical Details & TODOs

#### ‚úÖ **Features**
- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).
- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.
- **Two URL modes**:
  - HTTPS: Uses GitHub API (supports branch/commit + path).
  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).
- **Flexible filtering**:
  - `include_patterns`: e.g., `{"*.py", "*.md"}`
  - `exclude_patterns`: e.g., `{"*test*", "*.log"}`
- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.
- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.

#### ‚ö†Ô∏è **Technical Notes**
- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.
- **File size is checked twice**: once from metadata, once from `download_url` headers.
- **Base64 decoding** used when `download_url` is missing (fallback).
- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).
- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).

#### üîß **TODOs / Improvements**
- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).
- [ ] **Add progress bar or logging** for large repositories.
- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.
- [ ] **Caching mechanism** to avoid re-downloading unchanged files.
- [ ] **Better handling of symlinks and binary files** (currently skipped silently).
- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.
- [ ] **Add retry logic** for transient network failures.

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Optional: Set token for private repos or to avoid rate limits
os.environ["GITHUB_TOKEN"] = "your_token_here"

result = crawl_github_files(
    repo_url="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core",
    token=None,  # uses GITHUB_TOKEN env
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"*test*"}
)

print(f"Downloaded {result['stats']['downloaded_count']} files")
for path, content in result["files"].items():
    print(f"{path}: {len(content)} chars")
```

> üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.
---


### File: crawl_local_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_local_files.py

# crawl_local_files.py - File Summary

### one_line
Crawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.

---

### Purpose
This utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.

---

### Major functions/classes

- **`crawl_local_files()`**  
  Main function that:
  - Walks a directory tree (`os.walk`)
  - Applies `.gitignore` rules (if present)
  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`
  - Enforces a maximum file size
  - Reads and returns file contents as a dictionary
  - Shows real-time progress with colored output

---

### Key technical details & TODOs

#### ‚úÖ **Features**
- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).
- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).
- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).
- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\033[92m`).
- **Relative Paths**: Optionally returns paths relative to input directory.
- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.
- **Error Handling**: Gracefully skips unreadable files and logs warnings.

#### ‚ö†Ô∏è **Limitations / TODOs**
- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).
- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.
- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.
- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.
- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.

> üîß **Suggested Improvements (TODOs):**
> - Add `verbose`/`quiet` mode flag to control output.
> - Add option to detect and skip binary files.
> - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).
> - Return metadata (size, mtime) alongside content.
> - Allow custom file read handlers (e.g., for line filtering).

---

### Short usage example

```python
# Crawl current directory, include only Python files, exclude test and cache dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"tests/*", "__pycache__/*", "*.log"},
    max_file_size=1024 * 1024,  # 1 MB limit
    use_relative_paths=True
)

# Print file paths
for path in result["files"]:
    print(path)

# Use file content
for path, content in result["files"].items():
    print(f"--- {path} ---\n{content[:200]}...\n")
```

> üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).
---


Architectural abstractions:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.

---

## 1. **TutorialFlow**
**Description**:  
Orchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.

**Files**:  
- `flow.py`, `main.py`, `nodes.py`

**Dependencies**:  
- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`

**Responsibilities**:  
- Defines and executes the sequential workflow (DAG) for tutorial generation.
- Manages data flow between nodes via the `shared` context.
- Enables retry logic and batch processing for robustness and performance.

---

## 2. **FetchRepo**
**Description**:  
Retrieves source code from a GitHub repository or local directory with filtering and size constraints.

**Files**:  
- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`

**Dependencies**:  
- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`

**Responsibilities**:  
- Parses repo URL or local path input.
- Downloads and filters files using glob patterns and `.gitignore`.
- Enforces maximum file size to prevent memory overflow.
- Returns structured list of `(path, content)` tuples for downstream processing.

---

## 3. **IdentifyAbstractions**
**Description**:  
Uses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.

**Files**:  
- `nodes.py` (class: `IdentifyAbstractions`)

**Dependencies**:  
- `call_llm`, `utils.read_json`, `yaml`, `shared` context

**Responsibilities**:  
- Constructs prompts with code context (by file index) for LLM.
- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.
- Validates output structure and enforces limits (e.g., max 10 abstractions).
- Stores results in `shared["abstractions"]`.

---

## 4. **AnalyzeRelationships**
**Description**:  
Analyzes how identified abstractions interact and generates a project summary and relationship map.

**Files**:  
- `nodes.py` (class: `AnalyzeRelationships`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Builds LLM prompt with abstractions and relevant file snippets.
- Extracts project summary and pairwise relationships (e.g., "X depends on Y").
- Outputs structured YAML into `shared["relationships"]` for tutorial structuring.

---

## 5. **OrderChapters**
**Description**:  
Determines a pedagogical order for tutorial chapters based on abstraction relationships.

**Files**:  
- `nodes.py` (class: `OrderChapters`)

**Dependencies**:  
- `call_llm`, `shared` context (abstractions, relationships)

**Responsibilities**:  
- Uses LLM to reason about dependencies and learning sequence.
- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).
- Stores result in `shared["chapters"]`.

---

## 6. **WriteChapters**
**Description**:  
Generates written content for each chapter (abstraction) using LLM, in a target language.

**Files**:  
- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Processes multiple abstractions in parallel (batch mode).
- For each chapter, generates beginner-friendly explanations with code examples.
- Supports multilingual output via language parameter.
- Stores chapter content in `shared["chapter_contents"]`.

---

## 7. **CombineTutorial**
**Description**:  
Merges all generated chapters into a single, structured tutorial document.

**Files**:  
- `nodes.py` (class: `CombineTutorial`)

**Dependencies**:  
- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)

**Responsibilities**:  
- Combines chapter content in the ordered sequence.
- Adds metadata (title, language, summary).
- Outputs final tutorial (Markdown/HTML) to specified output directory.

---

## 8. **CallLLM**
**Description**:  
Unified interface to call various LLMs (Gemini, Claude

Write a clear and concise tutorial chapter.


RESPONSE:
# **Chapter 3: Setting Up Your Environment**

**Objective**: Prepare your system to run and experiment with the AI-powered codebase tutorial generator.

By the end of this chapter, you'll have a fully configured development environment capable of analyzing GitHub repositories or local codebases and generating beginner-friendly, multilingual tutorials ‚Äî all powered by Large Language Models (LLMs) and the lightweight **PocketFlow** agent framework.

---

## **1. Prerequisites**

Before you begin, ensure the following tools are installed on your machine:

| Tool | Purpose | Installation |
|------|--------|--------------|
| **Python 3.10+** | Core runtime for the application | [python.org](https://www.python.org/downloads/) |
| **Git** | Clone repositories and manage source control | [git-scm.com](https://git-scm.com/) |
| **Docker** (optional) | Run the app in an isolated container | [docker.com](https://www.docker.com/products/docker-desktop/) |
| **LLM API Key** | Required for Gemini, Claude, or OpenAI | See [API Key Setup](#3-api-key-setup) |

> üí° **Note**: While Docker simplifies setup, you can run everything directly with Python if preferred.

---

## **2. Clone the Project**

Start by cloning the repository:

```bash
git clone https://github.com/your-org/pocketflow-tutorial-generator.git
cd pocketflow-tutorial-generator
```

> üîÑ Replace the URL with the actual repo if different. The structure should include:
> - `main.py` ‚Äì CLI entrypoint
> - `flow.py` ‚Äì Pipeline orchestration
> - `nodes.py` ‚Äì Core logic modules
> - `utils/` ‚Äì Helper tools (`call_llm.py`, `crawl_github_files.py`, etc.)
> - `Dockerfile` ‚Äì Container definition
> - `requirements.txt` ‚Äì Python dependencies

---

## **3. API Key Setup**

The system uses LLMs (like **Google Gemini 2.5 Pro**, **Anthropic Claude 3.7**, or **OpenAI O1**) to analyze code and generate tutorials.

### ‚úÖ **Recommended: Use Environment Variables**

Set your API key securely using environment variables:

#### For **Google Gemini** (default provider):
```bash
export GEMINI_API_KEY="your-gemini-api-key-here"
```

#### For **Anthropic Claude**:
```bash
export ANTHROPIC_API_KEY="your-anthropic-api-key-here"
```

#### For **OpenAI / Azure / OpenRouter**:
```bash
export OPENAI_API_KEY="..."          # For OpenAI
# OR
export OPENROUTER_API_KEY="..."      # For OpenRouter
```

> üîê **Best Practice**: Never commit API keys to version control. Use `.env` files or shell profiles (e.g., `.zshrc`, `.bash_profile`) to store them.

> üõ†Ô∏è **Switching Providers**: Open `utils/call_llm.py` and comment/uncomment the desired `call_llm` implementation. The rest of the code remains unchanged.

---

## **4. Install Dependencies**

### Option A: **Native Python (Recommended for Development)**

Install required packages:

```bash
pip install -r requirements.txt
```

> üì¶ Key dependencies include:
> - `google-generativeai`, `openai`, `anthropic` ‚Äì LLM clients
> - `requests` ‚Äì GitHub API access
> - `PyYAML` ‚Äì Parse LLM responses
> - `pathspec` ‚Äì `.gitignore` support
> - `PocketFlow` ‚Äì Lightweight workflow engine

### Option B: **Docker (Recommended for Reproducibility & Production)**

Build the image using the provided `Dockerfile`:

```bash
docker build -t pocketflow-tutorial-gen .
```

> ‚úÖ The Dockerfile:
> - Uses `python:3.10-slim` for minimal footprint
> - Installs `git` for GitHub cloning
> - Copies code and installs Python packages
> - Sets `main.py` as the entrypoint

> ‚ö†Ô∏è **Security Note**: By default, the container runs as root. For production, consider adding a non-root user:
> ```Dockerfile
> RUN useradd -m -s /bin/bash appuser
> USER appuser
> ```

> üìù **Tip**: Add a `.dockerignore` file to exclude logs, cache, and IDE files:
> ```
> __pycache__
> *.pyc
> .git
> .vscode
> logs/
> llm_cache.json
> ```

---

## **5. Configure Input & Output**

The
---

=== [2025-10-03T13:58:32+05:30] MODEL: llama3.2 ===
PROMPT:
You are writing a developer tutorial in english language. Use the chapter plan below plus the file summaries and architectural abstractions.

Chapter plan:
## **Chapter 4: Deep Dive ‚Äì Fetching the Codebase**
**Objective**: Understand how the system ingests and filters source code.


File summaries:
### File: Dockerfile
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\Dockerfile

# Dockerfile - File Summary

**one_line**:  
A minimal Dockerfile to containerize a Python application using Python 3.10, installing dependencies and running `main.py`.

**Purpose of the file**:  
This Dockerfile defines a lightweight container environment for a Python application. It sets up the OS-level dependencies (like `git`), installs Python packages from `requirements.txt`, and configures the container to run the application's entrypoint script (`main.py`) when launched.

**Major functions/classes**:  
- Not applicable (Dockerfile contains only build-time instructions and no code functions/classes).
- However, the key *build stages* are:
  - Base image setup (`python:3.10-slim`)
  - System package installation (`git`)
  - Python dependency installation via `pip`
  - Application code copy
  - Entrypoint configuration to run `main.py`

**Key technical details & TODOs**:  
- **Base Image**: Uses `python:3.10-slim` for a small footprint and security best practices.
- **System Dependencies**: Installs `git` (likely needed for some `pip install` operations or app functionality), then cleans up apt cache to reduce image size.
- **Dependency Management**: Installs packages from `requirements.txt` with `--no-cache-dir` to minimize layer size.
- **Security Note**: Avoids running as root (though not explicitly configured here; consider adding a non-root user for production).
- **Efficiency**: Uses minimal layers by chaining `apt` commands and cleaning cache in the same `RUN` step.
- **TODOs / Recommendations**:
  - ‚úÖ **Add `.dockerignore`** to prevent unnecessary files (e.g., `__pycache__`, `.git`) from being copied.
  - ‚ö†Ô∏è **Consider multi-stage build** if final image size is critical (e.g., for production).
  - üîê **Run as non-root user** (e.g., `useradd` + `USER`) for improved security.
  - üß™ **Verify `main.py` exists** in the project root to avoid runtime failures.

**Short usage example if applicable**:  
```bash
# Build the image
docker build -t my-python-app .

# Run the container (assumes main.py accepts args or runs standalone)
docker run --rm my-python-app

# Example with environment variables
docker run --rm -e ENV=prod my-python-app
```
---


### File: README.md
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\README.md

# README.md - File Summary

## one_line
A tutorial project that uses AI to automatically analyze GitHub repositories and generate beginner-friendly, well-visualized codebase tutorials.

## Purpose of the file
The `README.md` serves as the main documentation and landing page for a project that transforms any codebase into an accessible, AI-generated tutorial using Pocket Flow‚Äîa lightweight LLM agent framework. It explains how to set up, run, and customize the tool, showcases example outputs from popular open-source projects, and provides development context and usage instructions.

## Major functions/classes
- **Main entry point**: `main.py` ‚Äì Orchestrates the entire flow: repo/dir crawling, code analysis, abstraction detection, and tutorial generation.
- **Codebase crawler**: Downloads GitHub repos or reads local directories, filters files based on include/exclude patterns and size limits.
- **Abstraction analyzer**: Uses LLM agents to detect core concepts, patterns, and interactions in the codebase.
- **Tutorial generator**: Transforms analysis into structured, beginner-friendly tutorials with visualizations (e.g., diagrams, flowcharts).
- **LLM interface**: `utils/call_llm.py` ‚Äì Handles communication with Gemini, Claude, or other LLMs for reasoning and content generation.
- **Pocket Flow integration**: Lightweight framework enabling modular, agent-driven workflow execution (see [PocketFlow](https://github.com/The-Pocket/PocketFlow)).

## Key technical details & TODOs
- **LLM-powered analysis**: Uses modern LLMs (Gemini 2.5 Pro, Claude 3.7 with thinking, O1) to reason about code structure and abstractions.
- **Caching**: Enabled by default to avoid redundant LLM calls; can be disabled via `--no-cache`.
- **Language support**: Tutorials can be generated in any language (`--language` flag).
- **Configurable scope**: Filter files by extension (`--include`), exclude paths (`--exclude`), and limit file size (`--max-size`).
- **GitHub integration**: Supports public/private repos via `--repo` and `--token` or `GITHUB_TOKEN`.
- **Docker support**: Fully containerized with volume mounts for output and code input.
- **Extensible design**: Built using Pocket Flow's agentic coding paradigm‚Äîagents build agents‚Äîfor modular, scalable workflows.
- **TODOs / Improvements (implied)**:
  - Add support for more visualization types (e.g., Mermaid diagrams in output).
  - Improve handling of large or complex codebases (e.g., better abstraction summarization).
  - Enhance caching strategy (e.g., persistent cache across runs).
  - Support multi-repo or monorepo analysis.

## Short usage example
```bash
# Analyze a GitHub repo and generate an English tutorial
python main.py --repo https://github.com/encode/fastapi --include "*.py" --exclude "tests/*" --max-size 100000

# Analyze a local directory and generate a Chinese tutorial
python main.py --dir ./my-project --include "*.js" "*.ts" --language "Chinese"

# Run in Docker (requires GEMINI_API_KEY)
docker run -it --rm \
  -e GEMINI_API_KEY="your_key" \
  -v "$(pwd)/output":/app/output \
  pocketflow-app --repo https://github.com/pallets/flask
```

> ‚úÖ Output: A fully AI-generated tutorial (HTML or Markdown) saved in `./output`, explaining the codebase's architecture, key components, and how everything fits together‚Äîperfect for onboarding or learning.
---


### File: flow.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\flow.py

# flow.py - File Summary

**one_line**:  
Orchestrates a sequential workflow to generate a structured tutorial from a codebase using modular nodes.

**Purpose**:  
This file defines and configures a `Flow` (using the PocketFlow framework) that automates the process of analyzing a code repository and generating a coherent, well-structured tutorial. It connects specialized processing steps‚Äîfrom fetching the repo to combining the final tutorial‚Äîinto a single executable pipeline.

**Major functions/classes**:  
- `create_tutorial_flow()`:  
  The main function that:
  - Instantiates all node classes (processing units) for each stage of tutorial generation.
  - Chains them together in a directed sequence using the `>>` operator (PocketFlow's flow syntax).
  - Returns a `Flow` object starting at `FetchRepo`.

  **Key Nodes Used** (imported from `nodes.py`):
  - `FetchRepo`: Retrieves/clones the target repository.
  - `IdentifyAbstractions`: Discovers key abstractions (e.g., classes, functions, patterns) in the code.
  - `AnalyzeRelationships`: Maps dependencies and interactions between abstractions.
  - `OrderChapters`: Determines a logical narrative order for tutorial chapters.
  - `WriteChapters`: Generates written content for each chapter (*note: this is a `BatchNode`*, implying it processes multiple items in parallel).
  - `CombineTutorial`: Merges individual chapters into a single cohesive tutorial document.

**Key technical details & TODOs**:  
- **PocketFlow Framework**: Uses `Flow` and node chaining (`>>`) to define a directed acyclic graph (DAG) of tasks.
- **Retry Logic**: Most nodes include `max_retries=5, wait=20`, suggesting resilience to transient failures (e.g., API timeouts, LLM rate limits).
- **Batch Processing**: `WriteChapters` is noted as a `BatchNode`, indicating it can process multiple chapters concurrently‚Äîlikely for performance.
- **Extensibility**: The flow is decoupled from node logic (all in `nodes.py`), making it easy to modify or replace individual steps.
- **TODO (implied)**:  
  - Error handling or fallback logic is not visible here (likely handled in nodes).  
  - No configuration parameters (e.g., repo URL, output format) are passed‚Äîthese may be injected via node constructors or context.  
  - Consider making the flow configurable (e.g., via arguments) for reuse across projects.

**Short usage example**:  
```python
from flow import create_tutorial_flow

# Create and run the tutorial generation pipeline
flow = create_tutorial_flow()
flow.run()  # Executes the full flow: fetch ‚Üí analyze ‚Üí write ‚Üí combine
```  

> **Note**: The actual input (e.g., repo URL) and output (e.g., tutorial file path) are likely managed within the nodes or via shared context, not visible in this file.
---


### File: main.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\main.py

# main.py - File Summary

**one_line**:  
CLI entry point for generating multilingual tutorials from GitHub repos or local directories using a modular flow pipeline.

**Purpose of the file**:  
This script serves as the command-line interface (CLI) for a tutorial generation system that analyzes codebases (either from a GitHub URL or a local directory), extracts key abstractions and relationships, and generates structured, language-specific tutorials. It parses user inputs, configures the processing pipeline, and orchestrates the flow execution.

**Major functions/classes**:  
- `main()`:  
  - Sets up argument parsing using `argparse`.
  - Validates and prepares inputs (source, patterns, token, language, etc.).
  - Initializes the `shared` state dictionary passed through the flow.
  - Instantiates and runs the tutorial generation flow via `create_tutorial_flow()`.
- **Imported dependency**: `create_tutorial_flow()` from `flow` module ‚Äî defines the core processing steps (not shown here, but assumed to be a DAG of nodes).

**Key technical details & TODOs**:  
- **Source flexibility**: Supports both `--repo` (public GitHub URL) and `--dir` (local path) as mutually exclusive sources.
- **Pattern filtering**: Uses glob-style `include`/`exclude` patterns (defaults defined for common code files and build/test assets).
- **GitHub token handling**: Falls back to `GITHUB_TOKEN` environment variable if not provided; warns on missing token for repo mode.
- **LLM caching control**: `--no-cache` flag disables caching for reproducibility or debugging.
- **Language support**: `--language` parameter allows multilingual output (default: English).
- **Abstraction limit**: `--max-abstractions` caps the number of high-level concepts extracted (default: 10).
- **File size limit**: Skips files larger than `max-size` (default: 100KB) to avoid processing huge files.
- **Extensible shared state**: The `shared` dictionary is the central data bus for the flow, containing inputs, outputs, and metadata.
- **TODOs (implied)**:  
  - No error handling for invalid directories or unreachable repos (relies on downstream flow nodes).
  - No validation of language codes (assumes valid input).
  - No progress tracking or verbose mode (could benefit from `--verbose` flag).

**Short usage example**:  
```bash
# Generate tutorial from GitHub repo in French, with custom patterns
python main.py \
  --repo https://github.com/user/repo \
  --token ghp_... \
  --output ./tutorials/repo \
  --include "*.py" "*.md" \
  --exclude "tests/*" \
  --language french \
  --max-abstractions 15 \
  --no-cache

# Generate tutorial from local directory
python main.py \
  --dir ./my_project \
  --name "My Project" \
  --output ./docs/tutorial \
  --language spanish
```
---


### File: nodes.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\nodes.py

# nodes.py - File Summary

### one_line  
A collection of PocketFlow `Node` classes that fetch codebases, identify core abstractions, analyze relationships, and order educational chapters for onboarding.

---

### Purpose  
This file implements a pipeline for **automatically analyzing codebases** (from GitHub or local directories) to generate beginner-friendly educational content. It identifies key abstractions, explains how they relate, and orders them into a coherent learning sequence ‚Äî ideal for documentation, onboarding, or codebase exploration tools.

---

### Major Functions/Classes

| Class | Role |
|------|------|
| `FetchRepo` | Fetches files from a GitHub repo or local directory using configurable filters (patterns, size). |
| `IdentifyAbstractions` | Uses an LLM to extract top core abstractions (with names, descriptions, and file references) from code. |
| `AnalyzeRelationships` | Analyzes how abstractions interact and generates a project summary + relationship graph. |
| `OrderChapters` | Determines a logical learning order (chapter sequence) for abstractions based on their relationships. |
| `get_content_for_indices()` *(helper)* | Extracts file content by index for context injection into LLM prompts. |

---

### Key Technical Details & TODOs

- **LLM Integration**: Uses `call_llm()` to process natural language prompts and parse structured YAML output.
- **YAML Parsing & Validation**: All LLM outputs are parsed as YAML and rigorously validated for required fields, types, and index bounds.
- **Language Support**: Non-English abstractions/summaries/labels are supported via `language` parameter (e.g., Spanish, Japanese).
- **Caching**: LLM calls use `use_cache` flag (default: `True`) and only cache first attempt to avoid stale retries.
- **File Handling**: 
  - Converts fetched files to list of `(path, content)` tuples.
  - Uses **index-based referencing** in LLM prompts to avoid path duplication.
- **Error Handling**: Raises `ValueError` on missing files, invalid YAML, or malformed LLM output.
- **Extensibility**: Designed to plug into a larger PocketFlow pipeline; uses `shared` context for data passing.

#### ‚úÖ TODOs / Improvements (inferred):
- [ ] Add **token budgeting** for large codebases in LLM context creation.
- [ ] Support **chunking or summarization** of very large files before LLM ingestion.
- [ ] Allow **custom prompt templates** per language or project type.
- [ ] Add **retry logic with exponential backoff** for LLM calls.
- [ ] Consider **abstraction deduplication** or merging for overlapping concepts.
- [ ] Validate that **all abstractions are included** in chapter order (currently not enforced).

---

### Short Usage Example

```python
from pocketflow import Flow
from nodes import FetchRepo, IdentifyAbstractions, AnalyzeRelationships, OrderChapters

# Shared context
shared = {
    "repo_url": "https://github.com/user/example-project",
    "include_patterns": ["*.py", "*.js"],
    "exclude_patterns": ["test*", "*.md"],
    "max_file_size": 50000,
    "language": "english",
    "use_cache": True,
    "max_abstraction_num": 8
}

# Build flow
flow = Flow(
    FetchRepo() >>
    IdentifyAbstractions() >>
    AnalyzeRelationships() >>
    OrderChapters()
)

# Run
flow.run(shared)

# Access results
print("Project Summary:", shared["relationships"]["summary"])
print("Abstractions:", shared["abstractions"])
print("Relationships:", shared["relationships"]["details"])
print("Chapter Order:", shared["chapters"])  # List of abstraction indices in order
```

> üìù This pipeline is ideal for generating **interactive documentation**, **onboarding guides**, or **codebase maps** automatically.
---


### File: __init__.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\__init__.py

# __init__.py - File Summary

**one_line**:  
Utility module initializer that imports and exposes key functions for shared use across the project.

**Purpose**:  
This `__init__.py` file serves as the entry point for the `utils` package, centralizing and exposing commonly used utility functions and classes to streamline imports throughout the codebase (e.g., `from utils import log, retry, ...`).

**Major functions/classes**:  
- Exposes the following (imported from submodules):
  - `log` ‚Äì Standardized logging function with timestamps and levels.
  - `retry` ‚Äì Decorator to retry a function on failure with exponential backoff.
  - `read_json`, `write_json` ‚Äì Safe JSON file I/O with error handling.
  - `hash_string` ‚Äì Utility to generate deterministic hash (e.g., for caching keys).
  - `Timer` ‚Äì Context manager for measuring execution time.

**Key technical details & TODOs**:  
- Uses `from .logging import log`, `from .decorators import retry`, etc., to keep internal structure modular.
- Designed to avoid circular imports by lazy-loading heavy dependencies inside functions where possible.
- All exposed utilities are stateless and thread-safe.
- **TODO**: Add type hints to all public functions in submodules.
- **TODO**: Consider adding a `__all__` list to explicitly control what gets exported on `from utils import *`.

**Short usage example**:  
```python
from utils import log, retry, read_json, Timer

@retry(max_attempts=3)
def fetch_data():
    log("Fetching data...")
    with Timer("Data load"):
        return read_json("data.json")

data = fetch_data()
```
---


### File: call_llm.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\call_llm.py

# call_llm.py - File Summary

**one_line**:  
Unified LLM API caller with caching, logging, and multi-provider support (Gemini, Azure, Anthropic, OpenAI, OpenRouter).

**Purpose**:  
Provides a single, consistent interface to call various Large Language Model (LLM) APIs, with built-in prompt/response logging, disk-based caching, and environment-based configuration. Designed for use in larger workflows (e.g., PocketFlow) to reduce costs and latency via caching and improve observability via structured logging.

**Major functions/classes**:
- `call_llm(prompt: str, use_cache: bool = True) -> str`  
  The main function to send a prompt to an LLM and get a response. Supports caching, logging, and currently defaults to **Google Gemini 2.5 Pro** via API key.
- **Commented alternative implementations** for:
  - Azure OpenAI
  - Anthropic Claude 3.7 Sonnet (with extended thinking)
  - OpenAI o1
  - OpenRouter (supports any model via OpenRouter API)
- **Logging setup**: All prompts and responses are logged to a daily rotating file in the `logs/` directory.
- **Caching mechanism**: Uses `llm_cache.json` to store prompt-response pairs, avoiding redundant API calls.

**Key technical details & TODOs**:
- ‚úÖ **Default provider**: Google Gemini via `genai.Client(api_key=...)` using `GEMINI_API_KEY` env var.
- ‚úÖ **Caching**: Simple JSON file cache (`llm_cache.json`) keyed by prompt. Thread-unsafe but sufficient for single-threaded use.
- ‚úÖ **Logging**: Logs to `logs/llm_calls_YYYYMMDD.log` with timestamps, levels, and full prompt/response.
- üîÅ **Multi-provider support**: Easily switch providers by uncommenting the desired implementation and setting relevant environment variables.
- üõë **Security**: API keys are read from environment variables (recommended), but fallbacks to hardcoded values (e.g., `"your-project-id"`) ‚Äî **TODO: Remove or warn about insecure defaults**.
- üß™ **Testing**: Includes a simple `__main__` block to test the function.
- ‚ö†Ô∏è **Cache race condition**: Cache is reloaded before write, but concurrent access could still cause issues.
- üì¶ **Dependencies**: Requires `google-generativeai`, and optionally `openai`, `anthropic`, or `requests` for other providers.
- üîß **Configurable via env vars**:
  - `LOG_DIR`: Log directory (default: `logs`)
  - `GEMINI_API_KEY`, `GEMINI_MODEL`, `GEMINI_PROJECT_ID`, `GEMINI_LOCATION`
  - `OPENROUTER_API_KEY`, `OPENROUTER_MODEL`
  - `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, etc.
- üßπ **TODO**: Add cache TTL, size limit, or hash-based keys to avoid JSON size issues with long prompts.
- üßπ **TODO**: Add error handling and retry logic for API failures.

**Short usage example**:
```python
from utils.call_llm import call_llm

# Set environment variables first (e.g., GEMINI_API_KEY)
response = call_llm("Explain quantum computing in simple terms")
print(response)

# Disable cache for fresh call
response_fresh = call_llm("Hello", use_cache=False)
```

> üí° **Tip**: Switch providers by commenting out the current `call_llm` and uncommenting another ‚Äî ensure required env vars and packages are set up.
---


### File: crawl_github_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_github_files.py

# crawl_github_files.py - File Summary

### one_line  
A utility to crawl and download files from a GitHub repository (public or private) at a specific commit/path, with support for filtering, size limits, and relative path handling.

---

### Purpose  
This script enables programmatic retrieval of source code or documentation files from a GitHub repository via the GitHub API (or SSH clone), with fine-grained control over which files are downloaded based on patterns, file size, and path. It is useful for tools that need to analyze or process repository contents (e.g., code generation, documentation extraction, CI/CD automation).

---

### Major Functions/Classes

#### `crawl_github_files(...)`
Main function that orchestrates crawling:
- Parses GitHub URL (including commit/branch and subdirectory).
- Handles both **HTTPS** (via GitHub API) and **SSH** (via `git clone`) URLs.
- Downloads files recursively from a specified subdirectory.
- Applies **include/exclude glob patterns** to filter files.
- Enforces **maximum file size** to avoid memory issues.
- Returns a dictionary with file contents and crawl statistics.

#### `should_include_file(file_path, file_name)`
Helper to evaluate inclusion based on `include_patterns` and `exclude_patterns` using `fnmatch`.

#### `fetch_contents(path)` (nested)
Recursive function to traverse GitHub repository structure using the GitHub REST API (`/contents` endpoint), downloading files and skipping directories based on filters.

#### `fetch_branches()` and `check_tree()`
Used to validate and extract commit/branch reference from the URL when not explicitly provided.

---

### Key Technical Details & TODOs

#### ‚úÖ **Features**
- **Supports both public and private repositories** via GitHub token (`GITHUB_TOKEN` env or argument).
- **Handles rate limits** by pausing and retrying when hitting GitHub API limits.
- **Two URL modes**:
  - HTTPS: Uses GitHub API (supports branch/commit + path).
  - SSH: Falls back to `git clone` into temp directory (no branch parsing in URL; uses default).
- **Flexible filtering**:
  - `include_patterns`: e.g., `{"*.py", "*.md"}`
  - `exclude_patterns`: e.g., `{"*test*", "*.log"}`
- **Relative paths option**: Strips base directory prefix if `use_relative_paths=True`.
- **Robust error handling** for 403, 404, encoding, file I/O, and size checks.

#### ‚ö†Ô∏è **Technical Notes**
- **SSH URLs do not support branch/commit parsing** from the URL ‚Äî relies on default branch.
- **File size is checked twice**: once from metadata, once from `download_url` headers.
- **Base64 decoding** used when `download_url` is missing (fallback).
- **Recursive directory traversal** with early pruning using exclude patterns (optimized in new implementation).
- Uses **`tempfile.TemporaryDirectory`** for SSH cloning (auto-cleaned).

#### üîß **TODOs / Improvements**
- [ ] **Add support for SSH URLs with branch/commit** (e.g., `git@github.com:owner/repo.git#commit/path`).
- [ ] **Add progress bar or logging** for large repositories.
- [ ] **Support for shallow clones** in SSH mode to reduce bandwidth.
- [ ] **Caching mechanism** to avoid re-downloading unchanged files.
- [ ] **Better handling of symlinks and binary files** (currently skipped silently).
- [ ] **Validate `include_patterns`/`exclude_patterns` syntax** before use.
- [ ] **Add retry logic** for transient network failures.

---

### Short Usage Example

```python
import os
from crawl_github_files import crawl_github_files

# Optional: Set token for private repos or to avoid rate limits
os.environ["GITHUB_TOKEN"] = "your_token_here"

result = crawl_github_files(
    repo_url="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-core",
    token=None,  # uses GITHUB_TOKEN env
    max_file_size=500 * 1024,  # 500 KB
    use_relative_paths=True,
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"*test*"}
)

print(f"Downloaded {result['stats']['downloaded_count']} files")
for path, content in result["files"].items():
    print(f"{path}: {len(content)} chars")
```

> üí° **Tip**: Use `include_patterns` and `exclude_patterns` to focus on relevant files (e.g., source code, docs) and skip binaries, logs, or tests.
---


### File: crawl_local_files.py
Path: C:\Users\KERC\AppData\Local\Temp\pocketflow_repo_3651656279\utils\crawl_local_files.py

# crawl_local_files.py - File Summary

### one_line
Crawls local directories to collect file contents with pattern-based inclusion/exclusion, `.gitignore` support, size limits, and progress tracking.

---

### Purpose
This utility mimics a GitHub file crawler for **local filesystems**, enabling selective reading of files based on include/exclude patterns, `.gitignore` rules, and file size constraints. It is designed for use in code analysis, automation, or AI training data preparation tools where controlled file ingestion is required.

---

### Major functions/classes

- **`crawl_local_files()`**  
  Main function that:
  - Walks a directory tree (`os.walk`)
  - Applies `.gitignore` rules (if present)
  - Filters files using `include_patterns` and `exclude_patterns` via `fnmatch`
  - Enforces a maximum file size
  - Reads and returns file contents as a dictionary
  - Shows real-time progress with colored output

---

### Key technical details & TODOs

#### ‚úÖ **Features**
- **`.gitignore` Support**: Uses `pathspec.PathSpec` to respect `.gitignore` rules (both files and directories).
- **Pattern Matching**: Uses `fnmatch` for glob-style patterns (e.g., `*.py`, `tests/*`).
- **Early Directory Pruning**: Skips entire directories during `os.walk` if excluded (improves performance).
- **Progress Feedback**: Prints real-time progress (file count, percentage, status) in green (`\033[92m`).
- **Relative Paths**: Optionally returns paths relative to input directory.
- **Robust Encoding**: Uses `utf-8-sig` to handle BOM in files.
- **Error Handling**: Gracefully skips unreadable files and logs warnings.

#### ‚ö†Ô∏è **Limitations / TODOs**
- **No Symlink Handling**: Follows symlinks by default (potential cycles or unintended reads).
- **No Binary Detection**: Attempts to read all files as text ‚Äî may fail on binaries.
- **Progress Overhead**: Frequent `print()` calls may slow down large crawls; consider optional verbosity flag.
- **Hardcoded Color Codes**: ANSI colors may not render well in all terminals.
- **No Async Support**: Synchronous I/O; not suitable for huge repositories without optimization.

> üîß **Suggested Improvements (TODOs):**
> - Add `verbose`/`quiet` mode flag to control output.
> - Add option to detect and skip binary files.
> - Support for custom `.gitignore`-like files (e.g., `.crawlignore`).
> - Return metadata (size, mtime) alongside content.
> - Allow custom file read handlers (e.g., for line filtering).

---

### Short usage example

```python
# Crawl current directory, include only Python files, exclude test and cache dirs
result = crawl_local_files(
    directory=".",
    include_patterns={"*.py", "*.md"},
    exclude_patterns={"tests/*", "__pycache__/*", "*.log"},
    max_file_size=1024 * 1024,  # 1 MB limit
    use_relative_paths=True
)

# Print file paths
for path in result["files"]:
    print(path)

# Use file content
for path, content in result["files"].items():
    print(f"--- {path} ---\n{content[:200]}...\n")
```

> üí° **Tip**: Run as a script (`python crawl_local_files.py`) to crawl the parent directory with default exclusions (e.g., `.git`, `.venv`).
---


Architectural abstractions:
```markdown
# Core Architectural Abstractions & Components

Below are up to 15 core abstractions/components identified from the codebase, based on file summaries and their relationships.

---

## 1. **TutorialFlow**
**Description**:  
Orchestrates the end-to-end pipeline for generating AI-powered codebase tutorials using PocketFlow.

**Files**:  
- `flow.py`, `main.py`, `nodes.py`

**Dependencies**:  
- `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`, `call_llm`, `crawl_github_files`, `crawl_local_files`

**Responsibilities**:  
- Defines and executes the sequential workflow (DAG) for tutorial generation.
- Manages data flow between nodes via the `shared` context.
- Enables retry logic and batch processing for robustness and performance.

---

## 2. **FetchRepo**
**Description**:  
Retrieves source code from a GitHub repository or local directory with filtering and size constraints.

**Files**:  
- `nodes.py` (class: `FetchRepo`), `utils/crawl_github_files.py`, `utils/crawl_local_files.py`

**Dependencies**:  
- `crawl_github_files`, `crawl_local_files`, `pathspec`, `requests`, `tempfile`

**Responsibilities**:  
- Parses repo URL or local path input.
- Downloads and filters files using glob patterns and `.gitignore`.
- Enforces maximum file size to prevent memory overflow.
- Returns structured list of `(path, content)` tuples for downstream processing.

---

## 3. **IdentifyAbstractions**
**Description**:  
Uses LLMs to extract high-level abstractions (e.g., classes, patterns) from raw code.

**Files**:  
- `nodes.py` (class: `IdentifyAbstractions`)

**Dependencies**:  
- `call_llm`, `utils.read_json`, `yaml`, `shared` context

**Responsibilities**:  
- Constructs prompts with code context (by file index) for LLM.
- Parses LLM-generated YAML to extract abstraction names, descriptions, and file references.
- Validates output structure and enforces limits (e.g., max 10 abstractions).
- Stores results in `shared["abstractions"]`.

---

## 4. **AnalyzeRelationships**
**Description**:  
Analyzes how identified abstractions interact and generates a project summary and relationship map.

**Files**:  
- `nodes.py` (class: `AnalyzeRelationships`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Builds LLM prompt with abstractions and relevant file snippets.
- Extracts project summary and pairwise relationships (e.g., "X depends on Y").
- Outputs structured YAML into `shared["relationships"]` for tutorial structuring.

---

## 5. **OrderChapters**
**Description**:  
Determines a pedagogical order for tutorial chapters based on abstraction relationships.

**Files**:  
- `nodes.py` (class: `OrderChapters`)

**Dependencies**:  
- `call_llm`, `shared` context (abstractions, relationships)

**Responsibilities**:  
- Uses LLM to reason about dependencies and learning sequence.
- Returns a list of abstraction indices in logical order (e.g., from foundational to advanced).
- Stores result in `shared["chapters"]`.

---

## 6. **WriteChapters**
**Description**:  
Generates written content for each chapter (abstraction) using LLM, in a target language.

**Files**:  
- `nodes.py` (class: `WriteChapters`, implemented as `BatchNode`)

**Dependencies**:  
- `call_llm`, `get_content_for_indices`, `shared` context

**Responsibilities**:  
- Processes multiple abstractions in parallel (batch mode).
- For each chapter, generates beginner-friendly explanations with code examples.
- Supports multilingual output via language parameter.
- Stores chapter content in `shared["chapter_contents"]`.

---

## 7. **CombineTutorial**
**Description**:  
Merges all generated chapters into a single, structured tutorial document.

**Files**:  
- `nodes.py` (class: `CombineTutorial`)

**Dependencies**:  
- `shared` context (`chapter_contents`, `project_summary`, `abstractions`)

**Responsibilities**:  
- Combines chapter content in the ordered sequence.
- Adds metadata (title, language, summary).
- Outputs final tutorial (Markdown/HTML) to specified output directory.

---

## 8. **CallLLM**
**Description**:  
Unified interface to call various LLMs (Gemini, Claude

Write a clear and concise tutorial chapter.


RESPONSE:
# **Chapter 4: Deep Dive ‚Äì Fetching the Codebase**  
**Objective**: Understand how the system ingests and filters source code.

Before any AI can analyze a codebase, it must first **fetch** the right files ‚Äî not too many, not too few ‚Äî in a way that's efficient, safe, and reproducible. This chapter takes you behind the scenes of how the system retrieves code from **GitHub repositories** or **local directories**, applies intelligent filtering, and prepares it for analysis.

We‚Äôll explore:
- How the system fetches code from GitHub or your local machine
- How it filters files using patterns and `.gitignore`
- How file size and structure are managed to avoid overload
- The role of caching and resilience in the pipeline

Let‚Äôs dive into the **core ingestion engine** of the tutorial generator.

---

## üîç **1. The Entry Point: `FetchRepo` Node**

At the start of every tutorial generation flow is the `FetchRepo` class in `nodes.py`. This is the **gatekeeper** of the pipeline ‚Äî responsible for retrieving source files and passing them downstream.

```python
class FetchRepo(Node):
    def prep(self, shared):
        # Get configuration from shared context
        repo_url = shared.get("repo_url")
        local_dir = shared.get("dir")
        include_patterns = shared.get("include_patterns", ["*.py", "*.js", "*.ts", "*.go", "*.rs", "*.md", "*.txt", "*.html", "*.css", "*.json", "*.yaml", "*.yml"])
        exclude_patterns = shared.get("exclude_patterns", ["*test*", "*spec*", "*.log", "__pycache__/*", "node_modules/*", ".git/*", ".venv/*", "dist/*", "build/*"])
        max_file_size = shared.get("max_file_size", 100 * 1024)  # 100 KB default
        use_relative_paths = True

        # Choose source: GitHub or local
        if repo_url:
            return crawl_github_files(
                repo_url=repo_url,
                token=shared.get("github_token"),
                max_file_size=max_file_size,
                use_relative_paths=use_relative_paths,
                include_patterns=set(include_patterns),
                exclude_patterns=set(exclude_patterns)
            )
        else:
            return crawl_local_files(
                directory=local_dir,
                include_patterns=set(include_patterns),
                exclude_patterns=set(exclude_patterns),
                max_file_size=max_file_size,
                use_relative_paths=use_relative_paths
            )
```

> ‚úÖ **Key Insight**: `FetchRepo` is **source-agnostic**. Whether the code lives on GitHub or your laptop, the same filtering logic applies.

---

## üåê **2. Fetching from GitHub: Smart Crawling with API or Git**

When a GitHub URL is provided (e.g., `--repo https://github.com/encode/fastapi`), the system uses `crawl_github_files.py` to retrieve files.

### üîß **Two Modes of Operation**

| Mode | When Used | How It Works |
|------|-----------|-------------|
| **GitHub API Mode (HTTPS URL)** | Default for public/private repos | Uses GitHub‚Äôs `/contents` REST API to list and download files recursively. Supports branches, commits, and subdirectories (e.g., `tree/main/src`). |
| **Git Clone Mode (SSH URL)** | Fallback for SSH or complex cases | Clones the repo into a temporary directory using `git`, then reads files locally. Less efficient but more reliable for private repos with SSH access. |

### üõ°Ô∏è **Security & Rate Limiting**
- Uses `GITHUB_TOKEN` (from env or CLI) to access private repos and avoid rate limits.
- Automatically pauses and retries when hitting GitHub API rate limits (403 responses).
- Validates file size **before** downloading to prevent memory bloat.

### üéØ **Pattern Filtering**
Files are filtered using **glob-style patterns**:
```python
include_patterns = {"*.py", "*.md"}
exclude_patterns = {"*test*", "*.log"}
```
This ensures only relevant source and documentation files are included ‚Äî skipping tests, logs, binaries, and build artifacts.

> üí° **Pro Tip**: You can customize these patterns via CLI flags:
> ```bash
> --include "*.py" "*.md" --exclude "tests/*" "*.min.js"
> ```

---

## üíæ **3. Fetching from Local Directories: `.gitignore`-Aware Crawling**

When analyzing a local project
---
